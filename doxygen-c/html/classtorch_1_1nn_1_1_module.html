<!-- HTML header for doxygen 1.8.14-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Caffe2 - C++ API: torch::nn::Module Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="/static/favicon.png" type="image/x-icon">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
<link href="main.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo" width="56"><a href="/"><img alt="Logo" src="Caffe2-with-name-55-tall.png"/></a></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Caffe2 - C++ API
   </div>
   <div id="projectbrief">A deep learning, cross platform ML framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li class="current"><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="/doxygen-c/html/classes.html"><span>C++&#160;API</span></a></li>
      <li><a href="/doxygen-python/html/annotated.html"><span>Python&#160;API</span></a></li>
      <li><a href="https://github.com/caffe2/caffe2"><span>GitHub</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li><a href="classes.html"><span>Data&#160;Structure&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Data&#160;Fields</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>torch</b></li><li class="navelem"><b>nn</b></li><li class="navelem"><a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-types">Public Types</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#friends">Friends</a>  </div>
  <div class="headertitle">
<div class="title">torch::nn::Module Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>The base class for all modules in PyTorch.  
 <a href="classtorch_1_1nn_1_1_module.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for torch::nn::Module:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classtorch_1_1nn_1_1_module.png" usemap="#torch::nn::Module_map" alt=""/>
  <map id="torch::nn::Module_map" name="torch::nn::Module_map">
<area href="class_a.html" title="does bound shape inference given a C2 net. " alt="A" shape="rect" coords="269,112,528,136"/>
<area href="struct_a_g_i_unit.html" alt="AGIUnit" shape="rect" coords="269,168,528,192"/>
<area href="struct_a_impl.html" alt="AImpl" shape="rect" coords="269,224,528,248"/>
<area href="struct_b.html" alt="B" shape="rect" coords="269,280,528,304"/>
<area href="struct_buffer_test_module.html" alt="BufferTestModule" shape="rect" coords="269,336,528,360"/>
<area href="struct_c.html" alt="C" shape="rect" coords="269,392,528,416"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; BatchNormImpl &gt;" shape="rect" coords="269,448,528,472"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; Conv1dImpl &gt;" shape="rect" coords="269,504,528,528"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; Conv2dImpl &gt;" shape="rect" coords="269,560,528,584"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; Conv3dImpl &gt;" shape="rect" coords="269,616,528,640"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; DropoutImpl &gt;" shape="rect" coords="269,672,528,696"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; EmbeddingImpl &gt;" shape="rect" coords="269,728,528,752"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; FeatureDropoutImpl &gt;" shape="rect" coords="269,784,528,808"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; GRUImpl &gt;" shape="rect" coords="269,840,528,864"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; LinearImpl &gt;" shape="rect" coords="269,896,528,920"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; LSTMImpl &gt;" shape="rect" coords="269,952,528,976"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; Net &gt;" shape="rect" coords="269,1008,528,1032"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; RNNImpl &gt;" shape="rect" coords="269,1064,528,1088"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; SequentialImpl &gt;" shape="rect" coords="269,1120,528,1144"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" alt="torch::nn::Cloneable&lt; SimpleContainer &gt;" shape="rect" coords="269,1176,528,1200"/>
<area href="struct_d.html" alt="D" shape="rect" coords="269,1232,528,1256"/>
<area href="struct_e.html" alt="E" shape="rect" coords="269,1288,528,1312"/>
<area href="struct_empty_module.html" alt="EmptyModule" shape="rect" coords="269,1344,528,1368"/>
<area href="struct_m.html" alt="M" shape="rect" coords="269,1400,528,1424"/>
<area href="struct_module_with_non_tensor_forward_impl.html" alt="ModuleWithNonTensorForwardImpl" shape="rect" coords="269,1456,528,1480"/>
<area href="class_nested_model.html" alt="NestedModel" shape="rect" coords="269,1512,528,1536"/>
<area href="struct_parameter_test_module.html" alt="ParameterTestModule" shape="rect" coords="269,1568,528,1592"/>
<area href="structtest_1_1_a_g_i_unit.html" alt="test::AGIUnit" shape="rect" coords="269,1624,528,1648"/>
<area href="structtest_1_1_a_g_i_unit2.html" alt="test::AGIUnit2" shape="rect" coords="269,1680,528,1704"/>
<area href="struct_test_container.html" alt="TestContainer" shape="rect" coords="269,1736,528,1760"/>
<area href="class_test_model.html" alt="TestModel" shape="rect" coords="269,1792,528,1816"/>
<area href="struct_test_module.html" alt="TestModule" shape="rect" coords="269,1848,528,1872"/>
<area href="classtorch_1_1nn_1_1_cloneable.html" title="The clone() method in the base Module class does not have knowledge of the concrete runtime type of i..." alt="torch::nn::Cloneable&lt; Derived &gt;" shape="rect" coords="269,1904,528,1928"/>
</map>
 </div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-types"></a>
Public Types</h2></td></tr>
<tr class="memitem:a8a47ce884e1eb218f07d65d5fb9a4eae"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a8a47ce884e1eb218f07d65d5fb9a4eae"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><b>ModuleApplyFunction</b> = std::function&lt; void(<a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &amp;)&gt;</td></tr>
<tr class="separator:a8a47ce884e1eb218f07d65d5fb9a4eae"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a621e17f1e564b3f2b795a9248a0834ee"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a621e17f1e564b3f2b795a9248a0834ee"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><b>ConstModuleApplyFunction</b> = std::function&lt; void(const <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &amp;)&gt;</td></tr>
<tr class="separator:a621e17f1e564b3f2b795a9248a0834ee"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad3fc42479f682663add3b5eb9e707afc"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad3fc42479f682663add3b5eb9e707afc"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><b>NamedModuleApplyFunction</b> = std::function&lt; void(const std::string &amp;, <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &amp;)&gt;</td></tr>
<tr class="separator:ad3fc42479f682663add3b5eb9e707afc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a321b55e7501c7bd3e2bd4ef09cb43fb8"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a321b55e7501c7bd3e2bd4ef09cb43fb8"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><b>ConstNamedModuleApplyFunction</b> = std::function&lt; void(const std::string &amp;, const <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &amp;)&gt;</td></tr>
<tr class="separator:a321b55e7501c7bd3e2bd4ef09cb43fb8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a068ca6621e67f3d4ef1828b719a8a347"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a068ca6621e67f3d4ef1828b719a8a347"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><b>ModulePointerApplyFunction</b> = std::function&lt; void(const std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &amp;)&gt;</td></tr>
<tr class="separator:a068ca6621e67f3d4ef1828b719a8a347"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1656aeb55c051e8368db6871d536e341"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a1656aeb55c051e8368db6871d536e341"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><b>NamedModulePointerApplyFunction</b> = std::function&lt; void(const std::string &amp;, const std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &amp;)&gt;</td></tr>
<tr class="separator:a1656aeb55c051e8368db6871d536e341"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a33ac482c601ffecdaabe46a0f364cc51"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a33ac482c601ffecdaabe46a0f364cc51"></a>
&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a33ac482c601ffecdaabe46a0f364cc51">Module</a> (std::string <a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403">name</a>)</td></tr>
<tr class="memdesc:a33ac482c601ffecdaabe46a0f364cc51"><td class="mdescLeft">&#160;</td><td class="mdescRight">Tells the base <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> about the name of the submodule. <br /></td></tr>
<tr class="separator:a33ac482c601ffecdaabe46a0f364cc51"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a941f24af291e05af5428e125d20f486a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a941f24af291e05af5428e125d20f486a">Module</a> ()</td></tr>
<tr class="memdesc:a941f24af291e05af5428e125d20f486a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs the module without immediate knowledge of the submodule's name.  <a href="#a941f24af291e05af5428e125d20f486a">More...</a><br /></td></tr>
<tr class="separator:a941f24af291e05af5428e125d20f486a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab503bb4fdf163c00ac1e9fc695f55403"><td class="memItemLeft" align="right" valign="top">const std::string &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403">name</a> () const noexcept</td></tr>
<tr class="memdesc:ab503bb4fdf163c00ac1e9fc695f55403"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the name of the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>.  <a href="#ab503bb4fdf163c00ac1e9fc695f55403">More...</a><br /></td></tr>
<tr class="separator:ab503bb4fdf163c00ac1e9fc695f55403"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e9244756b2265ad97c716cc7f885d4d"><td class="memItemLeft" align="right" valign="top">virtual std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d">clone</a> (const <a class="el" href="classc10_1_1optional.html">optional</a>&lt; <a class="el" href="structc10_1_1_device.html">Device</a> &gt; &amp;device=nullopt) const </td></tr>
<tr class="memdesc:a1e9244756b2265ad97c716cc7f885d4d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs a recursive deep copy of the module and all its registered parameters, buffers and submodules.  <a href="#a1e9244756b2265ad97c716cc7f885d4d">More...</a><br /></td></tr>
<tr class="separator:a1e9244756b2265ad97c716cc7f885d4d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e7bf8192a37c7e6593b5d1dd5d15903"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a3e7bf8192a37c7e6593b5d1dd5d15903">apply</a> (const ModuleApplyFunction &amp;function)</td></tr>
<tr class="memdesc:a3e7bf8192a37c7e6593b5d1dd5d15903"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule.  <a href="#a3e7bf8192a37c7e6593b5d1dd5d15903">More...</a><br /></td></tr>
<tr class="separator:a3e7bf8192a37c7e6593b5d1dd5d15903"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a295c66db3d747a25fba77cfe43357701"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a295c66db3d747a25fba77cfe43357701">apply</a> (const ConstModuleApplyFunction &amp;function) const </td></tr>
<tr class="memdesc:a295c66db3d747a25fba77cfe43357701"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule.  <a href="#a295c66db3d747a25fba77cfe43357701">More...</a><br /></td></tr>
<tr class="separator:a295c66db3d747a25fba77cfe43357701"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aac037622ec1099f031a284bb8e084832"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#aac037622ec1099f031a284bb8e084832">apply</a> (const NamedModuleApplyFunction &amp;function, const std::string &amp;name_prefix=std::string())</td></tr>
<tr class="memdesc:aac037622ec1099f031a284bb8e084832"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule.  <a href="#aac037622ec1099f031a284bb8e084832">More...</a><br /></td></tr>
<tr class="separator:aac037622ec1099f031a284bb8e084832"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a40c5e885c7de689d6346af6151f05e83"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a40c5e885c7de689d6346af6151f05e83">apply</a> (const ConstNamedModuleApplyFunction &amp;function, const std::string &amp;name_prefix=std::string()) const </td></tr>
<tr class="memdesc:a40c5e885c7de689d6346af6151f05e83"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule.  <a href="#a40c5e885c7de689d6346af6151f05e83">More...</a><br /></td></tr>
<tr class="separator:a40c5e885c7de689d6346af6151f05e83"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0450302ffa15382970fec94966e47dc8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a0450302ffa15382970fec94966e47dc8">apply</a> (const ModulePointerApplyFunction &amp;function) const </td></tr>
<tr class="memdesc:a0450302ffa15382970fec94966e47dc8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule.  <a href="#a0450302ffa15382970fec94966e47dc8">More...</a><br /></td></tr>
<tr class="separator:a0450302ffa15382970fec94966e47dc8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a312358768ca495290c0c5541d672a039"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a312358768ca495290c0c5541d672a039">apply</a> (const NamedModulePointerApplyFunction &amp;function, const std::string &amp;name_prefix=std::string()) const </td></tr>
<tr class="memdesc:a312358768ca495290c0c5541d672a039"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule.  <a href="#a312358768ca495290c0c5541d672a039">More...</a><br /></td></tr>
<tr class="separator:a312358768ca495290c0c5541d672a039"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae4c84424addf8712d1752566a93d99ad"><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#ae4c84424addf8712d1752566a93d99ad">parameters</a> (bool recurse=true) const </td></tr>
<tr class="memdesc:ae4c84424addf8712d1752566a93d99ad"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the parameters of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and if <code>recurse</code> is true, also recursively of every submodule.  <a href="#ae4c84424addf8712d1752566a93d99ad">More...</a><br /></td></tr>
<tr class="separator:ae4c84424addf8712d1752566a93d99ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3efdc049d212c8f49cd45831eb5effdb"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a3efdc049d212c8f49cd45831eb5effdb">named_parameters</a> (bool recurse=true) const </td></tr>
<tr class="memdesc:a3efdc049d212c8f49cd45831eb5effdb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> with the parameters of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> along with their keys, and if <code>recurse</code> is true also recursively of every submodule.  <a href="#a3efdc049d212c8f49cd45831eb5effdb">More...</a><br /></td></tr>
<tr class="separator:a3efdc049d212c8f49cd45831eb5effdb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a92cc52f55f49f84f9d2b277ff4816c95"><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a92cc52f55f49f84f9d2b277ff4816c95">buffers</a> (bool recurse=true) const </td></tr>
<tr class="memdesc:a92cc52f55f49f84f9d2b277ff4816c95"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the buffers of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and if <code>recurse</code> is true, also recursively of every submodule.  <a href="#a92cc52f55f49f84f9d2b277ff4816c95">More...</a><br /></td></tr>
<tr class="separator:a92cc52f55f49f84f9d2b277ff4816c95"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aece5594a6fc5b452208d81cbe828e557"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#aece5594a6fc5b452208d81cbe828e557">named_buffers</a> (bool recurse=true) const </td></tr>
<tr class="memdesc:aece5594a6fc5b452208d81cbe828e557"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> with the buffers of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> along with their keys, and if <code>recurse</code> is true also recursively of every submodule.  <a href="#aece5594a6fc5b452208d81cbe828e557">More...</a><br /></td></tr>
<tr class="separator:aece5594a6fc5b452208d81cbe828e557"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a284627aba1381c6db639b62207cdd560"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a284627aba1381c6db639b62207cdd560">modules</a> (bool include_self=true) const </td></tr>
<tr class="memdesc:a284627aba1381c6db639b62207cdd560"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the submodules of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> (the entire submodule hierarchy) and if <code>include_self</code> is true, also inserts a <code>shared_ptr</code> to this module in the first position.  <a href="#a284627aba1381c6db639b62207cdd560">More...</a><br /></td></tr>
<tr class="separator:a284627aba1381c6db639b62207cdd560"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0f82eaf10c89119841658ba26d9ec4fa"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a0f82eaf10c89119841658ba26d9ec4fa">named_modules</a> (const std::string &amp;name_prefix=std::string(), bool include_self=true) const </td></tr>
<tr class="memdesc:a0f82eaf10c89119841658ba26d9ec4fa"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> of he submodules of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> (the entire submodule hierarchy) and thei keys, and if <code>include_self</code> is true, also inserts a <code>shared_ptr</code> to this module in the first position.  <a href="#a0f82eaf10c89119841658ba26d9ec4fa">More...</a><br /></td></tr>
<tr class="separator:a0f82eaf10c89119841658ba26d9ec4fa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a16b16fd01a55fd7fdb6e7ed30d593055"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a16b16fd01a55fd7fdb6e7ed30d593055"></a>
std::vector&lt; std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a16b16fd01a55fd7fdb6e7ed30d593055">children</a> () const </td></tr>
<tr class="memdesc:a16b16fd01a55fd7fdb6e7ed30d593055"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the direct submodules of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. <br /></td></tr>
<tr class="separator:a16b16fd01a55fd7fdb6e7ed30d593055"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ffc35c81a390ab5d7f1e3cea318aae0"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a1ffc35c81a390ab5d7f1e3cea318aae0">named_children</a> () const </td></tr>
<tr class="memdesc:a1ffc35c81a390ab5d7f1e3cea318aae0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> of the direct submodules of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and their keys.  <a href="#a1ffc35c81a390ab5d7f1e3cea318aae0">More...</a><br /></td></tr>
<tr class="separator:a1ffc35c81a390ab5d7f1e3cea318aae0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7df32818dc554d4c59f82bfdf3248fd"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ab7df32818dc554d4c59f82bfdf3248fd"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#ab7df32818dc554d4c59f82bfdf3248fd">train</a> (bool on=true)</td></tr>
<tr class="memdesc:ab7df32818dc554d4c59f82bfdf3248fd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Enables "training" mode. <br /></td></tr>
<tr class="separator:ab7df32818dc554d4c59f82bfdf3248fd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0be79d2e17a200b5f69023ba6f02598"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#af0be79d2e17a200b5f69023ba6f02598">eval</a> ()</td></tr>
<tr class="memdesc:af0be79d2e17a200b5f69023ba6f02598"><td class="mdescLeft">&#160;</td><td class="mdescRight">Calls train(false) to enable "eval" mode.  <a href="#af0be79d2e17a200b5f69023ba6f02598">More...</a><br /></td></tr>
<tr class="separator:af0be79d2e17a200b5f69023ba6f02598"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0489238ce8594bf93c210e36e971d314"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a0489238ce8594bf93c210e36e971d314">is_training</a> () const noexcept</td></tr>
<tr class="memdesc:a0489238ce8594bf93c210e36e971d314"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if the module is in training mode.  <a href="#a0489238ce8594bf93c210e36e971d314">More...</a><br /></td></tr>
<tr class="separator:a0489238ce8594bf93c210e36e971d314"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9335a64808dda0178374d0818403f88f"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a9335a64808dda0178374d0818403f88f">to</a> (<a class="el" href="structc10_1_1_device.html">torch::Device</a> device, torch::Dtype dtype, bool non_blocking=false)</td></tr>
<tr class="memdesc:a9335a64808dda0178374d0818403f88f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recursively casts all parameters to the given <code>dtype</code> and <code>device</code>.  <a href="#a9335a64808dda0178374d0818403f88f">More...</a><br /></td></tr>
<tr class="separator:a9335a64808dda0178374d0818403f88f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a06916399bc5c0270d3db18153b4a0ad8"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a06916399bc5c0270d3db18153b4a0ad8">to</a> (torch::Dtype dtype, bool non_blocking=false)</td></tr>
<tr class="memdesc:a06916399bc5c0270d3db18153b4a0ad8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recursively casts all parameters to the given dtype.  <a href="#a06916399bc5c0270d3db18153b4a0ad8">More...</a><br /></td></tr>
<tr class="separator:a06916399bc5c0270d3db18153b4a0ad8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abd24812c2304761b85420f4d5edfa828"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#abd24812c2304761b85420f4d5edfa828">to</a> (<a class="el" href="structc10_1_1_device.html">torch::Device</a> device, bool non_blocking=false)</td></tr>
<tr class="memdesc:abd24812c2304761b85420f4d5edfa828"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recursively moves all parameters to the given device.  <a href="#abd24812c2304761b85420f4d5edfa828">More...</a><br /></td></tr>
<tr class="separator:abd24812c2304761b85420f4d5edfa828"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0ae8830982a7a15bfa6df1dcc7d5e7f3"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a0ae8830982a7a15bfa6df1dcc7d5e7f3"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a0ae8830982a7a15bfa6df1dcc7d5e7f3">zero_grad</a> ()</td></tr>
<tr class="memdesc:a0ae8830982a7a15bfa6df1dcc7d5e7f3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recursively zeros out the <code>grad</code> value of each registered parameter. <br /></td></tr>
<tr class="separator:a0ae8830982a7a15bfa6df1dcc7d5e7f3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab933f8f978b718c513fbc39ab70de97b"><td class="memTemplParams" colspan="2">template&lt;typename ModuleType &gt; </td></tr>
<tr class="memitem:ab933f8f978b718c513fbc39ab70de97b"><td class="memTemplItemLeft" align="right" valign="top">ModuleType::ContainedType *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#ab933f8f978b718c513fbc39ab70de97b">as</a> () noexcept</td></tr>
<tr class="memdesc:ab933f8f978b718c513fbc39ab70de97b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>.  <a href="#ab933f8f978b718c513fbc39ab70de97b">More...</a><br /></td></tr>
<tr class="separator:ab933f8f978b718c513fbc39ab70de97b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7afebbed7aa93eed170c27ba08efce9"><td class="memTemplParams" colspan="2">template&lt;typename ModuleType &gt; </td></tr>
<tr class="memitem:ab7afebbed7aa93eed170c27ba08efce9"><td class="memTemplItemLeft" align="right" valign="top">const ModuleType::ContainedType *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#ab7afebbed7aa93eed170c27ba08efce9">as</a> () const noexcept</td></tr>
<tr class="memdesc:ab7afebbed7aa93eed170c27ba08efce9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>.  <a href="#ab7afebbed7aa93eed170c27ba08efce9">More...</a><br /></td></tr>
<tr class="separator:ab7afebbed7aa93eed170c27ba08efce9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4af0d4c015e6f453f57f078860f607ff"><td class="memTemplParams" colspan="2">template&lt;typename ModuleType , typename  = torch::detail::disable_if_module_holder_t&lt;ModuleType&gt;&gt; </td></tr>
<tr class="memitem:a4af0d4c015e6f453f57f078860f607ff"><td class="memTemplItemLeft" align="right" valign="top">ModuleType *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a4af0d4c015e6f453f57f078860f607ff">as</a> () noexcept</td></tr>
<tr class="memdesc:a4af0d4c015e6f453f57f078860f607ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>.  <a href="#a4af0d4c015e6f453f57f078860f607ff">More...</a><br /></td></tr>
<tr class="separator:a4af0d4c015e6f453f57f078860f607ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1bd17ad80f9b084d2a612142f962bc8a"><td class="memTemplParams" colspan="2">template&lt;typename ModuleType , typename  = torch::detail::disable_if_module_holder_t&lt;ModuleType&gt;&gt; </td></tr>
<tr class="memitem:a1bd17ad80f9b084d2a612142f962bc8a"><td class="memTemplItemLeft" align="right" valign="top">const ModuleType *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a1bd17ad80f9b084d2a612142f962bc8a">as</a> () const noexcept</td></tr>
<tr class="memdesc:a1bd17ad80f9b084d2a612142f962bc8a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>.  <a href="#a1bd17ad80f9b084d2a612142f962bc8a">More...</a><br /></td></tr>
<tr class="separator:a1bd17ad80f9b084d2a612142f962bc8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a46468a45a78135c1749d9f211db3775d"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a46468a45a78135c1749d9f211db3775d"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a46468a45a78135c1749d9f211db3775d">save</a> (<a class="el" href="classtorch_1_1serialize_1_1_output_archive.html">serialize::OutputArchive</a> &amp;archive) const </td></tr>
<tr class="memdesc:a46468a45a78135c1749d9f211db3775d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Serializes the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> into the given <code>OutputArchive</code>. <br /></td></tr>
<tr class="separator:a46468a45a78135c1749d9f211db3775d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3cbd51804b6656e981dbe5da7ccdbb26"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a3cbd51804b6656e981dbe5da7ccdbb26"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a3cbd51804b6656e981dbe5da7ccdbb26">load</a> (<a class="el" href="classtorch_1_1serialize_1_1_input_archive.html">serialize::InputArchive</a> &amp;archive)</td></tr>
<tr class="memdesc:a3cbd51804b6656e981dbe5da7ccdbb26"><td class="mdescLeft">&#160;</td><td class="mdescRight">Deserializes the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> from the given <code>InputArchive</code>. <br /></td></tr>
<tr class="separator:a3cbd51804b6656e981dbe5da7ccdbb26"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a79ca1cb6eed4935f7680758a9c3dbb4d"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a79ca1cb6eed4935f7680758a9c3dbb4d">pretty_print</a> (std::ostream &amp;stream) const </td></tr>
<tr class="memdesc:a79ca1cb6eed4935f7680758a9c3dbb4d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Streams a pretty representation of the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> into the given <code>stream</code>.  <a href="#a79ca1cb6eed4935f7680758a9c3dbb4d">More...</a><br /></td></tr>
<tr class="separator:a79ca1cb6eed4935f7680758a9c3dbb4d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:aab880a8567a7aaff03677207e00bae93"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classat_1_1_tensor.html">Tensor</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#aab880a8567a7aaff03677207e00bae93">register_parameter</a> (std::string <a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403">name</a>, <a class="el" href="classat_1_1_tensor.html">Tensor</a> tensor, bool requires_grad=true)</td></tr>
<tr class="memdesc:aab880a8567a7aaff03677207e00bae93"><td class="mdescLeft">&#160;</td><td class="mdescRight">Registers a parameter with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>.  <a href="#aab880a8567a7aaff03677207e00bae93">More...</a><br /></td></tr>
<tr class="separator:aab880a8567a7aaff03677207e00bae93"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a036e8a4cad8eb1d1253f3de7f355a650"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classat_1_1_tensor.html">Tensor</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a036e8a4cad8eb1d1253f3de7f355a650">register_buffer</a> (std::string <a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403">name</a>, <a class="el" href="classat_1_1_tensor.html">Tensor</a> tensor)</td></tr>
<tr class="memdesc:a036e8a4cad8eb1d1253f3de7f355a650"><td class="mdescLeft">&#160;</td><td class="mdescRight">Registers a buffer with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>.  <a href="#a036e8a4cad8eb1d1253f3de7f355a650">More...</a><br /></td></tr>
<tr class="separator:a036e8a4cad8eb1d1253f3de7f355a650"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a505feb18878e17ed60038c4ed87406f5"><td class="memTemplParams" colspan="2">template&lt;typename ModuleType &gt; </td></tr>
<tr class="memitem:a505feb18878e17ed60038c4ed87406f5"><td class="memTemplItemLeft" align="right" valign="top">std::shared_ptr&lt; ModuleType &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#a505feb18878e17ed60038c4ed87406f5">register_module</a> (std::string <a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403">name</a>, std::shared_ptr&lt; ModuleType &gt; module)</td></tr>
<tr class="memdesc:a505feb18878e17ed60038c4ed87406f5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Registers a submodule with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>.  <a href="#a505feb18878e17ed60038c4ed87406f5">More...</a><br /></td></tr>
<tr class="separator:a505feb18878e17ed60038c4ed87406f5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae21020d776f84f91ebc8679da84c3fc7"><td class="memTemplParams" colspan="2">template&lt;typename ModuleType &gt; </td></tr>
<tr class="memitem:ae21020d776f84f91ebc8679da84c3fc7"><td class="memTemplItemLeft" align="right" valign="top">std::shared_ptr&lt; ModuleType &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#ae21020d776f84f91ebc8679da84c3fc7">register_module</a> (std::string <a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403">name</a>, <a class="el" href="classtorch_1_1nn_1_1_module_holder.html">ModuleHolder</a>&lt; ModuleType &gt; module_holder)</td></tr>
<tr class="memdesc:ae21020d776f84f91ebc8679da84c3fc7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Registers a submodule with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>.  <a href="#ae21020d776f84f91ebc8679da84c3fc7">More...</a><br /></td></tr>
<tr class="separator:ae21020d776f84f91ebc8679da84c3fc7"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="friends"></a>
Friends</h2></td></tr>
<tr class="memitem:a16ae4ef9b7318a77056d0f23eace8111"><td class="memTemplParams" colspan="2"><a class="anchor" id="a16ae4ef9b7318a77056d0f23eace8111"></a>
template&lt;typename Derived &gt; </td></tr>
<tr class="memitem:a16ae4ef9b7318a77056d0f23eace8111"><td class="memTemplItemLeft" align="right" valign="top">class&#160;</td><td class="memTemplItemRight" valign="bottom"><b>Cloneable</b></td></tr>
<tr class="separator:a16ae4ef9b7318a77056d0f23eace8111"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad95b65328d50791f0a1b22eff86e0ae1"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad95b65328d50791f0a1b22eff86e0ae1"></a>
TORCH_API friend std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classtorch_1_1nn_1_1_module.html#ad95b65328d50791f0a1b22eff86e0ae1">operator&lt;&lt;</a> (std::ostream &amp;stream, const <a class="el" href="classtorch_1_1nn_1_1_module.html">nn::Module</a> &amp;module)</td></tr>
<tr class="memdesc:ad95b65328d50791f0a1b22eff86e0ae1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Pretty prints the given <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> into the <code>ostream</code>. <br /></td></tr>
<tr class="separator:ad95b65328d50791f0a1b22eff86e0ae1"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>The base class for all modules in PyTorch. </p>
<p>.. note:: The design and implementation of this class is largely based on the Python API. You may want to consult the python documentation for :py:class:<code>pytorch:<a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">torch.nn.Module</a></code> for further clarification on certain methods or behavior. </p>
<p><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> is an abstraction over the implementation of some function or algorithm, possibly associated with some persistent data. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> may contain further <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>s ("submodules"), each with their own implementation, persistent data and further submodules. <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>s can thus be said to form a recursive tree structure. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> is registered as a submodule to another <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> by calling <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a505feb18878e17ed60038c4ed87406f5" title="Registers a submodule with this Module. ">register_module()</a></code>, typically from within a parent module's constructor.</p>
<p><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> distinction is made between three kinds of persistent data that may be associated with a <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>:</p>
<ol type="1">
<li><em>Parameters</em>: tensors that record gradients, typically weights updated during the backward step (e.g. the <code>weight</code> of a <code>Linear</code> module),</li>
<li><em>Buffers</em>: tensors that do not record gradients, typically updated during the forward step, such as running statistics (e.g. <code>mean</code> and <code>variance</code> in the <code>BatchNorm</code> module),</li>
<li>Any additional state, not necessarily tensors, required for the implementation or configuration of a <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>.</li>
</ol>
<p>The first two kinds of state are special in that they may be registered with the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> system to allow convenient access and batch configuration. For example, registered parameters in any <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> may be iterated over via the <code><a class="el" href="classtorch_1_1nn_1_1_module.html#ae4c84424addf8712d1752566a93d99ad" title="Returns the parameters of this Module and if recurse is true, also recursively of every submodule...">parameters()</a></code> accessor. Further, changing the data type of a <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>'s registered parameters can be done conveniently via <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a9335a64808dda0178374d0818403f88f" title="Recursively casts all parameters to the given dtype and device. ">Module::to()</a></code>, e.g. <code>module-&gt;to(torch::kCUDA)</code> to move all parameters to GPU memory. Lastly, registered parameters and buffers are handled specially during a <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d" title="Performs a recursive deep copy of the module and all its registered parameters, buffers and submodule...">clone()</a></code> operation, which performs a deepcopy of a cloneable <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> hierarchy.</p>
<p>Parameters are registered with a <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> via <code>register_parameter</code>. Buffers are registered separately via <code>register_buffer</code>. These methods are part of the protected API of <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and are typically invoked from within a concrete <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>s constructor. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html#l00062">62</a> of file <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a941f24af291e05af5428e125d20f486a"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::nn::Module::Module </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Constructs the module without immediate knowledge of the submodule's name. </p>
<p>The name of the submodule is inferred via RTTI (if possible) the first time <code>.<a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403" title="Returns the name of the Module. ">name()</a></code> is invoked. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00046">46</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="a3e7bf8192a37c7e6593b5d1dd5d15903"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::apply </td>
          <td>(</td>
          <td class="paramtype">const ModuleApplyFunction &amp;&#160;</td>
          <td class="paramname"><em>function</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule. </p>
<p>The function must accept a <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a>&amp;</code>.</p>
<p>.. code-block:: cpp MyModule module; module-&gt;apply([](<a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">nn::Module</a>&amp; module) { std::cout &lt;&lt; module.name() &lt;&lt; std::endl; });  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00087">87</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a295c66db3d747a25fba77cfe43357701"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::apply </td>
          <td>(</td>
          <td class="paramtype">const ConstModuleApplyFunction &amp;&#160;</td>
          <td class="paramname"><em>function</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule. </p>
<p>The function must accept a <code>const <a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a>&amp;</code>.</p>
<p>.. code-block:: cpp MyModule module; module-&gt;apply([](const <a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">nn::Module</a>&amp; module) { std::cout &lt;&lt; module.name() &lt;&lt; std::endl; });  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00095">95</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="aac037622ec1099f031a284bb8e084832"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::apply </td>
          <td>(</td>
          <td class="paramtype">const NamedModuleApplyFunction &amp;&#160;</td>
          <td class="paramname"><em>function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>name_prefix</em> = <code>std::string()</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule. </p>
<p>The function must accept a <code>const std::string&amp;</code> for the key of the module, and a <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a>&amp;</code>. The key of the module itself is the empty string. If <code>name_prefix</code> is given, it is prepended to every key as <code>&lt;name_prefix&gt;.&lt;key&gt;</code> (and just <code>name_prefix</code> for the module itself).</p>
<p>.. code-block:: cpp MyModule module; module-&gt;apply([](const std::string&amp; key, <a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">nn::Module</a>&amp; module) { std::cout &lt;&lt; key &lt;&lt; ": " &lt;&lt; module.name() &lt;&lt; std::endl; });  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00103">103</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a40c5e885c7de689d6346af6151f05e83"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::apply </td>
          <td>(</td>
          <td class="paramtype">const ConstNamedModuleApplyFunction &amp;&#160;</td>
          <td class="paramname"><em>function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>name_prefix</em> = <code>std::string()</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule. </p>
<p>The function must accept a <code>const std::string&amp;</code> for the key of the module, and a <code>const <a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a>&amp;</code>. The key of the module itself is the empty string. If <code>name_prefix</code> is given, it is prepended to every key as <code>&lt;name_prefix&gt;.&lt;key&gt;</code> (and just <code>name_prefix</code> for the module itself).</p>
<p>.. code-block:: cpp MyModule module; module-&gt;apply([](const std::string&amp; key, const <a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">nn::Module</a>&amp; module) { std::cout &lt;&lt; key &lt;&lt; ": " &lt;&lt; module.name() &lt;&lt; std::endl; });  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00115">115</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a0450302ffa15382970fec94966e47dc8"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::apply </td>
          <td>(</td>
          <td class="paramtype">const ModulePointerApplyFunction &amp;&#160;</td>
          <td class="paramname"><em>function</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule. </p>
<p>The function must accept a <code>const std::shared_ptr&lt;<a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a>&gt;&amp;</code>.</p>
<p>.. code-block:: cpp MyModule module; module-&gt;apply([](const std::shared_ptr&lt;nn::Module&gt;&amp; module) { std::cout &lt;&lt; module-&gt;<a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403" title="Returns the name of the Module. ">name()</a> &lt;&lt; std::endl; });  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00127">127</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a312358768ca495290c0c5541d672a039"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::apply </td>
          <td>(</td>
          <td class="paramtype">const NamedModulePointerApplyFunction &amp;&#160;</td>
          <td class="paramname"><em>function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>name_prefix</em> = <code>std::string()</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the <code>function</code> to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and recursively to every submodule. </p>
<p>The function must accept a <code>const std::string&amp;</code> for the key of the module, and a <code>const std::shared_ptr&lt;<a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a>&gt;&amp;</code>. The key of the module itself is the empty string. If <code>name_prefix</code> is given, it is prepended to every key as <code>&lt;name_prefix&gt;.&lt;key&gt;</code> (and just <code>name_prefix</code> for the module itself).</p>
<p>.. code-block:: cpp MyModule module; module-&gt;apply([](const std::string&amp; key, const std::shared_ptr&lt;nn::Module&gt;&amp; module) { std::cout &lt;&lt; key &lt;&lt; ": " &lt;&lt; module-&gt;<a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403" title="Returns the name of the Module. ">name()</a> &lt;&lt; std::endl; });  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00135">135</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="ab933f8f978b718c513fbc39ab70de97b"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ModuleType &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">ModuleType::ContainedType * torch::nn::Module::as </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>. </p>
<p>This method is useful when calling <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a3e7bf8192a37c7e6593b5d1dd5d15903" title="Applies the function to the Module and recursively to every submodule. ">apply()</a></code>.  .. code-block:: cpp</p>
<p>void initialize_weights(nn::Module&amp; module) { <a class="el" href="structtorch_1_1_no_grad_guard.html">torch::NoGradGuard</a> no_grad; if (auto* linear = module.as&lt;nn::Linear&gt;()) { linear-&gt;weight.normal_(0.0, 0.02); } }</p>
<p>MyModule module; module-&gt;apply(initialize_weights);  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html#l00532">532</a> of file <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>.</p>

</div>
</div>
<a class="anchor" id="ab7afebbed7aa93eed170c27ba08efce9"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ModuleType &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">const ModuleType::ContainedType * torch::nn::Module::as </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>. </p>
<p>This method is useful when calling <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a3e7bf8192a37c7e6593b5d1dd5d15903" title="Applies the function to the Module and recursively to every submodule. ">apply()</a></code>.  .. code-block:: cpp void initialize_weights(nn::Module&amp; module) { <a class="el" href="structtorch_1_1_no_grad_guard.html">torch::NoGradGuard</a> no_grad; if (auto* linear = module.as&lt;nn::Linear&gt;()) { linear-&gt;weight.normal_(0.0, 0.02); } }</p>
<p>MyModule module; module-&gt;apply(initialize_weights);  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html#l00539">539</a> of file <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>.</p>

</div>
</div>
<a class="anchor" id="a4af0d4c015e6f453f57f078860f607ff"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ModuleType , typename &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">ModuleType * torch::nn::Module::as </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>. </p>
<p>This method is useful when calling <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a3e7bf8192a37c7e6593b5d1dd5d15903" title="Applies the function to the Module and recursively to every submodule. ">apply()</a></code>.  .. code-block:: cpp</p>
<p>void initialize_weights(nn::Module&amp; module) { <a class="el" href="structtorch_1_1_no_grad_guard.html">torch::NoGradGuard</a> no_grad; if (auto* linear = module.as&lt;nn::Linear&gt;()) { linear-&gt;weight.normal_(0.0, 0.02); } }</p>
<p>MyModule module; module.apply(initialize_weights);  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html#l00546">546</a> of file <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>.</p>

</div>
</div>
<a class="anchor" id="a1bd17ad80f9b084d2a612142f962bc8a"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ModuleType , typename &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">const ModuleType * torch::nn::Module::as </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Attempts to cast this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> to the given <code>ModuleType</code>. </p>
<p>This method is useful when calling <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a3e7bf8192a37c7e6593b5d1dd5d15903" title="Applies the function to the Module and recursively to every submodule. ">apply()</a></code>.  .. code-block:: cpp</p>
<p>void initialize_weights(nn::Module&amp; module) { <a class="el" href="structtorch_1_1_no_grad_guard.html">torch::NoGradGuard</a> no_grad; if (auto* linear = module.as&lt;nn::Linear&gt;()) { linear-&gt;weight.normal_(0.0, 0.02); } }</p>
<p>MyModule module; module.apply(initialize_weights);  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html#l00551">551</a> of file <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>.</p>

</div>
</div>
<a class="anchor" id="a92cc52f55f49f84f9d2b277ff4816c95"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt; torch::nn::Module::buffers </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>recurse</em> = <code>true</code></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the buffers of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and if <code>recurse</code> is true, also recursively of every submodule. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00166">166</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a1e9244756b2265ad97c716cc7f885d4d"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; torch::nn::Module::clone </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classc10_1_1optional.html">optional</a>&lt; <a class="el" href="structc10_1_1_device.html">Device</a> &gt; &amp;&#160;</td>
          <td class="paramname"><em>device</em> = <code>nullopt</code></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs a recursive deep copy of the module and all its registered parameters, buffers and submodules. </p>
<p>Optionally, this method sets the current device to the one supplied before cloning. If no device is given, each parameter and buffer will be moved to the device of its source.</p>
<p>.. attention:: Attempting to call the <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d" title="Performs a recursive deep copy of the module and all its registered parameters, buffers and submodule...">clone()</a></code> method inherited from the base <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> class (the one documented here) will fail. To inherit an actual implementation of <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d" title="Performs a recursive deep copy of the module and all its registered parameters, buffers and submodule...">clone()</a></code>, you must subclass <code><a class="el" href="classtorch_1_1nn_1_1_cloneable.html" title="The clone() method in the base Module class does not have knowledge of the concrete runtime type of i...">Cloneable</a></code>. <code><a class="el" href="classtorch_1_1nn_1_1_cloneable.html" title="The clone() method in the base Module class does not have knowledge of the concrete runtime type of i...">Cloneable</a></code> is templatized on the concrete module type, and can thus properly copy a <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. This method is provided on the base class' API solely for an easier-to-use polymorphic interface.  </p>

<p>Reimplemented in <a class="el" href="classtorch_1_1nn_1_1_sequential_impl.html#ae54370d79a4fd2946620a72b7ba41f5f">torch::nn::SequentialImpl</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; Derived &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; BatchNormImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; SimpleContainer &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; SequentialImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; Conv2dImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; RNNImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; Conv3dImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; LSTMImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; Net &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; EmbeddingImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; LinearImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; DropoutImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; FeatureDropoutImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; Conv1dImpl &gt;</a>, and <a class="el" href="classtorch_1_1nn_1_1_cloneable.html#a0d49912e01a7fb312f3f52a100da50d4">torch::nn::Cloneable&lt; GRUImpl &gt;</a>.</p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00078">78</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="af0be79d2e17a200b5f69023ba6f02598"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::eval </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Calls train(false) to enable "eval" mode. </p>
<p>Do not override this method, override <code><a class="el" href="classtorch_1_1nn_1_1_module.html#ab7df32818dc554d4c59f82bfdf3248fd" title="Enables &quot;training&quot; mode. ">train()</a></code> instead. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00240">240</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a0489238ce8594bf93c210e36e971d314"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool torch::nn::Module::is_training </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if the module is in training mode. </p>
<p>Every <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> has a boolean associated with it that determines whether the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> is currently in <em>training</em> mode (set via <code>.<a class="el" href="classtorch_1_1nn_1_1_module.html#ab7df32818dc554d4c59f82bfdf3248fd" title="Enables &quot;training&quot; mode. ">train()</a></code>) or in <em>evaluation</em> (inference) mode (set via <code>.<a class="el" href="classtorch_1_1nn_1_1_module.html#af0be79d2e17a200b5f69023ba6f02598" title="Calls train(false) to enable &quot;eval&quot; mode. ">eval()</a></code>). This property is exposed via <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a0489238ce8594bf93c210e36e971d314" title="True if the module is in training mode. ">is_training()</a></code>, and may be used by the implementation of a concrete module to modify its runtime behavior. See the <code>BatchNorm</code> or <code>Dropout</code> modules for examples of <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>s that use different code paths depending on this property. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00256">256</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a284627aba1381c6db639b62207cdd560"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &gt; torch::nn::Module::modules </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>include_self</em> = <code>true</code></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the submodules of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> (the entire submodule hierarchy) and if <code>include_self</code> is true, also inserts a <code>shared_ptr</code> to this module in the first position. </p>
<p>.. warning:: Only pass <code>include_self</code> as <code>true</code> if this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> is stored in a <code>shared_ptr</code>! Otherwise an exception will be thrown. You may still call this method with <code>include_self</code> set to false if your <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> is not stored in a <code>shared_ptr</code>.  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00187">187</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="ab503bb4fdf163c00ac1e9fc695f55403"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">const std::string &amp; torch::nn::Module::name </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the name of the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. </p>
<p><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> has an associated <code>name</code>, which is a string representation of the kind of concrete <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> it represents, such as <code>"Linear"</code> for the <code>Linear</code> module. Under most circumstances, this name is automatically inferred via runtime type information (RTTI). In the unusual circumstance that you have this feature disabled, you may want to manually name your <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>s by passing the string name to the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> base class' constructor. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00053">53</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="aece5594a6fc5b452208d81cbe828e557"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt; torch::nn::Module::named_buffers </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>recurse</em> = <code>true</code></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> with the buffers of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> along with their keys, and if <code>recurse</code> is true also recursively of every submodule. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00174">174</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a1ffc35c81a390ab5d7f1e3cea318aae0"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &gt; torch::nn::Module::named_children </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> of the direct submodules of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and their keys. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00228">228</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a0f82eaf10c89119841658ba26d9ec4fa"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, std::shared_ptr&lt; <a class="el" href="classtorch_1_1nn_1_1_module.html">Module</a> &gt; &gt; torch::nn::Module::named_modules </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>name_prefix</em> = <code>std::string()</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>include_self</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> of he submodules of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> (the entire submodule hierarchy) and thei keys, and if <code>include_self</code> is true, also inserts a <code>shared_ptr</code> to this module in the first position. </p>
<p>If <code>name_prefix</code> is given, it is prepended to every key as <code>&lt;name_prefix&gt;.&lt;key&gt;</code> (and just <code>name_prefix</code> for the module itself).</p>
<p>.. warning:: Only pass <code>include_self</code> as <code>true</code> if this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> is stored in a <code>shared_ptr</code>! Otherwise an exception will be thrown. You may still call this method with <code>include_self</code> set to false if your <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> is not stored in a <code>shared_ptr</code>.  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00202">202</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a3efdc049d212c8f49cd45831eb5effdb"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classtorch_1_1_ordered_dict.html">OrderedDict</a>&lt; std::string, <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt; torch::nn::Module::named_parameters </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>recurse</em> = <code>true</code></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns an <code><a class="el" href="classtorch_1_1_ordered_dict.html" title="An ordered dictionary implementation, akin to Python&#39;s OrderedDict. ">OrderedDict</a></code> with the parameters of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> along with their keys, and if <code>recurse</code> is true also recursively of every submodule. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00153">153</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="ae4c84424addf8712d1752566a93d99ad"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; <a class="el" href="classat_1_1_tensor.html">Tensor</a> &gt; torch::nn::Module::parameters </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>recurse</em> = <code>true</code></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns the parameters of this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> and if <code>recurse</code> is true, also recursively of every submodule. </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00143">143</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a79ca1cb6eed4935f7680758a9c3dbb4d"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::pretty_print </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;&#160;</td>
          <td class="paramname"><em>stream</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Streams a pretty representation of the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code> into the given <code>stream</code>. </p>
<p>By default, this representation will be the name of the module (taken from <code><a class="el" href="classtorch_1_1nn_1_1_module.html#ab503bb4fdf163c00ac1e9fc695f55403" title="Returns the name of the Module. ">name()</a></code>), followed by a recursive pretty print of all of the <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>'s submodules.</p>
<p>Override this method to change the pretty print. The input <code>stream</code> should be returned from the method, to allow easy chaining. </p>

<p>Reimplemented in <a class="el" href="classtorch_1_1nn_1_1_r_n_n_impl.html#a7174bbd78c854e46c1f227b7240e1967">torch::nn::RNNImpl</a>, <a class="el" href="classtorch_1_1nn_1_1_sequential_impl.html#a2cabe1fd2c4ce632db9749d2becee8c7">torch::nn::SequentialImpl</a>, <a class="el" href="classtorch_1_1nn_1_1_conv_impl.html#a0813ee36cb94974cad81461a06843028">torch::nn::ConvImpl&lt; D, Derived &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_conv_impl.html#a0813ee36cb94974cad81461a06843028">torch::nn::ConvImpl&lt; 1, Conv1dImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_conv_impl.html#a0813ee36cb94974cad81461a06843028">torch::nn::ConvImpl&lt; 3, Conv3dImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_conv_impl.html#a0813ee36cb94974cad81461a06843028">torch::nn::ConvImpl&lt; 2, Conv2dImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab1e8e0e5b515e60cf6d91a1536d36f75">torch::nn::detail::RNNImplBase&lt; Derived &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab1e8e0e5b515e60cf6d91a1536d36f75">torch::nn::detail::RNNImplBase&lt; RNNImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab1e8e0e5b515e60cf6d91a1536d36f75">torch::nn::detail::RNNImplBase&lt; LSTMImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab1e8e0e5b515e60cf6d91a1536d36f75">torch::nn::detail::RNNImplBase&lt; GRUImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1_feature_dropout_impl.html#a66cac12d0f4877ec6a8fa061cec013c5">torch::nn::FeatureDropoutImpl</a>, <a class="el" href="classtorch_1_1nn_1_1_batch_norm_impl.html#a3c6893850361ae6886e37397a6901583">torch::nn::BatchNormImpl</a>, <a class="el" href="classtorch_1_1nn_1_1_dropout_impl.html#a1c3ff740af94f9bd0df2ec0e57bcdd0a">torch::nn::DropoutImpl</a>, <a class="el" href="classtorch_1_1nn_1_1_linear_impl.html#afd7d95c2b884e0330b83348477f054df">torch::nn::LinearImpl</a>, and <a class="el" href="classtorch_1_1nn_1_1_embedding_impl.html#a8613b5d8fe0e7518cfa61a907adfa3bf">torch::nn::EmbeddingImpl</a>.</p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00325">325</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a036e8a4cad8eb1d1253f3de7f355a650"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classat_1_1_tensor.html">Tensor</a> &amp; torch::nn::Module::register_buffer </td>
          <td>(</td>
          <td class="paramtype">std::string&#160;</td>
          <td class="paramname"><em>name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classat_1_1_tensor.html">Tensor</a>&#160;</td>
          <td class="paramname"><em>tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Registers a buffer with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. </p>
<p><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> buffer is intended to be state in your module that does not record gradients, such as running statistics. Registering it makes it available to methods such as <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a92cc52f55f49f84f9d2b277ff4816c95" title="Returns the buffers of this Module and if recurse is true, also recursively of every submodule...">buffers()</a></code>, <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d" title="Performs a recursive deep copy of the module and all its registered parameters, buffers and submodule...">clone()</a></code> or `to().</p>
<p>.. code-block:: cpp</p>
<p>MyModule::MyModule() { mean_ = register_buffer("mean", torch::empty({num_features_})); }  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00315">315</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a505feb18878e17ed60038c4ed87406f5"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ModuleType &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::shared_ptr&lt; ModuleType &gt; torch::nn::Module::register_module </td>
          <td>(</td>
          <td class="paramtype">std::string&#160;</td>
          <td class="paramname"><em>name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::shared_ptr&lt; ModuleType &gt;&#160;</td>
          <td class="paramname"><em>module</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Registers a submodule with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. </p>
<p>Registering a module makes it available to methods such as <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a284627aba1381c6db639b62207cdd560" title="Returns the submodules of this Module (the entire submodule hierarchy) and if include_self is true...">modules()</a></code>, <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d" title="Performs a recursive deep copy of the module and all its registered parameters, buffers and submodule...">clone()</a></code> or <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a9335a64808dda0178374d0818403f88f" title="Recursively casts all parameters to the given dtype and device. ">to()</a></code>.</p>
<p>.. code-block:: cpp</p>
<p>MyModule::MyModule() { submodule_ = register_module("linear", torch::nn::Linear(3, 4)); }  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html#l00556">556</a> of file <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>.</p>

</div>
</div>
<a class="anchor" id="ae21020d776f84f91ebc8679da84c3fc7"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ModuleType &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::shared_ptr&lt; ModuleType &gt; torch::nn::Module::register_module </td>
          <td>(</td>
          <td class="paramtype">std::string&#160;</td>
          <td class="paramname"><em>name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classtorch_1_1nn_1_1_module_holder.html">ModuleHolder</a>&lt; ModuleType &gt;&#160;</td>
          <td class="paramname"><em>module_holder</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Registers a submodule with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. </p>
<p>This method deals with <code><a class="el" href="classtorch_1_1nn_1_1_module_holder.html" title="A ModuleHolder is essentially a wrapper around std::shared_ptr&lt;M&gt; where M is an nn::Module subclass...">ModuleHolder</a></code>s.</p>
<p>Registering a module makes it available to methods such as <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a284627aba1381c6db639b62207cdd560" title="Returns the submodules of this Module (the entire submodule hierarchy) and if include_self is true...">modules()</a></code>, <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d" title="Performs a recursive deep copy of the module and all its registered parameters, buffers and submodule...">clone()</a></code> or <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a9335a64808dda0178374d0818403f88f" title="Recursively casts all parameters to the given dtype and device. ">to()</a></code>.</p>
<p>.. code-block:: cpp</p>
<p>MyModule::MyModule() { submodule_ = register_module("linear", torch::nn::Linear(3, 4)); }  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html#l00570">570</a> of file <a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a>.</p>

</div>
</div>
<a class="anchor" id="aab880a8567a7aaff03677207e00bae93"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classat_1_1_tensor.html">Tensor</a> &amp; torch::nn::Module::register_parameter </td>
          <td>(</td>
          <td class="paramtype">std::string&#160;</td>
          <td class="paramname"><em>name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classat_1_1_tensor.html">Tensor</a>&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>requires_grad</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Registers a parameter with this <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. </p>
<p><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> parameter should be any gradient-recording tensor used in the implementation of your <code><a class="el" href="classtorch_1_1nn_1_1_module.html" title="The base class for all modules in PyTorch. ">Module</a></code>. Registering it makes it available to methods such as <code><a class="el" href="classtorch_1_1nn_1_1_module.html#ae4c84424addf8712d1752566a93d99ad" title="Returns the parameters of this Module and if recurse is true, also recursively of every submodule...">parameters()</a></code>, <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a1e9244756b2265ad97c716cc7f885d4d" title="Performs a recursive deep copy of the module and all its registered parameters, buffers and submodule...">clone()</a></code> or <code><a class="el" href="classtorch_1_1nn_1_1_module.html#a9335a64808dda0178374d0818403f88f" title="Recursively casts all parameters to the given dtype and device. ">to()</a>.</code></p>
<p>.. code-block:: cpp</p>
<p>MyModule::MyModule() { weight_ = register_parameter("weight", torch::randn({<a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a>, <a class="el" href="struct_b.html">B</a>})); }  </p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00301">301</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a9335a64808dda0178374d0818403f88f"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::to </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="structc10_1_1_device.html">torch::Device</a>&#160;</td>
          <td class="paramname"><em>device</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Dtype&#160;</td>
          <td class="paramname"><em>dtype</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>non_blocking</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Recursively casts all parameters to the given <code>dtype</code> and <code>device</code>. </p>
<p>If <code>non_blocking</code> is true and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. </p>

<p>Reimplemented in <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab43e1c869ddbc2f08a947ace0651298e">torch::nn::detail::RNNImplBase&lt; Derived &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab43e1c869ddbc2f08a947ace0651298e">torch::nn::detail::RNNImplBase&lt; RNNImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab43e1c869ddbc2f08a947ace0651298e">torch::nn::detail::RNNImplBase&lt; LSTMImpl &gt;</a>, and <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#ab43e1c869ddbc2f08a947ace0651298e">torch::nn::detail::RNNImplBase&lt; GRUImpl &gt;</a>.</p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00244">244</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a06916399bc5c0270d3db18153b4a0ad8"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::to </td>
          <td>(</td>
          <td class="paramtype">torch::Dtype&#160;</td>
          <td class="paramname"><em>dtype</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>non_blocking</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Recursively casts all parameters to the given dtype. </p>
<p>If <code>non_blocking</code> is true and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. </p>

<p>Reimplemented in <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aa613dc592204256410bec59137b09d87">torch::nn::detail::RNNImplBase&lt; Derived &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aa613dc592204256410bec59137b09d87">torch::nn::detail::RNNImplBase&lt; RNNImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aa613dc592204256410bec59137b09d87">torch::nn::detail::RNNImplBase&lt; LSTMImpl &gt;</a>, and <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aa613dc592204256410bec59137b09d87">torch::nn::detail::RNNImplBase&lt; GRUImpl &gt;</a>.</p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00248">248</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="abd24812c2304761b85420f4d5edfa828"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void torch::nn::Module::to </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="structc10_1_1_device.html">torch::Device</a>&#160;</td>
          <td class="paramname"><em>device</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>non_blocking</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Recursively moves all parameters to the given device. </p>
<p>If <code>non_blocking</code> is true and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. </p>

<p>Reimplemented in <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aba6d742c9c97af668cb661e5ee488bdf">torch::nn::detail::RNNImplBase&lt; Derived &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aba6d742c9c97af668cb661e5ee488bdf">torch::nn::detail::RNNImplBase&lt; RNNImpl &gt;</a>, <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aba6d742c9c97af668cb661e5ee488bdf">torch::nn::detail::RNNImplBase&lt; LSTMImpl &gt;</a>, and <a class="el" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html#aba6d742c9c97af668cb661e5ee488bdf">torch::nn::detail::RNNImplBase&lt; GRUImpl &gt;</a>.</p>

<p>Definition at line <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html#l00252">252</a> of file <a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>torch/csrc/api/include/torch/nn/<a class="el" href="torch_2csrc_2api_2include_2torch_2nn_2module_8h_source.html">module.h</a></li>
<li>torch/csrc/api/src/nn/<a class="el" href="torch_2csrc_2api_2src_2nn_2module_8cpp_source.html">module.cpp</a></li>
</ul>
</div><!-- contents -->
<!-- HTML footer for doxygen 1.8.14-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Thu Mar 21 2019 13:06:32 for Caffe2 - C++ API by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
<div class="footerContainer">
  <div id="footer_wrap" class="wrapper footerWrapper">
    <div class="footerBlocks">
      <div id="fb_oss" class="footerSection fbOpenSourceFooter">
          <svg class="facebookOSSLogoSvg" viewBox="0 0 1133.9 1133.9" x="0px" y="0px" height=50 width=50>
            <g>
              <path class="logoRing outerRing" d="M 498.3 3.7 c 153.6 88.9 307.3 177.7 461.1 266.2 c 7.6 4.4 10.3 9.1 10.3 17.8 c -0.3 179.1 -0.2 358.3 0 537.4 c 0 8.1 -2.4 12.8 -9.7 17.1 c -154.5 88.9 -308.8 178.1 -462.9 267.5 c -9 5.2 -15.5 5.3 -24.6 0.1 c -153.9 -89.2 -307.9 -178 -462.1 -266.8 C 3 838.8 0 833.9 0 825.1 c 0.3 -179.1 0.2 -358.3 0 -537.4 c 0 -8.6 2.6 -13.6 10.2 -18 C 164.4 180.9 318.4 92 472.4 3 C 477 -1.5 494.3 -0.7 498.3 3.7 Z M 48.8 555.3 c 0 79.9 0.2 159.9 -0.2 239.8 c -0.1 10 3 15.6 11.7 20.6 c 137.2 78.8 274.2 157.8 411 237.3 c 9.9 5.7 17 5.7 26.8 0.1 c 137.5 -79.8 275.2 -159.2 412.9 -238.5 c 7.4 -4.3 10.5 -8.9 10.5 -17.8 c -0.3 -160.2 -0.3 -320.5 0 -480.7 c 0 -8.8 -2.8 -13.6 -10.3 -18 C 772.1 218 633.1 137.8 494.2 57.4 c -6.5 -3.8 -11.5 -4.5 -18.5 -0.5 C 336.8 137.4 197.9 217.7 58.8 297.7 c -7.7 4.4 -10.2 9.2 -10.2 17.9 C 48.9 395.5 48.8 475.4 48.8 555.3 Z" />
              <path class="logoRing middleRing" d="M 184.4 555.9 c 0 -33.3 -1 -66.7 0.3 -100 c 1.9 -48 24.1 -86 64.7 -110.9 c 54.8 -33.6 110.7 -65.5 167 -96.6 c 45.7 -25.2 92.9 -24.7 138.6 1 c 54.4 30.6 108.7 61.5 162.2 93.7 c 44 26.5 67.3 66.8 68 118.4 c 0.9 63.2 0.9 126.5 0 189.7 c -0.7 50.6 -23.4 90.7 -66.6 116.9 c -55 33.4 -110.8 65.4 -167.1 96.5 c -43.4 24 -89 24.2 -132.3 0.5 c -57.5 -31.3 -114.2 -64 -170 -98.3 c -41 -25.1 -62.9 -63.7 -64.5 -112.2 C 183.5 621.9 184.3 588.9 184.4 555.9 Z M 232.9 556.3 c 0 29.5 0.5 59.1 -0.1 88.6 c -0.8 39.2 16.9 67.1 50.2 86.2 c 51.2 29.4 102.2 59.2 153.4 88.4 c 31.4 17.9 63.6 18.3 95 0.6 c 53.7 -30.3 107.1 -61.2 160.3 -92.5 c 29.7 -17.5 45 -44.5 45.3 -78.8 c 0.6 -61.7 0.5 -123.5 0 -185.2 c -0.3 -34.4 -15.3 -61.5 -44.9 -79 C 637.7 352.6 583 320.8 527.9 290 c -27.5 -15.4 -57.2 -16.1 -84.7 -0.7 c -56.9 31.6 -113.4 64 -169.1 97.6 c -26.4 15.9 -40.7 41.3 -41.1 72.9 C 232.6 491.9 232.9 524.1 232.9 556.3 Z" />
              <path class="logoRing innerRing" d="M 484.9 424.4 c 69.8 -2.8 133.2 57.8 132.6 132 C 617 630 558.5 688.7 484.9 689.1 c -75.1 0.4 -132.6 -63.6 -132.7 -132.7 C 352.1 485 413.4 421.5 484.9 424.4 Z M 401.3 556.7 c -3.4 37.2 30.5 83.6 83 84.1 c 46.6 0.4 84.8 -37.6 84.9 -84 c 0.1 -46.6 -37.2 -84.4 -84.2 -84.6 C 432.2 472.1 397.9 518.3 401.3 556.7 Z" />
            </g>
          </svg>
        <h2>Facebook Open Source</h2>
      </div>
      <div class="footerSection">
        <a class="footerLink" href="https://code.facebook.com/projects/" target="_blank">Open Source Projects</a>
        <a class="footerLink" href="https://github.com/facebook/" target="_blank">GitHub</a>
        <a class="footerLink" href="https://twitter.com/fbOpenSource" target="_blank">Twitter</a>
      </div>
      <div class="footerSection rightAlign">
        <a class="footerLink" href="https://github.com/caffe2/caffe2" target="_blank">Contribute to this project on GitHub</a>
      </div>
    </div>
  </div>
</div>
<script type="text/javascript" src="/js/jekyll-link-anchors.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '{{ site.gacode }}', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
