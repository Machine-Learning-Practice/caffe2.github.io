<!-- HTML header for doxygen 1.8.14-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Caffe2 - C++ API: c10::TensorImpl Struct Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="/static/favicon.png" type="image/x-icon">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
<link href="main.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo" width="56"><a href="/"><img alt="Logo" src="Caffe2-with-name-55-tall.png"/></a></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Caffe2 - C++ API
   </div>
   <div id="projectbrief">A deep learning, cross platform ML framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li class="current"><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="/doxygen-c/html/classes.html"><span>C++&#160;API</span></a></li>
      <li><a href="/doxygen-python/html/annotated.html"><span>Python&#160;API</span></a></li>
      <li><a href="https://github.com/caffe2/caffe2"><span>GitHub</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li><a href="classes.html"><span>Data&#160;Structure&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Data&#160;Fields</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacec10.html">c10</a></li><li class="navelem"><a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a>  </div>
  <div class="headertitle">
<div class="title">c10::TensorImpl Struct Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>The low-level representation of a tensor, which contains a pointer to a storage (which contains the actual data) and metadata (e.g., sizes and strides) describing this particular view of the data as a tensor.  
 <a href="structc10_1_1_tensor_impl.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for c10::TensorImpl:</div>
<div class="dyncontent">
 <div class="center">
  <img src="structc10_1_1_tensor_impl.png" usemap="#c10::TensorImpl_map" alt=""/>
  <map id="c10::TensorImpl_map" name="c10::TensorImpl_map">
<area href="classc10_1_1intrusive__ptr__target.html" title="intrusive_ptr&lt;T&gt; is an alternative to shared_ptr&lt;T&gt; that has better performance because it does the r..." alt="c10::intrusive_ptr_target" shape="rect" coords="167,0,324,24"/>
<area href="structat_1_1_sparse_tensor_impl.html" alt="at::SparseTensorImpl" shape="rect" coords="0,112,157,136"/>
<area href="structc10_1_1_undefined_tensor_impl.html" alt="c10::UndefinedTensorImpl" shape="rect" coords="167,112,324,136"/>
<area href="structtorch_1_1jit_1_1_container_tensor.html" alt="torch::jit::ContainerTensor" shape="rect" coords="334,112,491,136"/>
</map>
 </div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a70af485166cfc35904dd7f22f5bcd430"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a70af485166cfc35904dd7f22f5bcd430">TensorImpl</a> (<a class="el" href="classc10_1_1_tensor_type_id.html">TensorTypeId</a> <a class="el" href="structc10_1_1_tensor_impl.html#a3fff488761f6870393167667d0496f9d">type_id</a>, const <a class="el" href="classcaffe2_1_1_type_meta.html">caffe2::TypeMeta</a> &amp;data_type, <a class="el" href="structc10_1_1_allocator.html">Allocator</a> *allocator, bool <a class="el" href="structc10_1_1_tensor_impl.html#a765410a53c2fce051a62c303632c0e21">is_variable</a>)</td></tr>
<tr class="memdesc:a70af485166cfc35904dd7f22f5bcd430"><td class="mdescLeft">&#160;</td><td class="mdescRight">Construct a 1-dim 0-size tensor with the given settings.  <a href="#a70af485166cfc35904dd7f22f5bcd430">More...</a><br /></td></tr>
<tr class="separator:a70af485166cfc35904dd7f22f5bcd430"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a19a25f65fd50b02d702c9b8efbb9691d"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a19a25f65fd50b02d702c9b8efbb9691d"></a>
&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a19a25f65fd50b02d702c9b8efbb9691d">TensorImpl</a> (<a class="el" href="structc10_1_1_storage.html">Storage</a> &amp;&amp;<a class="el" href="structc10_1_1_tensor_impl.html#a80116b4ec040c911734dbabc7db68f5d">storage</a>, <a class="el" href="classc10_1_1_tensor_type_id.html">TensorTypeId</a> <a class="el" href="structc10_1_1_tensor_impl.html#a3fff488761f6870393167667d0496f9d">type_id</a>, bool <a class="el" href="structc10_1_1_tensor_impl.html#a765410a53c2fce051a62c303632c0e21">is_variable</a>)</td></tr>
<tr class="memdesc:a19a25f65fd50b02d702c9b8efbb9691d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Construct a 1-dim 0-size tensor backed by the given storage. <br /></td></tr>
<tr class="separator:a19a25f65fd50b02d702c9b8efbb9691d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a16483cca0c56088f439c9e23ce4bf543"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a16483cca0c56088f439c9e23ce4bf543"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>TensorImpl</b> (const <a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;)=delete</td></tr>
<tr class="separator:a16483cca0c56088f439c9e23ce4bf543"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a526d8d273e784512de1a3c6a688ac2ff"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a526d8d273e784512de1a3c6a688ac2ff"></a>
<a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><b>operator=</b> (const <a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;)=delete</td></tr>
<tr class="separator:a526d8d273e784512de1a3c6a688ac2ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5c5d99c80adfb6350dd8148411b75186"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a5c5d99c80adfb6350dd8148411b75186"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>TensorImpl</b> (<a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;&amp;)=default</td></tr>
<tr class="separator:a5c5d99c80adfb6350dd8148411b75186"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace6d8d38d379c56c6e523736ba4e69e4"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ace6d8d38d379c56c6e523736ba4e69e4"></a>
<a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><b>operator=</b> (<a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;&amp;)=default</td></tr>
<tr class="separator:ace6d8d38d379c56c6e523736ba4e69e4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8e086291ae4d78f03ddb6ec12b1c03b7"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a8e086291ae4d78f03ddb6ec12b1c03b7">release_resources</a> () override</td></tr>
<tr class="memdesc:a8e086291ae4d78f03ddb6ec12b1c03b7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Release (decref) storage, and any other external allocations.  <a href="#a8e086291ae4d78f03ddb6ec12b1c03b7">More...</a><br /></td></tr>
<tr class="separator:a8e086291ae4d78f03ddb6ec12b1c03b7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3fff488761f6870393167667d0496f9d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classc10_1_1_tensor_type_id.html">TensorTypeId</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a3fff488761f6870393167667d0496f9d">type_id</a> () const </td></tr>
<tr class="memdesc:a3fff488761f6870393167667d0496f9d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the <a class="el" href="classc10_1_1_tensor_type_id.html" title="Dynamic type ID of a Tensor argument. ">TensorTypeId</a> corresponding to this <a class="el" href="struct_tensor.html">Tensor</a>.  <a href="#a3fff488761f6870393167667d0496f9d">More...</a><br /></td></tr>
<tr class="separator:a3fff488761f6870393167667d0496f9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2aee10a1a2468955724a364e883e5302"><td class="memItemLeft" align="right" valign="top">virtual <a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a2aee10a1a2468955724a364e883e5302">sizes</a> () const </td></tr>
<tr class="memdesc:a2aee10a1a2468955724a364e883e5302"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return a reference to the sizes of this tensor.  <a href="#a2aee10a1a2468955724a364e883e5302">More...</a><br /></td></tr>
<tr class="separator:a2aee10a1a2468955724a364e883e5302"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af5e15648f034f38ff5b9d5210f893298"><td class="memItemLeft" align="right" valign="top">virtual <a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#af5e15648f034f38ff5b9d5210f893298">strides</a> () const </td></tr>
<tr class="memdesc:af5e15648f034f38ff5b9d5210f893298"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return a reference to the strides of this tensor.  <a href="#af5e15648f034f38ff5b9d5210f893298">More...</a><br /></td></tr>
<tr class="separator:af5e15648f034f38ff5b9d5210f893298"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a665ea0a74febe6b47de99d597355367a"><td class="memItemLeft" align="right" valign="top">virtual int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a665ea0a74febe6b47de99d597355367a">dim</a> () const </td></tr>
<tr class="memdesc:a665ea0a74febe6b47de99d597355367a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the number of dimensions of this tensor.  <a href="#a665ea0a74febe6b47de99d597355367a">More...</a><br /></td></tr>
<tr class="separator:a665ea0a74febe6b47de99d597355367a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f870f592c567d083ac3fc8072e36ff1"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a4f870f592c567d083ac3fc8072e36ff1">has_storage</a> () const </td></tr>
<tr class="memdesc:a4f870f592c567d083ac3fc8072e36ff1"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if this tensor has storage.  <a href="#a4f870f592c567d083ac3fc8072e36ff1">More...</a><br /></td></tr>
<tr class="separator:a4f870f592c567d083ac3fc8072e36ff1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80116b4ec040c911734dbabc7db68f5d"><td class="memItemLeft" align="right" valign="top">virtual const <a class="el" href="structc10_1_1_storage.html">Storage</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a80116b4ec040c911734dbabc7db68f5d">storage</a> () const </td></tr>
<tr class="memdesc:a80116b4ec040c911734dbabc7db68f5d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the underlying storage of a <a class="el" href="struct_tensor.html">Tensor</a>.  <a href="#a80116b4ec040c911734dbabc7db68f5d">More...</a><br /></td></tr>
<tr class="separator:a80116b4ec040c911734dbabc7db68f5d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8e3b32d6afc37de6fbceda4556d03617"><td class="memItemLeft" align="right" valign="top">virtual int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a8e3b32d6afc37de6fbceda4556d03617">numel</a> () const </td></tr>
<tr class="memdesc:a8e3b32d6afc37de6fbceda4556d03617"><td class="mdescLeft">&#160;</td><td class="mdescRight">The number of elements in a tensor.  <a href="#a8e3b32d6afc37de6fbceda4556d03617">More...</a><br /></td></tr>
<tr class="separator:a8e3b32d6afc37de6fbceda4556d03617"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0a92e41852fe98ad115713b90575ced"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#af0a92e41852fe98ad115713b90575ced">is_contiguous</a> () const </td></tr>
<tr class="memdesc:af0a92e41852fe98ad115713b90575ced"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether or not a tensor is laid out in contiguous memory.  <a href="#af0a92e41852fe98ad115713b90575ced">More...</a><br /></td></tr>
<tr class="separator:af0a92e41852fe98ad115713b90575ced"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adda17cd54f7e55cf602e5fd1ee8d84ef"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="adda17cd54f7e55cf602e5fd1ee8d84ef"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>is_sparse</b> () const </td></tr>
<tr class="separator:adda17cd54f7e55cf602e5fd1ee8d84ef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aee7f9e6895fb665e92432c9a58e0c1d7"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aee7f9e6895fb665e92432c9a58e0c1d7"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>is_cuda</b> () const </td></tr>
<tr class="separator:aee7f9e6895fb665e92432c9a58e0c1d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad16c3a27011110e50a8861f4eaca00d5"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad16c3a27011110e50a8861f4eaca00d5"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>is_hip</b> () const </td></tr>
<tr class="separator:ad16c3a27011110e50a8861f4eaca00d5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac293dc46af693229ff3cc89c02caaa66"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ac293dc46af693229ff3cc89c02caaa66"></a>
int64_t&#160;</td><td class="memItemRight" valign="bottom"><b>get_device</b> () const </td></tr>
<tr class="separator:ac293dc46af693229ff3cc89c02caaa66"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad79c50b2f75eed7f2d3925e8ed12b4f5"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad79c50b2f75eed7f2d3925e8ed12b4f5"></a>
<a class="el" href="structc10_1_1_device.html">Device</a>&#160;</td><td class="memItemRight" valign="bottom"><b>device</b> () const </td></tr>
<tr class="separator:ad79c50b2f75eed7f2d3925e8ed12b4f5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1fd766de1f2d54d9b5184efa15e1ab5b"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a1fd766de1f2d54d9b5184efa15e1ab5b"></a>
Layout&#160;</td><td class="memItemRight" valign="bottom"><b>layout</b> () const </td></tr>
<tr class="separator:a1fd766de1f2d54d9b5184efa15e1ab5b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae3f850babf46aa6d1c746b70f57cfad5"><td class="memItemLeft" align="right" valign="top">virtual <a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#ae3f850babf46aa6d1c746b70f57cfad5">maybe_zero_dim</a> (bool condition_when_zero_dim)</td></tr>
<tr class="memdesc:ae3f850babf46aa6d1c746b70f57cfad5"><td class="mdescLeft">&#160;</td><td class="mdescRight">If <code>condition_when_zero_dim</code> is true, and the tensor is a 1-dim, 1-size tensor, reshape the tensor into a 0-dim tensor (scalar).  <a href="#ae3f850babf46aa6d1c746b70f57cfad5">More...</a><br /></td></tr>
<tr class="separator:ae3f850babf46aa6d1c746b70f57cfad5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2578ac673dd6fcfe882961f1554f695"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#ae2578ac673dd6fcfe882961f1554f695">is_wrapped_number</a> () const </td></tr>
<tr class="memdesc:ae2578ac673dd6fcfe882961f1554f695"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if a tensor was auto-wrapped from a C++ or Python number.  <a href="#ae2578ac673dd6fcfe882961f1554f695">More...</a><br /></td></tr>
<tr class="separator:ae2578ac673dd6fcfe882961f1554f695"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a136601d4d5c4f05139b7774660ae6cd0"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a136601d4d5c4f05139b7774660ae6cd0">set_wrapped_number</a> (bool value)</td></tr>
<tr class="memdesc:a136601d4d5c4f05139b7774660ae6cd0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set whether or not a tensor was auto-wrapped from a C++ or Python number.  <a href="#a136601d4d5c4f05139b7774660ae6cd0">More...</a><br /></td></tr>
<tr class="separator:a136601d4d5c4f05139b7774660ae6cd0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a090c75ffdeb4877600dd6969ee2282df"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a090c75ffdeb4877600dd6969ee2282df">set_requires_grad</a> (bool <a class="el" href="structc10_1_1_tensor_impl.html#af33ba2c31af6ce915f3cbaff43f00d6d">requires_grad</a>)</td></tr>
<tr class="memdesc:a090c75ffdeb4877600dd6969ee2282df"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set whether or not a tensor requires gradient.  <a href="#a090c75ffdeb4877600dd6969ee2282df">More...</a><br /></td></tr>
<tr class="separator:a090c75ffdeb4877600dd6969ee2282df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af33ba2c31af6ce915f3cbaff43f00d6d"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#af33ba2c31af6ce915f3cbaff43f00d6d">requires_grad</a> () const </td></tr>
<tr class="memdesc:af33ba2c31af6ce915f3cbaff43f00d6d"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if a tensor requires gradient.  <a href="#af33ba2c31af6ce915f3cbaff43f00d6d">More...</a><br /></td></tr>
<tr class="separator:af33ba2c31af6ce915f3cbaff43f00d6d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6e98b38d1387963595ba8b124fb0fc07"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classat_1_1_tensor.html">at::Tensor</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a6e98b38d1387963595ba8b124fb0fc07">grad</a> ()</td></tr>
<tr class="memdesc:a6e98b38d1387963595ba8b124fb0fc07"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return a mutable reference to the gradient.  <a href="#a6e98b38d1387963595ba8b124fb0fc07">More...</a><br /></td></tr>
<tr class="separator:a6e98b38d1387963595ba8b124fb0fc07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeb60d7221242fd7eed6e5426411e1c00"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="classat_1_1_tensor.html">at::Tensor</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#aeb60d7221242fd7eed6e5426411e1c00">grad</a> () const </td></tr>
<tr class="memdesc:aeb60d7221242fd7eed6e5426411e1c00"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the accumulated gradient of a tensor.  <a href="#aeb60d7221242fd7eed6e5426411e1c00">More...</a><br /></td></tr>
<tr class="separator:aeb60d7221242fd7eed6e5426411e1c00"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a63073c2e89fa7a07327764a3f44fe002"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a63073c2e89fa7a07327764a3f44fe002"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="struct_t.html">T</a> *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002">data</a> () const </td></tr>
<tr class="memdesc:a63073c2e89fa7a07327764a3f44fe002"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return a typed data pointer to the actual data which this tensor refers to.  <a href="#a63073c2e89fa7a07327764a3f44fe002">More...</a><br /></td></tr>
<tr class="separator:a63073c2e89fa7a07327764a3f44fe002"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aca6d30b4e20c3318953331557ee99ef9"><td class="memItemLeft" align="right" valign="top">void *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#aca6d30b4e20c3318953331557ee99ef9">data</a> () const </td></tr>
<tr class="memdesc:aca6d30b4e20c3318953331557ee99ef9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return a void* data pointer to the actual data which this tensor refers to.  <a href="#aca6d30b4e20c3318953331557ee99ef9">More...</a><br /></td></tr>
<tr class="separator:aca6d30b4e20c3318953331557ee99ef9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6e91235862be52cf3cbf6d35c0d328e7"><td class="memItemLeft" align="right" valign="top">virtual void *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a6e91235862be52cf3cbf6d35c0d328e7">slow_data</a> () const </td></tr>
<tr class="memdesc:a6e91235862be52cf3cbf6d35c0d328e7"><td class="mdescLeft">&#160;</td><td class="mdescRight">This is just like <a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002" title="Return a typed data pointer to the actual data which this tensor refers to. ">data()</a>, except it works with Variables.  <a href="#a6e91235862be52cf3cbf6d35c0d328e7">More...</a><br /></td></tr>
<tr class="separator:a6e91235862be52cf3cbf6d35c0d328e7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af2a80605f247ac1cd3ea8309e51f1c76"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:af2a80605f247ac1cd3ea8309e51f1c76"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="struct_t.html">T</a> *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#af2a80605f247ac1cd3ea8309e51f1c76">unsafe_data</a> () const </td></tr>
<tr class="memdesc:af2a80605f247ac1cd3ea8309e51f1c76"><td class="mdescLeft">&#160;</td><td class="mdescRight">Like <a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002" title="Return a typed data pointer to the actual data which this tensor refers to. ">data&lt;T&gt;()</a>, but performs no checks.  <a href="#af2a80605f247ac1cd3ea8309e51f1c76">More...</a><br /></td></tr>
<tr class="separator:af2a80605f247ac1cd3ea8309e51f1c76"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a90251cdcd95f40e9e1d70e02ce8aa4d3"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a90251cdcd95f40e9e1d70e02ce8aa4d3"></a>
const <a class="el" href="classcaffe2_1_1_type_meta.html">caffe2::TypeMeta</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a90251cdcd95f40e9e1d70e02ce8aa4d3">dtype</a> () const </td></tr>
<tr class="memdesc:a90251cdcd95f40e9e1d70e02ce8aa4d3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the TypeMeta of a tensor, which describes what data type it is (e.g., int, float, ...) <br /></td></tr>
<tr class="separator:a90251cdcd95f40e9e1d70e02ce8aa4d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ca17502803f574e11aea0fcb961a7c8"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a7ca17502803f574e11aea0fcb961a7c8"></a>
size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a7ca17502803f574e11aea0fcb961a7c8">itemsize</a> () const </td></tr>
<tr class="memdesc:a7ca17502803f574e11aea0fcb961a7c8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the size of a single element of this tensor in bytes. <br /></td></tr>
<tr class="separator:a7ca17502803f574e11aea0fcb961a7c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abacd934ef7dea30d886210d7244204b9"><td class="memItemLeft" align="right" valign="top">virtual int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#abacd934ef7dea30d886210d7244204b9">storage_offset</a> () const </td></tr>
<tr class="memdesc:abacd934ef7dea30d886210d7244204b9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the offset in number of elements into the storage that this tensor points to.  <a href="#abacd934ef7dea30d886210d7244204b9">More...</a><br /></td></tr>
<tr class="separator:abacd934ef7dea30d886210d7244204b9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a08c60853d31b5f7918fa278d16dcd7c8"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a08c60853d31b5f7918fa278d16dcd7c8"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a08c60853d31b5f7918fa278d16dcd7c8">is_empty</a> () const </td></tr>
<tr class="memdesc:a08c60853d31b5f7918fa278d16dcd7c8"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if a tensor has no elements (e.g., <a class="el" href="structc10_1_1_tensor_impl.html#a8e3b32d6afc37de6fbceda4556d03617" title="The number of elements in a tensor. ">numel()</a> == 0). <br /></td></tr>
<tr class="separator:a08c60853d31b5f7918fa278d16dcd7c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5577a5d039a0e705a25963e29ef86671"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a5577a5d039a0e705a25963e29ef86671">resize_dim</a> (int64_t ndim)</td></tr>
<tr class="memdesc:a5577a5d039a0e705a25963e29ef86671"><td class="mdescLeft">&#160;</td><td class="mdescRight">Change the dimensionality of a tensor.  <a href="#a5577a5d039a0e705a25963e29ef86671">More...</a><br /></td></tr>
<tr class="separator:a5577a5d039a0e705a25963e29ef86671"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8381c35ea2d0b97a7896cd53151c13e5"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a8381c35ea2d0b97a7896cd53151c13e5">set_size</a> (int64_t <a class="el" href="structc10_1_1_tensor_impl.html#a665ea0a74febe6b47de99d597355367a">dim</a>, int64_t new_size)</td></tr>
<tr class="memdesc:a8381c35ea2d0b97a7896cd53151c13e5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Change the size at some dimension.  <a href="#a8381c35ea2d0b97a7896cd53151c13e5">More...</a><br /></td></tr>
<tr class="separator:a8381c35ea2d0b97a7896cd53151c13e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a30f148c90175f21e6562f16f44e88c8e"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a30f148c90175f21e6562f16f44e88c8e">set_stride</a> (int64_t <a class="el" href="structc10_1_1_tensor_impl.html#a665ea0a74febe6b47de99d597355367a">dim</a>, int64_t new_stride)</td></tr>
<tr class="memdesc:a30f148c90175f21e6562f16f44e88c8e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Change the stride at some dimension.  <a href="#a30f148c90175f21e6562f16f44e88c8e">More...</a><br /></td></tr>
<tr class="separator:a30f148c90175f21e6562f16f44e88c8e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a066047c007b3d77bcd72dc642857751b"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a066047c007b3d77bcd72dc642857751b">set_storage_offset</a> (int64_t <a class="el" href="structc10_1_1_tensor_impl.html#abacd934ef7dea30d886210d7244204b9">storage_offset</a>)</td></tr>
<tr class="memdesc:a066047c007b3d77bcd72dc642857751b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the offset into the storage of this tensor.  <a href="#a066047c007b3d77bcd72dc642857751b">More...</a><br /></td></tr>
<tr class="separator:a066047c007b3d77bcd72dc642857751b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af6af3fc10fa72bed0fda05f3fc7870bf"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#af6af3fc10fa72bed0fda05f3fc7870bf">set_sizes_contiguous</a> (<a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a> new_size)</td></tr>
<tr class="memdesc:af6af3fc10fa72bed0fda05f3fc7870bf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Like set_sizes_and_strides but assumes contiguous strides.  <a href="#af6af3fc10fa72bed0fda05f3fc7870bf">More...</a><br /></td></tr>
<tr class="separator:af6af3fc10fa72bed0fda05f3fc7870bf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8a88e608d07ed11711511132bb700d3f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a8a88e608d07ed11711511132bb700d3f">set_sizes_and_strides</a> (<a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a> new_size, <a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a> new_stride)</td></tr>
<tr class="memdesc:a8a88e608d07ed11711511132bb700d3f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the sizes and strides of a tensor.  <a href="#a8a88e608d07ed11711511132bb700d3f">More...</a><br /></td></tr>
<tr class="separator:a8a88e608d07ed11711511132bb700d3f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a974f82abdbab6499f5fdd067ddaebc1a"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a974f82abdbab6499f5fdd067ddaebc1a"></a>
virtual int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a974f82abdbab6499f5fdd067ddaebc1a">size</a> (int64_t d) const </td></tr>
<tr class="memdesc:a974f82abdbab6499f5fdd067ddaebc1a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the size of a tensor at some dimension. <br /></td></tr>
<tr class="separator:a974f82abdbab6499f5fdd067ddaebc1a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1c5bb785c48cb4bbd41d9b5080217701"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a1c5bb785c48cb4bbd41d9b5080217701"></a>
virtual int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a1c5bb785c48cb4bbd41d9b5080217701">stride</a> (int64_t d) const </td></tr>
<tr class="memdesc:a1c5bb785c48cb4bbd41d9b5080217701"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the stride of a tensor at some dimension. <br /></td></tr>
<tr class="separator:a1c5bb785c48cb4bbd41d9b5080217701"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a765410a53c2fce051a62c303632c0e21"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a765410a53c2fce051a62c303632c0e21">is_variable</a> () const </td></tr>
<tr class="memdesc:a765410a53c2fce051a62c303632c0e21"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if a tensor is a variable.  <a href="#a765410a53c2fce051a62c303632c0e21">More...</a><br /></td></tr>
<tr class="separator:a765410a53c2fce051a62c303632c0e21"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9338ea2bdbf102e615aa4473e0110cf3"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a9338ea2bdbf102e615aa4473e0110cf3">set_allow_tensor_metadata_change</a> (bool value)</td></tr>
<tr class="memdesc:a9338ea2bdbf102e615aa4473e0110cf3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set whether a tensor allows changes to its metadata (e.g.  <a href="#a9338ea2bdbf102e615aa4473e0110cf3">More...</a><br /></td></tr>
<tr class="separator:a9338ea2bdbf102e615aa4473e0110cf3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4fc1d14dd7e6af683f851372c2cb6f83"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a4fc1d14dd7e6af683f851372c2cb6f83">allow_tensor_metadata_change</a> () const </td></tr>
<tr class="memdesc:a4fc1d14dd7e6af683f851372c2cb6f83"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if a tensor allows changes to its metadata (e.g.  <a href="#a4fc1d14dd7e6af683f851372c2cb6f83">More...</a><br /></td></tr>
<tr class="separator:a4fc1d14dd7e6af683f851372c2cb6f83"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a868dacef60b4c6e937537eb086543d25"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a868dacef60b4c6e937537eb086543d25"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a868dacef60b4c6e937537eb086543d25">set_autograd_meta</a> (std::unique_ptr&lt; <a class="el" href="structc10_1_1_autograd_meta_interface.html">c10::AutogradMetaInterface</a> &gt; <a class="el" href="structc10_1_1_tensor_impl.html#a718f71931444d9e09ff39feca96ce06e">autograd_meta</a>)</td></tr>
<tr class="memdesc:a868dacef60b4c6e937537eb086543d25"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the pointer to autograd metadata. <br /></td></tr>
<tr class="separator:a868dacef60b4c6e937537eb086543d25"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a718f71931444d9e09ff39feca96ce06e"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a718f71931444d9e09ff39feca96ce06e"></a>
<a class="el" href="structc10_1_1_autograd_meta_interface.html">c10::AutogradMetaInterface</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a718f71931444d9e09ff39feca96ce06e">autograd_meta</a> () const </td></tr>
<tr class="memdesc:a718f71931444d9e09ff39feca96ce06e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the pointer to autograd metadata. <br /></td></tr>
<tr class="separator:a718f71931444d9e09ff39feca96ce06e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad7caeea7f6629be2b78b1a43db6b4603"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad7caeea7f6629be2b78b1a43db6b4603"></a>
std::unique_ptr&lt; <a class="el" href="structc10_1_1_autograd_meta_interface.html">c10::AutogradMetaInterface</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#ad7caeea7f6629be2b78b1a43db6b4603">detach_autograd_meta</a> ()</td></tr>
<tr class="memdesc:ad7caeea7f6629be2b78b1a43db6b4603"><td class="mdescLeft">&#160;</td><td class="mdescRight">Detach the autograd metadata unique_ptr from this tensor, and return it. <br /></td></tr>
<tr class="separator:ad7caeea7f6629be2b78b1a43db6b4603"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a42e83a594d1549c67921a79ff226257a"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a42e83a594d1549c67921a79ff226257a"></a>
virtual <a class="el" href="classc10_1_1intrusive__ptr.html">c10::intrusive_ptr</a>&lt; <a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>shallow_copy_and_detach</b> () const </td></tr>
<tr class="separator:a42e83a594d1549c67921a79ff226257a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5f9c5b2101ccad1d8f83446820cd7c07"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a5f9c5b2101ccad1d8f83446820cd7c07"></a>
DeviceType&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a5f9c5b2101ccad1d8f83446820cd7c07">device_type</a> () const </td></tr>
<tr class="memdesc:a5f9c5b2101ccad1d8f83446820cd7c07"><td class="mdescLeft">&#160;</td><td class="mdescRight">The device type of a <a class="el" href="struct_tensor.html">Tensor</a>, e.g., DeviceType::CPU or DeviceType::CUDA. <br /></td></tr>
<tr class="separator:a5f9c5b2101ccad1d8f83446820cd7c07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a64309ef1cc41ad4156150c461257a99c"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a64309ef1cc41ad4156150c461257a99c"></a>
<a class="el" href="structc10_1_1_device.html">Device</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a64309ef1cc41ad4156150c461257a99c">GetDevice</a> () const </td></tr>
<tr class="memdesc:a64309ef1cc41ad4156150c461257a99c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The device of a <a class="el" href="struct_tensor.html">Tensor</a>; e.g., Device(kCUDA, 1) (the 1-index CUDA device). <br /></td></tr>
<tr class="separator:a64309ef1cc41ad4156150c461257a99c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a936fa42b901da59ac9a27f428cff75a7"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a936fa42b901da59ac9a27f428cff75a7">Extend</a> (int64_t num, float growthPct)</td></tr>
<tr class="memdesc:a936fa42b901da59ac9a27f428cff75a7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Extends the outer-most dimension of this tensor by num elements, preserving the existing data.  <a href="#a936fa42b901da59ac9a27f428cff75a7">More...</a><br /></td></tr>
<tr class="separator:a936fa42b901da59ac9a27f428cff75a7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a65b3ce1c6185d9ef7f6a6347a2b66bac"><td class="memTemplParams" colspan="2">template&lt;class T &gt; </td></tr>
<tr class="memitem:a65b3ce1c6185d9ef7f6a6347a2b66bac"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a65b3ce1c6185d9ef7f6a6347a2b66bac">ReserveSpace</a> (const <a class="el" href="struct_t.html">T</a> &amp;outer_dim)</td></tr>
<tr class="memdesc:a65b3ce1c6185d9ef7f6a6347a2b66bac"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reserve space for the underlying tensor.  <a href="#a65b3ce1c6185d9ef7f6a6347a2b66bac">More...</a><br /></td></tr>
<tr class="separator:a65b3ce1c6185d9ef7f6a6347a2b66bac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a47ce59523d6014898bf0ab9a6e039196"><td class="memTemplParams" colspan="2">template&lt;typename... Ts&gt; </td></tr>
<tr class="memitem:a47ce59523d6014898bf0ab9a6e039196"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a47ce59523d6014898bf0ab9a6e039196">Resize</a> (Ts...dim_source)</td></tr>
<tr class="memdesc:a47ce59523d6014898bf0ab9a6e039196"><td class="mdescLeft">&#160;</td><td class="mdescRight">Resizes a tensor.  <a href="#a47ce59523d6014898bf0ab9a6e039196">More...</a><br /></td></tr>
<tr class="separator:a47ce59523d6014898bf0ab9a6e039196"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6f2d7bc200872432e20682fc8f26fa8d"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a6f2d7bc200872432e20682fc8f26fa8d">Reshape</a> (const std::vector&lt; int64_t &gt; &amp;dims)</td></tr>
<tr class="memdesc:a6f2d7bc200872432e20682fc8f26fa8d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Resizes the tensor without touching underlying storage.  <a href="#a6f2d7bc200872432e20682fc8f26fa8d">More...</a><br /></td></tr>
<tr class="separator:a6f2d7bc200872432e20682fc8f26fa8d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a12f517489c9841da628d8a92b361521f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a12f517489c9841da628d8a92b361521f">FreeMemory</a> ()</td></tr>
<tr class="memdesc:a12f517489c9841da628d8a92b361521f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Release whatever memory the tensor was holding but keep size and type information.  <a href="#a12f517489c9841da628d8a92b361521f">More...</a><br /></td></tr>
<tr class="separator:a12f517489c9841da628d8a92b361521f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae3e04fc66818d60233f88d4ddbd9183c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#ae3e04fc66818d60233f88d4ddbd9183c">ShareData</a> (const <a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;src)</td></tr>
<tr class="memdesc:ae3e04fc66818d60233f88d4ddbd9183c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Shares the data with another tensor.  <a href="#ae3e04fc66818d60233f88d4ddbd9183c">More...</a><br /></td></tr>
<tr class="separator:ae3e04fc66818d60233f88d4ddbd9183c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3547fc1e6d8bf836cb69e220d91e5bc9"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a3547fc1e6d8bf836cb69e220d91e5bc9"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>ShareExternalPointer</b> (<a class="el" href="classc10_1_1_data_ptr.html">DataPtr</a> &amp;&amp;data_ptr, const <a class="el" href="classcaffe2_1_1_type_meta.html">caffe2::TypeMeta</a> &amp;data_type, size_t capacity)</td></tr>
<tr class="separator:a3547fc1e6d8bf836cb69e220d91e5bc9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67ec56c97e4ada205444d0ae4a50e48e"><td class="memItemLeft" align="right" valign="top">void *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a67ec56c97e4ada205444d0ae4a50e48e">raw_mutable_data</a> (const <a class="el" href="classcaffe2_1_1_type_meta.html">caffe2::TypeMeta</a> &amp;meta)</td></tr>
<tr class="memdesc:a67ec56c97e4ada205444d0ae4a50e48e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a mutable raw pointer of the underlying storage.  <a href="#a67ec56c97e4ada205444d0ae4a50e48e">More...</a><br /></td></tr>
<tr class="separator:a67ec56c97e4ada205444d0ae4a50e48e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef23733a10c22f034c111faa2787eb15"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:aef23733a10c22f034c111faa2787eb15"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="struct_t.html">T</a> *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15">mutable_data</a> ()</td></tr>
<tr class="memdesc:aef23733a10c22f034c111faa2787eb15"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a typed pointer of the underlying storage.  <a href="#aef23733a10c22f034c111faa2787eb15">More...</a><br /></td></tr>
<tr class="separator:aef23733a10c22f034c111faa2787eb15"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5f93ba4342dad6abb175a9e69cac8478"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a5f93ba4342dad6abb175a9e69cac8478">storage_initialized</a> () const noexcept</td></tr>
<tr class="memdesc:a5f93ba4342dad6abb175a9e69cac8478"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if a tensor is storage initialized.  <a href="#a5f93ba4342dad6abb175a9e69cac8478">More...</a><br /></td></tr>
<tr class="separator:a5f93ba4342dad6abb175a9e69cac8478"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a944bc5a7f7a9514afd6dae6b34924ab7"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a944bc5a7f7a9514afd6dae6b34924ab7">dtype_initialized</a> () const noexcept</td></tr>
<tr class="memdesc:a944bc5a7f7a9514afd6dae6b34924ab7"><td class="mdescLeft">&#160;</td><td class="mdescRight">True if a tensor is dtype initialized.  <a href="#a944bc5a7f7a9514afd6dae6b34924ab7">More...</a><br /></td></tr>
<tr class="separator:a944bc5a7f7a9514afd6dae6b34924ab7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7f8d836a91decdccedb5caffd13652cb"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a7f8d836a91decdccedb5caffd13652cb"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>set_storage</b> (<a class="el" href="structc10_1_1_storage.html">at::Storage</a> <a class="el" href="structc10_1_1_tensor_impl.html#a80116b4ec040c911734dbabc7db68f5d">storage</a>)</td></tr>
<tr class="separator:a7f8d836a91decdccedb5caffd13652cb"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:a288353977c1eee5087ef9ebd24fcaa6c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a288353977c1eee5087ef9ebd24fcaa6c">refresh_numel</a> ()</td></tr>
<tr class="memdesc:a288353977c1eee5087ef9ebd24fcaa6c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recompute the cached numel of a tensor.  <a href="#a288353977c1eee5087ef9ebd24fcaa6c">More...</a><br /></td></tr>
<tr class="separator:a288353977c1eee5087ef9ebd24fcaa6c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a311e9b6aa2a9c50a4451c131a71d51e3"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structc10_1_1_tensor_impl.html#a311e9b6aa2a9c50a4451c131a71d51e3">refresh_contiguous</a> ()</td></tr>
<tr class="memdesc:a311e9b6aa2a9c50a4451c131a71d51e3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recompute the cached contiguity of a tensor.  <a href="#a311e9b6aa2a9c50a4451c131a71d51e3">More...</a><br /></td></tr>
<tr class="separator:a311e9b6aa2a9c50a4451c131a71d51e3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_methods_classc10_1_1intrusive__ptr__target"><td colspan="2" onclick="javascript:toggleInherit('pro_methods_classc10_1_1intrusive__ptr__target')"><img src="closed.png" alt="-"/>&#160;Protected Member Functions inherited from <a class="el" href="classc10_1_1intrusive__ptr__target.html">c10::intrusive_ptr_target</a></td></tr>
<tr class="memitem:a57cdeb878b0cf89b93c420c66582d6a7 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a57cdeb878b0cf89b93c420c66582d6a7"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>intrusive_ptr_target</b> (<a class="el" href="classc10_1_1intrusive__ptr__target.html">intrusive_ptr_target</a> &amp;&amp;other) noexcept</td></tr>
<tr class="separator:a57cdeb878b0cf89b93c420c66582d6a7 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a360753f19ff2f6be6472ea3363f33633 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a360753f19ff2f6be6472ea3363f33633"></a>
<a class="el" href="classc10_1_1intrusive__ptr__target.html">intrusive_ptr_target</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><b>operator=</b> (<a class="el" href="classc10_1_1intrusive__ptr__target.html">intrusive_ptr_target</a> &amp;&amp;other) noexcept</td></tr>
<tr class="separator:a360753f19ff2f6be6472ea3363f33633 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a861994d3850562325fe5a61801d11378 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a861994d3850562325fe5a61801d11378"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>intrusive_ptr_target</b> (const <a class="el" href="classc10_1_1intrusive__ptr__target.html">intrusive_ptr_target</a> &amp;other) noexcept</td></tr>
<tr class="separator:a861994d3850562325fe5a61801d11378 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af17bd8a1a3a3884f2abf42167949d4b6 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="af17bd8a1a3a3884f2abf42167949d4b6"></a>
<a class="el" href="classc10_1_1intrusive__ptr__target.html">intrusive_ptr_target</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><b>operator=</b> (const <a class="el" href="classc10_1_1intrusive__ptr__target.html">intrusive_ptr_target</a> &amp;other) noexcept</td></tr>
<tr class="separator:af17bd8a1a3a3884f2abf42167949d4b6 inherit pro_methods_classc10_1_1intrusive__ptr__target"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr class="memitem:a47a0821837a7d4311ddcefe9d1fafd8b"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a47a0821837a7d4311ddcefe9d1fafd8b"></a>
<a class="el" href="structc10_1_1_storage.html">Storage</a>&#160;</td><td class="memItemRight" valign="bottom"><b>storage_</b></td></tr>
<tr class="separator:a47a0821837a7d4311ddcefe9d1fafd8b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a72f073f6bd616b46ad263fdbbc1193ea"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a72f073f6bd616b46ad263fdbbc1193ea"></a>
std::unique_ptr&lt; <a class="el" href="structc10_1_1_autograd_meta_interface.html">c10::AutogradMetaInterface</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>autograd_meta_</b> = nullptr</td></tr>
<tr class="separator:a72f073f6bd616b46ad263fdbbc1193ea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf92cd32b02c0cb9e00b4427e33fdbbc"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aaf92cd32b02c0cb9e00b4427e33fdbbc"></a>
<a class="el" href="classc10_1_1_small_vector.html">SmallVector</a>&lt; int64_t, 5 &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>sizes_</b></td></tr>
<tr class="separator:aaf92cd32b02c0cb9e00b4427e33fdbbc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acaefe39212b364c72a646a7dbbdd362b"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="acaefe39212b364c72a646a7dbbdd362b"></a>
<a class="el" href="classc10_1_1_small_vector.html">SmallVector</a>&lt; int64_t, 5 &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>strides_</b></td></tr>
<tr class="separator:acaefe39212b364c72a646a7dbbdd362b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a79072d370da1600cdad2d289b7ade468"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a79072d370da1600cdad2d289b7ade468"></a>
int64_t&#160;</td><td class="memItemRight" valign="bottom"><b>storage_offset_</b> = 0</td></tr>
<tr class="separator:a79072d370da1600cdad2d289b7ade468"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a900f513c9c954132cb1234bc0f3e29e2"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a900f513c9c954132cb1234bc0f3e29e2"></a>
int64_t&#160;</td><td class="memItemRight" valign="bottom"><b>numel_</b> = 1</td></tr>
<tr class="separator:a900f513c9c954132cb1234bc0f3e29e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a27d1fe7563754ec3cecb2c1c1bac085c"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a27d1fe7563754ec3cecb2c1c1bac085c"></a>
<a class="el" href="classcaffe2_1_1_type_meta.html">caffe2::TypeMeta</a>&#160;</td><td class="memItemRight" valign="bottom"><b>data_type_</b></td></tr>
<tr class="separator:a27d1fe7563754ec3cecb2c1c1bac085c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a309349c236429d335f88ebbf251a94a8"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a309349c236429d335f88ebbf251a94a8"></a>
<a class="el" href="classc10_1_1_tensor_type_id.html">TensorTypeId</a>&#160;</td><td class="memItemRight" valign="bottom"><b>type_id_</b></td></tr>
<tr class="separator:a309349c236429d335f88ebbf251a94a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:addd5826687b7cb5095541ff431884844"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="addd5826687b7cb5095541ff431884844"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>is_contiguous_</b> = true</td></tr>
<tr class="separator:addd5826687b7cb5095541ff431884844"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae04b0a4147dcc44a9d34df9ce2f64576"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ae04b0a4147dcc44a9d34df9ce2f64576"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>is_variable_</b> = false</td></tr>
<tr class="separator:ae04b0a4147dcc44a9d34df9ce2f64576"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab6b604083d0174425ceb4c35a621d2b3"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ab6b604083d0174425ceb4c35a621d2b3"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>is_wrapped_number_</b> = false</td></tr>
<tr class="separator:ab6b604083d0174425ceb4c35a621d2b3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a78918e5e67da90935f67c1e9fee86362"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a78918e5e67da90935f67c1e9fee86362"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>allow_tensor_metadata_change_</b> = true</td></tr>
<tr class="separator:a78918e5e67da90935f67c1e9fee86362"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae5600c1344d9cf9568cab84407d4e84b"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ae5600c1344d9cf9568cab84407d4e84b"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><b>reserved_</b> = false</td></tr>
<tr class="separator:ae5600c1344d9cf9568cab84407d4e84b"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>The low-level representation of a tensor, which contains a pointer to a storage (which contains the actual data) and metadata (e.g., sizes and strides) describing this particular view of the data as a tensor. </p>
<p>Some basic characteristics about our in-memory representation of tensors:</p>
<ul>
<li>It contains a pointer to a storage struct (Storage/StorageImpl) which contains the pointer to the actual data and records the data type and device of the view. This allows multiple tensors to alias the same underlying data, which allows to efficiently implement differing <em>views</em> on a tensor.</li>
<li>The tensor struct itself records view-specific metadata about the tensor, e.g., sizes, strides and offset into storage. Each view of a storage can have a different size or offset.</li>
<li>This class is intrusively refcounted. It is refcounted so that we can support prompt deallocation of large tensors; it is intrusively refcounted so that we can still perform reference counted operations on raw pointers, which is often more convenient when passing tensors across language boundaries.</li>
<li><p class="startli">For backwards-compatibility reasons, a tensor may be in an uninitialized state. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor may be uninitialized in the following two ways:</p><ul>
<li><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor may be DTYPE UNINITIALIZED. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor of this form has an uninitialized dtype. This situation most frequently arises when a user writes <a class="el" href="struct_tensor.html">Tensor</a> x(CPU). The dtype and is subsequently initialized when <a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15" title="Returns a typed pointer of the underlying storage. ">mutable_data&lt;T&gt;()</a> is invoked for the first time.</li>
<li><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor may be STORAGE UNINITIALIZED. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor of this form has non-zero size, but has a storage with a null data pointer. This situation most frequently arises when a user calls <a class="el" href="structc10_1_1_tensor_impl.html#a47ce59523d6014898bf0ab9a6e039196" title="Resizes a tensor. ">Resize()</a> or <a class="el" href="structc10_1_1_tensor_impl.html#a12f517489c9841da628d8a92b361521f" title="Release whatever memory the tensor was holding but keep size and type information. ">FreeMemory()</a>. This is because Caffe2 historically does lazy allocation: allocation of data doesn't occur until <a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15" title="Returns a typed pointer of the underlying storage. ">mutable_data&lt;T&gt;()</a> is invoked. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor with zero size is always storage initialized, because no allocation is necessary in this case.</li>
</ul>
<p class="startli">All combinations of these two uninitialized states are possible. Consider the following transcript in idiomatic Caffe2 API:</p>
<p class="startli"><a class="el" href="struct_tensor.html">Tensor</a> x(CPU); // x is storage-initialized, dtype-UNINITIALIZED x.Resize(4); // x is storage-UNINITIALIZED, dtype-UNINITIALIZED x.mutable_data&lt;float&gt;(); // x is storage-initialized, dtype-initialized x.FreeMemory(); // x is storage-UNINITIALIZED, dtype-initialized.</p>
<p class="startli">All other fields on tensor are always initialized. In particular, size is always valid. (Historically, a tensor declared as <a class="el" href="struct_tensor.html">Tensor</a> x(CPU) also had uninitialized size, encoded as numel == -1, but we have now decided to default to zero size, resulting in numel == 0).</p>
<p class="startli">Uninitialized storages MUST be uniquely owned, to keep our model simple. Thus, we will reject operations which could cause an uninitialized storage to become shared (or a shared storage to become uninitialized, e.g., from FreeMemory).</p>
<p class="startli">In practice, tensors which are storage-UNINITIALIZED and dtype-UNINITIALIZED are <em>extremely</em> ephemeral: essentially, after you do a <a class="el" href="structc10_1_1_tensor_impl.html#a47ce59523d6014898bf0ab9a6e039196" title="Resizes a tensor. ">Resize()</a>, you basically always call <a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15" title="Returns a typed pointer of the underlying storage. ">mutable_data()</a> immediately afterwards. Most functions are not designed to work if given a storage-UNINITIALIZED, dtype-UNINITIALIZED tensor.</p>
<p class="startli">We intend to eliminate all uninitialized states, so that every tensor is fully initialized in all fields. Please do not write new code that depends on these uninitialized states. </p>
</li>
</ul>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00211">211</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a70af485166cfc35904dd7f22f5bcd430"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">c10::TensorImpl::TensorImpl </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classc10_1_1_tensor_type_id.html">TensorTypeId</a>&#160;</td>
          <td class="paramname"><em>type_id</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classcaffe2_1_1_type_meta.html">caffe2::TypeMeta</a> &amp;&#160;</td>
          <td class="paramname"><em>data_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="structc10_1_1_allocator.html">Allocator</a> *&#160;</td>
          <td class="paramname"><em>allocator</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>is_variable</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Construct a 1-dim 0-size tensor with the given settings. </p>
<p>The provided allocator will be used to allocate data on subsequent resize. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00036">36</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="a4fc1d14dd7e6af683f851372c2cb6f83"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool c10::TensorImpl::allow_tensor_metadata_change </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if a tensor allows changes to its metadata (e.g. </p>
<p>sizes / strides / storage / storage_offset). </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00821">821</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a63073c2e89fa7a07327764a3f44fe002"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="struct_t.html">T</a>* c10::TensorImpl::data </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return a typed data pointer to the actual data which this tensor refers to. </p>
<p>This checks that the requested type (from the template parameter) matches the internal type of the tensor.</p>
<p>It is invalid to call <a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002" title="Return a typed data pointer to the actual data which this tensor refers to. ">data()</a> on a dtype-uninitialized tensor, even if the size is 0.</p>
<p>WARNING: If a tensor is not contiguous, you MUST use strides when performing index calculations to determine the location of elements in the tensor. We recommend using 'TensorAccessor' to handle this computation for you; this class is available from '<a class="el" href="struct_tensor.html">Tensor</a>'.</p>
<p>WARNING: It is NOT valid to call this method on a Variable. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00564">564</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="aca6d30b4e20c3318953331557ee99ef9"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void* c10::TensorImpl::data </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return a void* data pointer to the actual data which this tensor refers to. </p>
<p>It is invalid to call <a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002" title="Return a typed data pointer to the actual data which this tensor refers to. ">data()</a> on a dtype-uninitialized tensor, even if the size is 0.</p>
<p>WARNING: The data pointed to by this tensor may not contiguous; do NOT assume that <a class="el" href="structc10_1_1_tensor_impl.html#a7ca17502803f574e11aea0fcb961a7c8" title="Return the size of a single element of this tensor in bytes. ">itemsize()</a> * <a class="el" href="structc10_1_1_tensor_impl.html#a8e3b32d6afc37de6fbceda4556d03617" title="The number of elements in a tensor. ">numel()</a> is sufficient to compute the bytes that can be validly read from this tensor.</p>
<p>WARNING: It is NOT valid to call this method on a Variable. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00595">595</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a665ea0a74febe6b47de99d597355367a"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int64_t c10::TensorImpl::dim </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return the number of dimensions of this tensor. </p>
<p>Note that 0-dimension represents a <a class="el" href="struct_tensor.html">Tensor</a> that is a <a class="el" href="classc10_1_1_scalar.html" title="Scalar represents a 0-dimensional tensor which contains a single element. ">Scalar</a>, e.g., one that has a single element. </p>

<p>Reimplemented in <a class="el" href="structtorch_1_1jit_1_1_container_tensor.html#ace12db8c70e5f75108f1e710ab62c940">torch::jit::ContainerTensor</a>, <a class="el" href="structat_1_1_sparse_tensor_impl.html#a93e5afb006021c00fb0379df9e913cc0">at::SparseTensorImpl</a>, and <a class="el" href="structc10_1_1_undefined_tensor_impl.html#ab747ab8816d73a61ceab88d6cf2f7c00">c10::UndefinedTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00091">91</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a944bc5a7f7a9514afd6dae6b34924ab7"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool c10::TensorImpl::dtype_initialized </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if a tensor is dtype initialized. </p>
<p><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor allocated with Caffe2-style constructors is dtype uninitialized until the first time <a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15" title="Returns a typed pointer of the underlying storage. ">mutable_data&lt;T&gt;()</a> is called. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01246">1246</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a936fa42b901da59ac9a27f428cff75a7"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::Extend </td>
          <td>(</td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>num</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>growthPct</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Extends the outer-most dimension of this tensor by num elements, preserving the existing data. </p>
<p>The underlying data may be reallocated in order to accommodate the new elements, in which case this tensors' capacity is grown at a factor of growthPct. This ensures that Extend runs on an amortized O(1) time complexity.</p>
<p>This op is auto-asynchronous if the underlying device (CUDA) supports it. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00904">904</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a12f517489c9841da628d8a92b361521f"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::FreeMemory </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Release whatever memory the tensor was holding but keep size and type information. </p>
<p>Subsequent call to mutable_data will trigger new memory allocation. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01071">1071</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a6e98b38d1387963595ba8b124fb0fc07"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classat_1_1_tensor.html">at::Tensor</a> &amp; c10::TensorImpl::grad </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Return a mutable reference to the gradient. </p>
<p>This is conventionally used as <code>t.grad() = x</code> to set a gradient to a completely new tensor.</p>
<p>It is only valid to call this method on a Variable. See Note [<a class="el" href="struct_tensor.html">Tensor</a> versus Variable in C++]. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00020">20</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="aeb60d7221242fd7eed6e5426411e1c00"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="classat_1_1_tensor.html">at::Tensor</a> &amp; c10::TensorImpl::grad </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Return the accumulated gradient of a tensor. </p>
<p>This gradient is written into when performing backwards, when this tensor is a leaf tensor.</p>
<p>It is only valid to call this method on a Variable. See Note [<a class="el" href="struct_tensor.html">Tensor</a> versus Variable in C++]. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00028">28</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a4f870f592c567d083ac3fc8072e36ff1"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool c10::TensorImpl::has_storage </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if this tensor has storage. </p>
<p>See <a class="el" href="structc10_1_1_tensor_impl.html#a80116b4ec040c911734dbabc7db68f5d" title="Return the underlying storage of a Tensor. ">storage()</a> for details. </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#aa061fb605bdb5bd720c8d3be77c3725e">at::SparseTensorImpl</a>, and <a class="el" href="structc10_1_1_undefined_tensor_impl.html#acc69992456bacc3f5f6d491833611141">c10::UndefinedTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00113">113</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="af0a92e41852fe98ad115713b90575ced"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool c10::TensorImpl::is_contiguous </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Whether or not a tensor is laid out in contiguous memory. </p>
<p>Tensors with non-trivial strides are not contiguous. See compute_contiguous() for the exact definition of whether or not a tensor is contiguous or not. </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#a0c36dae2617b770b1d00f8fa413a6f8f">at::SparseTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00331">331</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a765410a53c2fce051a62c303632c0e21"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool c10::TensorImpl::is_variable </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if a tensor is a variable. </p>
<p>See Note [<a class="el" href="struct_tensor.html">Tensor</a> versus Variable in C++] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00809">809</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="ae2578ac673dd6fcfe882961f1554f695"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool c10::TensorImpl::is_wrapped_number </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if a tensor was auto-wrapped from a C++ or Python number. </p>
<p>For example, when you write 't + 2', 2 is auto-wrapped into a <a class="el" href="struct_tensor.html">Tensor</a> with <code>is_wrapped_number_</code> set to true.</p>
<p>Wrapped numbers do not participate in the result type computation for mixed-type operations if there are any Tensors that are not wrapped numbers. This is useful, because we want 't + 2' to work with any type of tensor, not just LongTensor (which is what integers in Python represent).</p>
<p>Otherwise, they behave like their non-wrapped equivalents. See [Result type computation] in <a class="el" href="_tensor_iterator_8h_source.html">TensorIterator.h</a>.</p>
<p>Why did we opt for wrapped numbers, as opposed to just having an extra function add(Tensor, Scalar)? This helps greatly reduce the amount of code we have to write for add, when actually a Tensor-Scalar addition is really just a Tensor-Tensor addition when the RHS is 0-dim (except for promotion behavior.)</p>
<p>WARNING: It is NOT valid to call this method on a Variable. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00441">441</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="ae3f850babf46aa6d1c746b70f57cfad5"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> * c10::TensorImpl::maybe_zero_dim </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>condition_when_zero_dim</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>If <code>condition_when_zero_dim</code> is true, and the tensor is a 1-dim, 1-size tensor, reshape the tensor into a 0-dim tensor (scalar). </p>
<p>This helper function is called from generated wrapper code, to help "fix up" tensors that legacy code didn't generate in the correct shape. For example, suppose that we have a legacy function 'add' which produces a tensor which is the same shape as its inputs; however, if the inputs were zero-dimensional, it produced a 1-dim 1-size tensor (don't ask). result-&gt;maybe_zero_dim(lhs-&gt;<a class="el" href="structc10_1_1_tensor_impl.html#a665ea0a74febe6b47de99d597355367a" title="Return the number of dimensions of this tensor. ">dim()</a> == 0 &amp;&amp; rhs-&gt;<a class="el" href="structc10_1_1_tensor_impl.html#a665ea0a74febe6b47de99d597355367a" title="Return the number of dimensions of this tensor. ">dim()</a> == 0) will be called, correctly resetting the dimension to 0 when when the inputs had 0-dim.</p>
<p>As we teach more and more of TH to handle 0-dim correctly, this function will become less necessary. At the moment, it is often called from functions that correctly handle the 0-dim case, and is just dead code in this case. In the glorious future, this function will be eliminated entirely. </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#a7091fd7734989cb21ca69cf9c3029e3c">at::SparseTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00105">105</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="aef23733a10c22f034c111faa2787eb15"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="struct_t.html">T</a>* c10::TensorImpl::mutable_data </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns a typed pointer of the underlying storage. </p>
<p>For fundamental types, we reuse possible existing storage if there is sufficient capacity. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01221">1221</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a8e3b32d6afc37de6fbceda4556d03617"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int64_t c10::TensorImpl::numel </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The number of elements in a tensor. </p>
<p>WARNING: Previously, if you were using the Caffe2 API, you could test <a class="el" href="structc10_1_1_tensor_impl.html#a8e3b32d6afc37de6fbceda4556d03617" title="The number of elements in a tensor. ">numel()</a> == -1 to see if a tensor was uninitialized. This is no longer true; numel always accurately reports the product of sizes of a tensor. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00317">317</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a67ec56c97e4ada205444d0ae4a50e48e"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void* c10::TensorImpl::raw_mutable_data </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classcaffe2_1_1_type_meta.html">caffe2::TypeMeta</a> &amp;&#160;</td>
          <td class="paramname"><em>meta</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns a mutable raw pointer of the underlying storage. </p>
<p>Since we will need to know the type of the data for allocation, a TypeMeta object is passed in to specify the necessary information. This is conceptually equivalent of calling <a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15" title="Returns a typed pointer of the underlying storage. ">mutable_data&lt;T&gt;()</a> where the TypeMeta parameter meta is derived from the type <a class="el" href="struct_t.html">T</a>. This function differs from <a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15" title="Returns a typed pointer of the underlying storage. ">mutable_data&lt;T&gt;()</a> in the sense that the type <a class="el" href="struct_t.html">T</a> can be specified during runtime via the TypeMeta object.</p>
<p>If the existing data does not match the desired type, it will be deleted and a new storage will be created. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01160">1160</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a311e9b6aa2a9c50a4451c131a71d51e3"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::refresh_contiguous </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Recompute the cached contiguity of a tensor. </p>
<p>Call this if you modify sizes or strides. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01358">1358</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a288353977c1eee5087ef9ebd24fcaa6c"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::refresh_numel </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Recompute the cached numel of a tensor. </p>
<p>Call this if you modify sizes. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01349">1349</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a8e086291ae4d78f03ddb6ec12b1c03b7"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::release_resources </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Release (decref) storage, and any other external allocations. </p>
<p>This override is for <code><a class="el" href="classc10_1_1intrusive__ptr__target.html" title="intrusive_ptr&lt;T&gt; is an alternative to shared_ptr&lt;T&gt; that has better performance because it does the r...">intrusive_ptr_target</a></code> and is used to implement weak tensors. </p>

<p>Reimplemented from <a class="el" href="classc10_1_1intrusive__ptr__target.html">c10::intrusive_ptr_target</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00085">85</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="af33ba2c31af6ce915f3cbaff43f00d6d"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool c10::TensorImpl::requires_grad </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if a tensor requires gradient. </p>
<p>Tensors which require gradient have history tracked for any operations performed on them, so that we can automatically differentiate back to them. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor that requires gradient and has no history is a "leaf" tensor, which we accumulate gradients into.</p>
<p>It is only valid to call this method on a Variable. See Note [<a class="el" href="struct_tensor.html">Tensor</a> versus Variable in C++]. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00521">521</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a65b3ce1c6185d9ef7f6a6347a2b66bac"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::ReserveSpace </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="struct_t.html">T</a> &amp;&#160;</td>
          <td class="paramname"><em>outer_dim</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Reserve space for the underlying tensor. </p>
<p>This must be called after <a class="el" href="structc10_1_1_tensor_impl.html#a47ce59523d6014898bf0ab9a6e039196" title="Resizes a tensor. ">Resize()</a>, since we only specify the first dimension This does not copy over the old data to the newly allocated space </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00969">969</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a6f2d7bc200872432e20682fc8f26fa8d"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::Reshape </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; int64_t &gt; &amp;&#160;</td>
          <td class="paramname"><em>dims</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Resizes the tensor without touching underlying storage. </p>
<p>This requires the total size of the tensor to remains constant. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01043">1043</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a47ce59523d6014898bf0ab9a6e039196"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename... Ts&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::Resize </td>
          <td>(</td>
          <td class="paramtype">Ts...&#160;</td>
          <td class="paramname"><em>dim_source</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Resizes a tensor. </p>
<p>Resize takes in a vector of ints specifying the dimensions of the tensor. You can pass in an empty vector to specify that it is a scalar (i.e. containing one single item).</p>
<p>The underlying storage may be deleted after calling Resize: if the new shape leads to a different number of items in the tensor, the old memory is deleted and new memory will be allocated next time you call <a class="el" href="structc10_1_1_tensor_impl.html#aef23733a10c22f034c111faa2787eb15" title="Returns a typed pointer of the underlying storage. ">mutable_data()</a>. However, if the shape is different but the total number of items is the same, the underlying storage is kept.</p>
<p>This method respects caffe2_keep_on_shrink. Consult the internal logic of this method to see exactly under what circumstances this flag matters. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01014">1014</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a5577a5d039a0e705a25963e29ef86671"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void c10::TensorImpl::resize_dim </td>
          <td>(</td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>ndim</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Change the dimensionality of a tensor. </p>
<p>This is truly a resize: old sizes, if they are still valid, are preserved (this invariant is utilized by some call-sites, e.g., the implementation of squeeze, which mostly wants the sizes to stay the same). New dimensions are given zero size and zero stride; this is probably not what you want&ndash;you should set_size/set_stride afterwards.</p>
<p>TODO: This should be jettisoned in favor of <code>set_sizes_and_strides</code>, which is harder to misuse. </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#afe0c268576fc6edeae58c80a30cab3e9">at::SparseTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00672">672</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a9338ea2bdbf102e615aa4473e0110cf3"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void c10::TensorImpl::set_allow_tensor_metadata_change </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>value</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set whether a tensor allows changes to its metadata (e.g. </p>
<p>sizes / strides / storage / storage_offset). </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00814">814</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a090c75ffdeb4877600dd6969ee2282df"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::set_requires_grad </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>requires_grad</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set whether or not a tensor requires gradient. </p>
<p>It is only valid to call this method on a Variable. See Note [<a class="el" href="struct_tensor.html">Tensor</a> versus Variable in C++]. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00503">503</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a8381c35ea2d0b97a7896cd53151c13e5"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void c10::TensorImpl::set_size </td>
          <td>(</td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>new_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Change the size at some dimension. </p>
<p>This DOES NOT update strides; thus, most changes to size will not preserve contiguity. You probably also want to call <a class="el" href="structc10_1_1_tensor_impl.html#a30f148c90175f21e6562f16f44e88c8e" title="Change the stride at some dimension. ">set_stride()</a> when you call this.</p>
<p>TODO: This should be jettisoned in favor of <code>set_sizes_and_strides</code>, which is harder to misuse. </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#a05a95c73a6dd4b33a4603afbb5729900">at::SparseTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00688">688</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a8a88e608d07ed11711511132bb700d3f"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::set_sizes_and_strides </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a>&#160;</td>
          <td class="paramname"><em>new_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a>&#160;</td>
          <td class="paramname"><em>new_stride</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the sizes and strides of a tensor. </p>
<p>WARNING: This function does not check if the requested sizes/strides are in bounds for the storage that is allocated; this is the responsibility of the caller</p>
<p>WARNING: It is NOT valid to call this method on a Variable. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00755">755</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="af6af3fc10fa72bed0fda05f3fc7870bf"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::set_sizes_contiguous </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a>&#160;</td>
          <td class="paramname"><em>new_size</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Like set_sizes_and_strides but assumes contiguous strides. </p>
<p>WARNING: This function does not check if the requested sizes/strides are in bounds for the storage that is allocated; this is the responsibility of the caller</p>
<p>WARNING: It is NOT valid to call this method on a Variable. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00730">730</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a066047c007b3d77bcd72dc642857751b"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void c10::TensorImpl::set_storage_offset </td>
          <td>(</td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>storage_offset</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set the offset into the storage of this tensor. </p>
<p>WARNING: This does NOT check if the tensor is in bounds for the new location at the storage; the caller is responsible for checking this (and resizing if necessary.) </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#a9cee74438ad0692e11a8c6451b2540f5">at::SparseTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00715">715</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a30f148c90175f21e6562f16f44e88c8e"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void c10::TensorImpl::set_stride </td>
          <td>(</td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t&#160;</td>
          <td class="paramname"><em>new_stride</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Change the stride at some dimension. </p>
<p>TODO: This should be jettisoned in favor of <code>set_sizes_and_strides</code>, which is harder to misuse. </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#a77f38dca456a927e70e0ec0c86064ac9">at::SparseTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00701">701</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a136601d4d5c4f05139b7774660ae6cd0"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::set_wrapped_number </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>value</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set whether or not a tensor was auto-wrapped from a C++ or Python number. </p>
<p>You probably don't want to call this, unless you are writing binding code.</p>
<p>WARNING: It is NOT valid to call this method on a Variable. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00454">454</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="ae3e04fc66818d60233f88d4ddbd9183c"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void c10::TensorImpl::ShareData </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structc10_1_1_tensor_impl.html">TensorImpl</a> &amp;&#160;</td>
          <td class="paramname"><em>src</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Shares the data with another tensor. </p>
<p>To share data between two tensors, the sizes of the two tensors must be equal already. The reason we do not implicitly do a Resize to make the two tensors have the same shape is that we want to allow tensors of different shapes but the same number of items to still be able to share data. This allows one to e.g. have a n-dimensional <a class="el" href="struct_tensor.html">Tensor</a> and a flattened version sharing the same underlying storage.</p>
<p>The source tensor should already have its data allocated. </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01090">1090</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a2aee10a1a2468955724a364e883e5302"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a> c10::TensorImpl::sizes </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return a reference to the sizes of this tensor. </p>
<p>This reference remains valid as long as the tensor is live and not resized. </p>

<p>Reimplemented in <a class="el" href="structtorch_1_1jit_1_1_container_tensor.html#ae0960af1abdb6552077fa497751947b4">torch::jit::ContainerTensor</a>, <a class="el" href="structat_1_1_sparse_tensor_impl.html#a491558261d5bba70d0434f91b2daccc3">at::SparseTensorImpl</a>, and <a class="el" href="structc10_1_1_undefined_tensor_impl.html#a6fa03ccd63effa2cc12bfd75de0e0e2c">c10::UndefinedTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00059">59</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a6e91235862be52cf3cbf6d35c0d328e7"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void* c10::TensorImpl::slow_data </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This is just like <a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002" title="Return a typed data pointer to the actual data which this tensor refers to. ">data()</a>, except it works with Variables. </p>
<p>This function will go away once Variable and <a class="el" href="struct_tensor.html">Tensor</a> are merged. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00609">609</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="a80116b4ec040c911734dbabc7db68f5d"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="structc10_1_1_storage.html">Storage</a> &amp; c10::TensorImpl::storage </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return the underlying storage of a <a class="el" href="struct_tensor.html">Tensor</a>. </p>
<p>Multiple tensors may share a single storage. <a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> <a class="el" href="structc10_1_1_storage.html">Storage</a> is an impoverished, Tensor-like class which supports far less operations than <a class="el" href="struct_tensor.html">Tensor</a>.</p>
<p>Avoid using this method if possible; try to use only <a class="el" href="struct_tensor.html">Tensor</a> APIs to perform operations. </p>

<p>Reimplemented in <a class="el" href="structtorch_1_1jit_1_1_container_tensor.html#a43f23489a795866b31bf8f3d2ac70b09">torch::jit::ContainerTensor</a>, <a class="el" href="structat_1_1_sparse_tensor_impl.html#a6a0b96264e4cb4440508063fc8a3d714">at::SparseTensorImpl</a>, and <a class="el" href="structc10_1_1_undefined_tensor_impl.html#a1cd5d5a75e1205466125d8ee074d61ef">c10::UndefinedTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00117">117</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a5f93ba4342dad6abb175a9e69cac8478"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool c10::TensorImpl::storage_initialized </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>True if a tensor is storage initialized. </p>
<p><a class="el" href="class_a.html" title="does bound shape inference given a C2 net. ">A</a> tensor may become storage UNINITIALIZED after a <a class="el" href="structc10_1_1_tensor_impl.html#a47ce59523d6014898bf0ab9a6e039196" title="Resizes a tensor. ">Resize()</a> or <a class="el" href="structc10_1_1_tensor_impl.html#a12f517489c9841da628d8a92b361521f" title="Release whatever memory the tensor was holding but keep size and type information. ">FreeMemory()</a> </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l01237">1237</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="abacd934ef7dea30d886210d7244204b9"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int64_t c10::TensorImpl::storage_offset </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return the offset in number of elements into the storage that this tensor points to. </p>
<p>Most tensors have <a class="el" href="structc10_1_1_tensor_impl.html#abacd934ef7dea30d886210d7244204b9" title="Return the offset in number of elements into the storage that this tensor points to. ">storage_offset()</a> == 0, but, for example, an index into a tensor will have a non-zero <a class="el" href="structc10_1_1_tensor_impl.html#abacd934ef7dea30d886210d7244204b9" title="Return the offset in number of elements into the storage that this tensor points to. ">storage_offset()</a>.</p>
<p>WARNING: This is NOT computed in bytes.</p>
<p>XXX: The only thing stopping this function from being virtual is Variable. </p>

<p>Reimplemented in <a class="el" href="structat_1_1_sparse_tensor_impl.html#ada5faa634372fa15a4d6a91f90ea39da">at::SparseTensorImpl</a>, and <a class="el" href="structc10_1_1_undefined_tensor_impl.html#a164302c882d59d6238d8eb19071ce49a">c10::UndefinedTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00650">650</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="af5e15648f034f38ff5b9d5210f893298"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classc10_1_1_array_ref.html">IntArrayRef</a> c10::TensorImpl::strides </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return a reference to the strides of this tensor. </p>
<p>This reference remains valid as long as the tensor is live and not restrided. </p>

<p>Reimplemented in <a class="el" href="structtorch_1_1jit_1_1_container_tensor.html#ae1852f5ab0de96400d814dc0dc9ddf6c">torch::jit::ContainerTensor</a>, <a class="el" href="structat_1_1_sparse_tensor_impl.html#a7d3aafd19eefea999ccca499257ee743">at::SparseTensorImpl</a>, and <a class="el" href="structc10_1_1_undefined_tensor_impl.html#a9671885fe4a633a877128c3985c163d9">c10::UndefinedTensorImpl</a>.</p>

<p>Definition at line <a class="el" href="_tensor_impl_8cpp_source.html#l00063">63</a> of file <a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a>.</p>

</div>
</div>
<a class="anchor" id="a3fff488761f6870393167667d0496f9d"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classc10_1_1_tensor_type_id.html">TensorTypeId</a> c10::TensorImpl::type_id </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return the <a class="el" href="classc10_1_1_tensor_type_id.html" title="Dynamic type ID of a Tensor argument. ">TensorTypeId</a> corresponding to this <a class="el" href="struct_tensor.html">Tensor</a>. </p>
<p>In the future, this will be the sole piece of information required to dispatch to an operator; however, at the moment, it is not used for dispatch.</p>
<p><a class="el" href="structc10_1_1_tensor_impl.html#a3fff488761f6870393167667d0496f9d" title="Return the TensorTypeId corresponding to this Tensor. ">type_id()</a> and type() are NOT in one-to-one correspondence; we only have a single <a class="el" href="structc10_1_1_tensor_impl.html#a3fff488761f6870393167667d0496f9d" title="Return the TensorTypeId corresponding to this Tensor. ">type_id()</a> for CPU tensors, but many Types (CPUFloatTensor, CPUDoubleTensor...) </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00274">274</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<a class="anchor" id="af2a80605f247ac1cd3ea8309e51f1c76"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="struct_t.html">T</a>* c10::TensorImpl::unsafe_data </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Like <a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002" title="Return a typed data pointer to the actual data which this tensor refers to. ">data&lt;T&gt;()</a>, but performs no checks. </p>
<p>You are responsible for ensuring that all invariants required by <a class="el" href="structc10_1_1_tensor_impl.html#a63073c2e89fa7a07327764a3f44fe002" title="Return a typed data pointer to the actual data which this tensor refers to. ">data()</a> are upheld here.</p>
<p>WARNING: It is NOT valid to call this method on a Variable. See Note [We regret making Variable hold a <a class="el" href="struct_tensor.html">Tensor</a>] </p>

<p>Definition at line <a class="el" href="_tensor_impl_8h_source.html#l00621">621</a> of file <a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a>.</p>

</div>
</div>
<hr/>The documentation for this struct was generated from the following files:<ul>
<li>c10/core/<a class="el" href="_tensor_impl_8h_source.html">TensorImpl.h</a></li>
<li>c10/core/<a class="el" href="_tensor_impl_8cpp_source.html">TensorImpl.cpp</a></li>
</ul>
</div><!-- contents -->
<!-- HTML footer for doxygen 1.8.14-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sat Mar 23 2019 13:05:53 for Caffe2 - C++ API by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
<div class="footerContainer">
  <div id="footer_wrap" class="wrapper footerWrapper">
    <div class="footerBlocks">
      <div id="fb_oss" class="footerSection fbOpenSourceFooter">
          <svg class="facebookOSSLogoSvg" viewBox="0 0 1133.9 1133.9" x="0px" y="0px" height=50 width=50>
            <g>
              <path class="logoRing outerRing" d="M 498.3 3.7 c 153.6 88.9 307.3 177.7 461.1 266.2 c 7.6 4.4 10.3 9.1 10.3 17.8 c -0.3 179.1 -0.2 358.3 0 537.4 c 0 8.1 -2.4 12.8 -9.7 17.1 c -154.5 88.9 -308.8 178.1 -462.9 267.5 c -9 5.2 -15.5 5.3 -24.6 0.1 c -153.9 -89.2 -307.9 -178 -462.1 -266.8 C 3 838.8 0 833.9 0 825.1 c 0.3 -179.1 0.2 -358.3 0 -537.4 c 0 -8.6 2.6 -13.6 10.2 -18 C 164.4 180.9 318.4 92 472.4 3 C 477 -1.5 494.3 -0.7 498.3 3.7 Z M 48.8 555.3 c 0 79.9 0.2 159.9 -0.2 239.8 c -0.1 10 3 15.6 11.7 20.6 c 137.2 78.8 274.2 157.8 411 237.3 c 9.9 5.7 17 5.7 26.8 0.1 c 137.5 -79.8 275.2 -159.2 412.9 -238.5 c 7.4 -4.3 10.5 -8.9 10.5 -17.8 c -0.3 -160.2 -0.3 -320.5 0 -480.7 c 0 -8.8 -2.8 -13.6 -10.3 -18 C 772.1 218 633.1 137.8 494.2 57.4 c -6.5 -3.8 -11.5 -4.5 -18.5 -0.5 C 336.8 137.4 197.9 217.7 58.8 297.7 c -7.7 4.4 -10.2 9.2 -10.2 17.9 C 48.9 395.5 48.8 475.4 48.8 555.3 Z" />
              <path class="logoRing middleRing" d="M 184.4 555.9 c 0 -33.3 -1 -66.7 0.3 -100 c 1.9 -48 24.1 -86 64.7 -110.9 c 54.8 -33.6 110.7 -65.5 167 -96.6 c 45.7 -25.2 92.9 -24.7 138.6 1 c 54.4 30.6 108.7 61.5 162.2 93.7 c 44 26.5 67.3 66.8 68 118.4 c 0.9 63.2 0.9 126.5 0 189.7 c -0.7 50.6 -23.4 90.7 -66.6 116.9 c -55 33.4 -110.8 65.4 -167.1 96.5 c -43.4 24 -89 24.2 -132.3 0.5 c -57.5 -31.3 -114.2 -64 -170 -98.3 c -41 -25.1 -62.9 -63.7 -64.5 -112.2 C 183.5 621.9 184.3 588.9 184.4 555.9 Z M 232.9 556.3 c 0 29.5 0.5 59.1 -0.1 88.6 c -0.8 39.2 16.9 67.1 50.2 86.2 c 51.2 29.4 102.2 59.2 153.4 88.4 c 31.4 17.9 63.6 18.3 95 0.6 c 53.7 -30.3 107.1 -61.2 160.3 -92.5 c 29.7 -17.5 45 -44.5 45.3 -78.8 c 0.6 -61.7 0.5 -123.5 0 -185.2 c -0.3 -34.4 -15.3 -61.5 -44.9 -79 C 637.7 352.6 583 320.8 527.9 290 c -27.5 -15.4 -57.2 -16.1 -84.7 -0.7 c -56.9 31.6 -113.4 64 -169.1 97.6 c -26.4 15.9 -40.7 41.3 -41.1 72.9 C 232.6 491.9 232.9 524.1 232.9 556.3 Z" />
              <path class="logoRing innerRing" d="M 484.9 424.4 c 69.8 -2.8 133.2 57.8 132.6 132 C 617 630 558.5 688.7 484.9 689.1 c -75.1 0.4 -132.6 -63.6 -132.7 -132.7 C 352.1 485 413.4 421.5 484.9 424.4 Z M 401.3 556.7 c -3.4 37.2 30.5 83.6 83 84.1 c 46.6 0.4 84.8 -37.6 84.9 -84 c 0.1 -46.6 -37.2 -84.4 -84.2 -84.6 C 432.2 472.1 397.9 518.3 401.3 556.7 Z" />
            </g>
          </svg>
        <h2>Facebook Open Source</h2>
      </div>
      <div class="footerSection">
        <a class="footerLink" href="https://code.facebook.com/projects/" target="_blank">Open Source Projects</a>
        <a class="footerLink" href="https://github.com/facebook/" target="_blank">GitHub</a>
        <a class="footerLink" href="https://twitter.com/fbOpenSource" target="_blank">Twitter</a>
      </div>
      <div class="footerSection rightAlign">
        <a class="footerLink" href="https://github.com/caffe2/caffe2" target="_blank">Contribute to this project on GitHub</a>
      </div>
    </div>
  </div>
</div>
<script type="text/javascript" src="/js/jekyll-link-anchors.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '{{ site.gacode }}', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
