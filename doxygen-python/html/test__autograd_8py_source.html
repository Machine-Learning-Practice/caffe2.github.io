<!-- HTML header for doxygen 1.8.14-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Caffe2 - Python API: test/test_autograd.py Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="/static/favicon.png" type="image/x-icon">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
<link href="main.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo" width="56"><a href="/"><img alt="Logo" src="Caffe2-with-name-55-tall.png"/></a></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Caffe2 - Python API
   </div>
   <div id="projectbrief">A deep learning, cross platform ML framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="namespaces.html"><span>Packages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li><a href="/doxygen-c/html/classes.html"><span>C++&#160;API</span></a></li>
      <li><a href="/doxygen-python/html/annotated.html"><span>Python&#160;API</span></a></li>
      <li><a href="https://github.com/caffe2/caffe2"><span>GitHub</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_13e138d54eb8818da29c3992edef070a.html">test</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">test_autograd.py</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="keyword">import</span> contextlib</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="keyword">import</span> gc</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="keyword">import</span> sys</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="keyword">import</span> math</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="keyword">import</span> torch</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="keyword">import</span> unittest</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="keyword">import</span> warnings</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="keyword">from</span> itertools <span class="keyword">import</span> product</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="keyword">from</span> operator <span class="keyword">import</span> mul, itemgetter</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="keyword">from</span> functools <span class="keyword">import</span> reduce, wraps</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1__six.html">torch._six</a> <span class="keyword">import</span> inf, nan, istuple</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1autograd_1_1gradcheck.html">torch.autograd.gradcheck</a> <span class="keyword">import</span> gradgradcheck, gradcheck</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1autograd_1_1function.html">torch.autograd.function</a> <span class="keyword">import</span> once_differentiable</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1autograd_1_1profiler.html">torch.autograd.profiler</a> <span class="keyword">import</span> profile</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1utils_1_1checkpoint.html">torch.utils.checkpoint</a> <span class="keyword">import</span> checkpoint</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="keyword">from</span> common_utils <span class="keyword">import</span> (TEST_MKL, TestCase, run_tests, skipIfNoLapack,</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;                          suppress_warnings, skipIfRocm,</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;                          prod_single_zero, random_square_matrix_of_rank,</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;                          random_symmetric_matrix, random_symmetric_psd_matrix,</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;                          random_symmetric_pd_matrix, make_nonzero_det,</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;                          random_fullrank_matrix_distinct_singular_value, load_tests)</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="keyword">from</span> common_cuda <span class="keyword">import</span> TEST_CUDA</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1autograd.html">torch.autograd</a> <span class="keyword">import</span> Variable, Function, detect_anomaly</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1autograd_1_1function.html">torch.autograd.function</a> <span class="keyword">import</span> InplaceFunction</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacetorch_1_1testing.html">torch.testing</a> <span class="keyword">import</span> make_non_contiguous, randn_like</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="keyword">from</span> common_methods_invocations <span class="keyword">import</span> (method_tests,</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;                                        create_input, unpack_variables,</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;                                        EXCLUDE_FUNCTIONAL, EXCLUDE_GRADCHECK,</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;                                        EXCLUDE_GRADGRADCHECK,</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;                                        EXCLUDE_GRADGRADCHECK_BY_TEST_NAME,</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;                                        exclude_tensor_method,</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;                                        mask_not_all_zeros,</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;                                        L, S)</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;<span class="comment"># load_tests from common_utils is used to automatically filter tests for</span></div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;<span class="comment"># sharding on sandcastle. This line silences flake warnings</span></div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;load_tests = load_tests</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;<span class="keywordflow">if</span> sys.version_info[0] == 2:</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;    <span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;<span class="keywordflow">else</span>:</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;    <span class="keyword">import</span> pickle</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;PRECISION = 1e-4</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;@contextlib.contextmanager</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;<span class="keyword">def </span>backward_engine(engine):</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;    _prev_engine = Variable._execution_engine</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;    Variable._execution_engine = engine()</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;    <span class="keywordflow">try</span>:</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;        <span class="keywordflow">yield</span></div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;    <span class="keywordflow">finally</span>:</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;        Variable._execution_engine = _prev_engine</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;<span class="keyword">def </span>graph_desc(fn):</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;    <span class="keywordflow">if</span> fn <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;        <span class="keywordflow">return</span> <span class="stringliteral">&#39;None&#39;</span></div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;    result = type(fn).__name__ + <span class="stringliteral">&#39;(&#39;</span></div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;    next_functions = fn.next_functions</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;    <span class="keywordflow">for</span> next_fn, _ <span class="keywordflow">in</span> next_functions:</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;        result += graph_desc(next_fn)</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;        result += <span class="stringliteral">&#39;, &#39;</span></div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;    <span class="keywordflow">if</span> next_functions:</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;        result = result[:-2]</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;    <span class="keywordflow">return</span> result + <span class="stringliteral">&#39;)&#39;</span></div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;</div><div class="line"><a name="l00072"></a><span class="lineno"><a class="line" href="classtest__autograd_1_1_test_autograd.html">   72</a></span>&#160;<span class="keyword">class </span><a class="code" href="classtest__autograd_1_1_test_autograd.html">TestAutograd</a>(TestCase):</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;    <span class="keyword">def </span>_function_test(self, cls):</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;        result = cls.apply(x, 2, y)</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;        go = torch.ones((), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;        result.sum().backward(go, create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;        self.assertEqual(x.grad.data, y.data + torch.ones(5, 5))</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;        self.assertEqual(y.grad.data, x.data + torch.ones(5, 5) * 2)</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;        self.assertIsNotNone(x.grad.grad_fn)</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;        self.assertIsNotNone(y.grad.grad_fn)</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;        <span class="keywordflow">return</span> x, y</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;    <span class="keyword">def </span>test_function(self):</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;            @staticmethod</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;            <span class="keyword">def </span>forward(ctx, tensor1, pyscalar, tensor2):</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;                ctx.pyscalar = pyscalar</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;                ctx.save_for_backward(tensor1, tensor2)</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;                <span class="keywordflow">return</span> tensor1 + pyscalar * tensor2 + tensor1 * tensor2</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;            @staticmethod</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;                var1, var2 = ctx.saved_tensors</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;                <span class="comment"># NOTE: self is the test case here</span></div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;                self.assertIsInstance(var1, torch.Tensor)</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;                self.assertIsInstance(var2, torch.Tensor)</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;                self.assertIsInstance(grad_output, torch.Tensor)</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;                <span class="keywordflow">return</span> (grad_output + grad_output * var2, <span class="keywordtype">None</span>,</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;                        grad_output * ctx.pyscalar + grad_output * var1)</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;        x, y = self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a3195838ca5a03a3b24576c7146e93f44">_function_test</a>(MyFunction)</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;        x_grad_desc = graph_desc(x.grad.grad_fn)</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;        y_grad_desc = graph_desc(y.grad.grad_fn)</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;        self.assertExpected(x_grad_desc, <span class="stringliteral">&quot;x_grad_desc&quot;</span>)</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;        self.assertExpected(y_grad_desc, <span class="stringliteral">&quot;y_grad_desc&quot;</span>)</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;    <span class="keyword">def </span>test_once_differentiable(self):</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;            @staticmethod</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;            <span class="keyword">def </span>forward(ctx, tensor1, pyscalar, tensor2):</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;                ctx.pyscalar = pyscalar</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;                ctx.save_for_backward(tensor1, tensor2)</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;                <span class="keywordflow">return</span> tensor1 + pyscalar * tensor2 + tensor1 * tensor2</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;            @staticmethod</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;            @once_differentiable</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;                self.assertFalse(torch.is_grad_enabled())</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;                t1, t2 = ctx.saved_tensors</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;                <span class="keywordflow">return</span> (grad_output + grad_output * t2, <span class="keywordtype">None</span>,</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;                        grad_output * ctx.pyscalar + grad_output * t1)</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;        x, y = self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a3195838ca5a03a3b24576c7146e93f44">_function_test</a>(MyFunction)</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;        self.assertEqual(graph_desc(x.grad.grad_fn),</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;                         <span class="stringliteral">&#39;CloneBackward(Error(AccumulateGrad(), None, AccumulateGrad()))&#39;</span>)</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;        self.assertEqual(graph_desc(y.grad.grad_fn),</div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;                         <span class="stringliteral">&#39;CloneBackward(Error(AccumulateGrad(), None, AccumulateGrad()))&#39;</span>)</div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;</div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;    <span class="keyword">def </span>test_function_returns_input(self):</div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;            @staticmethod</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;            <span class="keyword">def </span>forward(ctx, x):</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;                <span class="keywordflow">return</span> x</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;</div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;            @staticmethod</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;            <span class="keyword">def </span>backward(ctx, grad):</div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;                <span class="keywordflow">return</span> grad * 2</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;</div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;        <span class="keywordflow">for</span> shape <span class="keywordflow">in</span> [(1,), ()]:</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;            v = torch.ones(shape, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;            MyFunction.apply(v).backward()</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;            self.assertEqual(v.grad, torch.full(shape, 2))</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;            v.grad.data.zero_()</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;            MyFunction.apply(v.clone()).backward()</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;            self.assertEqual(v.grad, torch.full(shape, 2))</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;    <span class="keyword">def </span>test_legacy_function_none_grad(self):</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;            <span class="keyword">def </span>forward(self, x):</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;                <span class="keywordflow">return</span> torch.zeros(2, 2, 2)</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;            <span class="keyword">def </span>backward(self, grad_output):</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;                <span class="keywordflow">return</span> <span class="keywordtype">None</span></div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;        shape = (2, 3)</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;        v = torch.ones(shape, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;        y = v[0, 0].expand(3, 5).t().sum()</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;        MyFunction()(y).sum().backward()</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;        self.assertEqual(v.grad.data, torch.zeros(shape))</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;    <span class="keyword">def </span>test_invalid_gradients(self):</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;            @staticmethod</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;            <span class="keyword">def </span>forward(ctx, x):</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;                <span class="keywordflow">return</span> x * 2</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;            @staticmethod</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;                <span class="keywordflow">return</span> torch.randn(10, dtype=torch.float)</div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;expected shape&#39;</span>):</div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;            input = torch.randn(5, 5, dtype=torch.float, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;            MyFunction.apply(input).sum().backward()</div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;expected type&#39;</span>):</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;            input = torch.randn(10, dtype=torch.double, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;            MyFunction.apply(input).sum().backward()</div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;    <span class="keyword">def </span>test_accumulate_grad(self):</div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;        grad_output = torch.ones(5, 5)</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;        <span class="keyword">def </span>compute_grad(create_graph):</div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;            x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;            y = x + 2</div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;            y.backward(grad_output, retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;            x_grad = x.grad</div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;            x_grad_clone = x.grad.clone()</div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;            y.backward(grad_output, create_graph=create_graph)</div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;            <span class="keywordflow">return</span> x_grad, x_grad_clone</div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;</div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;        <span class="comment"># Accumulate in-place when create_graph is False</span></div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;        x_grad, x_grad_clone = compute_grad(create_graph=<span class="keyword">False</span>)</div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;        self.assertEqual(x_grad, x_grad_clone * 2)</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;</div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;        <span class="comment"># Accumulate out-of-place when create_graph is False</span></div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;        x_grad, x_grad_clone = compute_grad(create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;        self.assertEqual(x_grad, x_grad_clone)</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;</div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;    <span class="keyword">def </span>test_slogdet_sign(self):</div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;        a = torch.randn(3, 3, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;        s, logdet = a.slogdet()</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;</div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;        <span class="comment"># test that sign should not require grad</span></div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;        self.assertFalse(s.requires_grad)</div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;        <span class="comment"># test that backward through computation involving sign works</span></div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;        <span class="keyword">def </span>sign_mul_logdet(mat):</div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;            s, logdet = mat.slogdet()</div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;            <span class="keywordflow">return</span> s * logdet</div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;</div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;        u, s, v = a.detach().svd()</div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;        s.abs_().clamp_(0.0001)</div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;        <span class="keywordflow">for</span> sign <span class="keywordflow">in</span> (-1, 1):</div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;            s[-1] = sign</div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;            mat = torch.chain_matmul(u, s.diag(), v.t()).requires_grad_()</div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;            gradcheck(sign_mul_logdet, mat)</div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;            gradgradcheck(sign_mul_logdet, mat)</div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;</div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;    <span class="keyword">def </span>test_sum_to_with_empty_dim_grad(self):</div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;        a = torch.rand(4, 0, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;        b = torch.rand(4, 1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;        c = a + b</div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;        <span class="keyword">assert</span> c.shape == (4, 0)</div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;        c.sum().backward()</div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;</div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;        self.assertEqual(b.grad, torch.zeros(4, 1))</div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;        self.assertEqual(a.grad, torch.zeros(4, 0))</div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;</div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;    <span class="keyword">def </span>test_hessian_vector(self):</div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;        x = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;        y = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;</div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;        z = x ** 2 + y * x + y ** 2</div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;        z.backward(torch.ones(2, 2), create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;</div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;        x_grad = 2 * x.data + y.data</div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;        y_grad = x.data + 2 * y.data</div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;        self.assertEqual(x.grad.data, x_grad)</div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;        self.assertEqual(y.grad.data, y_grad)</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;        grad_sum = 2 * x.grad + y.grad</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;        grad_sum.backward(torch.ones(2, 2))</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;        x_hv = torch.ones(2, 2) * 5</div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;        y_hv = torch.ones(2, 2) * 4</div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;        self.assertEqual(x.grad.data, x_grad + x_hv)</div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;        self.assertEqual(y.grad.data, y_grad + y_hv)</div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;</div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;    <span class="keyword">def </span>test_grad(self):</div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;        x = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;        y = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;        z = x ** 2 + y * x + y ** 2</div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;        z.backward(torch.ones(2, 2), create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;</div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;        x_grad = 2 * x.data + y.data</div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;        y_grad = x.data + 2 * y.data</div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;        self.assertEqual(x.grad.data, x_grad)</div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;        self.assertEqual(y.grad.data, y_grad)</div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;</div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;        grad_sum = 2 * x.grad + y.grad</div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;        x_hv = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(</div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;            outputs=[grad_sum], grad_outputs=[torch.ones(2, 2)],</div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;            inputs=[x], create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;        expected_x_hv = torch.ones(2, 2) * 5</div><div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;        expected_y_hv = torch.ones(2, 2) * 4</div><div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;</div><div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;        self.assertEqual(x_hv[0].data, expected_x_hv)</div><div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;        self.assertEqual(x.grad.data, x_grad)</div><div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;        self.assertEqual(y.grad.data, y_grad)</div><div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;</div><div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;    <span class="keyword">def </span>test_grad_nonleaf(self):</div><div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;        x_init = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;        x = x_init</div><div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;        y = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;        grad_output = torch.ones(2, 2)</div><div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;</div><div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;        <span class="keyword">def </span>fn(x):</div><div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;            <span class="keywordflow">return</span> x ** 2 + y * x + y ** 2</div><div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;</div><div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;        <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(5):</div><div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;            grad_x, = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(</div><div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;                fn(x), x, grad_outputs=grad_output, create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;</div><div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;            grad_x_expected = 2 * x.data + y.data</div><div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;            self.assertIsNone(y.grad)</div><div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;            self.assertIsNone(x.grad)</div><div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;            self.assertEqual(grad_x.data, grad_x_expected)</div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;</div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;            x = x + 0.05 * grad_x</div><div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;</div><div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;        val_init = fn(x_init).data.sum()</div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;        val_final = fn(x).data.sum()</div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;        self.assertGreater(val_final, val_init)</div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;</div><div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;        x.backward(grad_output)</div><div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;        self.assertIsNotNone(y.grad)</div><div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;        self.assertIsNotNone(x_init.grad)</div><div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;</div><div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;    <span class="keyword">def </span>test_grad_nonleaf_many_outputs(self):</div><div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;        <span class="comment"># This checks an edge case for function callbacks</span></div><div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;        <span class="comment"># We want to capture two grads of a function, but can only</span></div><div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;        <span class="comment"># register a single callback.</span></div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;        x = torch.randn(4, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;        a, b = x.chunk(2)</div><div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;</div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;        <span class="keyword">def </span>hook(*grads):</div><div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;            hook_called[0] = <span class="keyword">True</span></div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;        hook_called = [<span class="keyword">False</span>]</div><div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;        x.register_hook(hook)</div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;</div><div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;        go = torch.randn(2, 2)</div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;        grad_a, grad_b = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(</div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;            (a + 2 * b), [a, b], grad_outputs=go, create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;</div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;        self.assertEqual(grad_a.data, go)</div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;        self.assertEqual(grad_b.data, go * 2)</div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;        self.assertFalse(hook_called[0])</div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;        self.assertIsNone(x.grad)</div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;</div><div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;    <span class="keyword">def </span>test_grad_nonleaf_register_hook(self):</div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;        <span class="comment"># This checks an edge case for register_hook.</span></div><div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;        <span class="comment"># We want to capture grad of a nonleaf tensor,</span></div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;        <span class="comment"># but avoid segfault during backward of other nonleaf tensors</span></div><div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;        x = torch.randn(5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;        x_list = x.unbind()</div><div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;</div><div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;        x0 = x_list[0]</div><div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;        hook_results = [<span class="keywordtype">None</span>]</div><div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;</div><div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;        <span class="keyword">def </span>hook(grad):</div><div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;            hook_results[0] = grad</div><div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;        x0.register_hook(hook)</div><div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;</div><div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;        x_list[0].backward()</div><div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;        self.assertEqual(hook_results[0], <a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>(1.))</div><div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;        expected_grad = <a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>([1., 0, 0, 0, 0])</div><div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;        self.assertEqual(x.grad, expected_grad)</div><div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;        self.assertIsNone(x_list[0].grad)</div><div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;</div><div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(1, 5, 1):</div><div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;            x_list[i].backward()</div><div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;            self.assertEqual(hook_results[0], <span class="keywordtype">None</span>)</div><div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;            expected_grad[i] = 1.0</div><div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;            self.assertEqual(x.grad, expected_grad)</div><div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;            self.assertIsNone(x_list[i].grad)</div><div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;</div><div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;    <span class="keyword">def </span>test_sharded_grad(self):</div><div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;        leaves = [torch.zeros(5, 5, requires_grad=<span class="keyword">True</span>) <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(10)]</div><div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;        intermediates = [l * i + l * l <span class="keywordflow">for</span> i, l <span class="keywordflow">in</span> enumerate(leaves)]</div><div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;        loss = sum(v * i <span class="keywordflow">for</span> i, v <span class="keywordflow">in</span> enumerate(intermediates)).sum()</div><div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;</div><div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;        <span class="comment"># define a helper for dividing intermediates into groups</span></div><div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;        <span class="keyword">def </span>group(l, group_size):</div><div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;            <span class="keywordflow">return</span> (l[i:i + group_size] <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(0, len(l), group_size))</div><div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;</div><div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;        <span class="comment"># Compute the d loss / d intermediates in chunks of shard_size</span></div><div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;        shard_size = 2</div><div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;        d_intermediates = [d_i <span class="keywordflow">for</span> intermediates_batch <span class="keywordflow">in</span> group(intermediates, shard_size)</div><div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;                           <span class="keywordflow">for</span> d_i <span class="keywordflow">in</span> <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(loss, intermediates_batch)]</div><div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;        <span class="comment"># Compute rest of backward pass</span></div><div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;        <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>(intermediates, d_intermediates)</div><div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;</div><div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;        <span class="keywordflow">for</span> i, l <span class="keywordflow">in</span> enumerate(leaves):</div><div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;            self.assertEqual(l.grad.data, i * i * (1 + l.data))</div><div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;</div><div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;    <span class="keyword">def </span>test_backward_badcalls(self):</div><div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;        x = torch.ones(1)</div><div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;does not require grad&#39;</span>):</div><div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;            x.backward()</div><div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;</div><div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;    <span class="keyword">def </span>test_grad_badcalls(self):</div><div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;        x = torch.ones(1)</div><div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;        y = x ** 2</div><div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;does not require grad&#39;</span>):</div><div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;            <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(x, y)</div><div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;does not require grad&#39;</span>):</div><div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;            <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(y, x)</div><div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;</div><div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;        x = torch.ones(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;        y = x ** 2</div><div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;        <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(y, x)  <span class="comment"># this should succeed now</span></div><div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;</div><div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;    <span class="keyword">def </span>test_grad_fn_badcalls(self):</div><div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;        error_regex = <span class="stringliteral">&#39;expected .* arguments, got .* instead&#39;</span></div><div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;        x = torch.ones(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;        y = x ** 2</div><div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;        with self.assertRaisesRegex(TypeError, error_regex):</div><div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;            y.grad_fn(x.detach(), x.detach())  <span class="comment"># too many</span></div><div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;        with self.assertRaisesRegex(TypeError, error_regex):</div><div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;            y.grad_fn()  <span class="comment"># too few</span></div><div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;</div><div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;        y.grad_fn(x.detach())  <span class="comment"># this should succeed</span></div><div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;</div><div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;    <span class="keyword">def </span>test_grad_unreachable(self):</div><div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;        x = torch.ones(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;        y = torch.ones(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;        <span class="comment"># Make sure x and y have grad accumulators allocated</span></div><div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;        z = x * 2</div><div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;        w = y * 2</div><div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;</div><div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;        grad_x, grad_y = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(x * 2, [x, y], allow_unused=<span class="keyword">True</span>)</div><div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;        self.assertEqual(grad_x, x * 2)</div><div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;        self.assertIsNone(grad_y)</div><div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;</div><div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;        <span class="comment"># This is slightly different than the case above, because z doesn&#39;t even</span></div><div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;        <span class="comment"># have a grad accumulator allocated.</span></div><div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;        z = torch.ones(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;        grad_x, grad_z = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(x * 2, [x, z], allow_unused=<span class="keyword">True</span>)</div><div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;        self.assertEqual(grad_x, x * 2)</div><div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;        self.assertIsNone(grad_z)</div><div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;</div><div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;    <span class="keyword">def </span>test_hooks(self):</div><div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;        x = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;        y = Variable(torch.ones(5, 5) * 4, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;</div><div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;        counter = [0]</div><div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;</div><div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;        <span class="keyword">def </span>bw_hook(inc, grad):</div><div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;            self.assertIsInstance(grad, torch.Tensor)</div><div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;            counter[0] += inc</div><div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;</div><div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;        z = x ** 2 + x * 2 + x * y + y</div><div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;        x.register_hook(<span class="keyword">lambda</span> *args: bw_hook(0, *args))</div><div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;        test = z.register_hook(<span class="keyword">lambda</span> *args: bw_hook(1, *args))</div><div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;        z.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;        self.assertEqual(counter[0], 1)</div><div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;</div><div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;        test2 = z.register_hook(<span class="keyword">lambda</span> *args: bw_hook(2, *args))</div><div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;        z.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;        self.assertEqual(counter[0], 4)</div><div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;</div><div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;        test2.remove()</div><div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;        z.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;        self.assertEqual(counter[0], 5)</div><div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;</div><div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;        <span class="keyword">def </span>bw_hook_modify(grad):</div><div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;            <span class="keywordflow">return</span> grad.mul(2)</div><div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;</div><div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;        test.remove()</div><div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;        z.register_hook(bw_hook_modify)</div><div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;        y.grad.data.zero_()</div><div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;        z.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;        self.assertEqual(y.grad.data, (x.data + 1) * 2)</div><div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;</div><div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;        y.register_hook(bw_hook_modify)</div><div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;        y.grad.data.zero_()</div><div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;        z.backward(torch.ones(5, 5))</div><div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;        self.assertEqual(y.grad.data, (x.data + 1) * 4)</div><div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;</div><div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;    <span class="keyword">def </span>test_hooks_cpp(self):</div><div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;        <span class="comment"># Tests hooks for autograd function implemented in C++</span></div><div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;        bn = torch.nn.BatchNorm1d(5, affine=<span class="keyword">False</span>)</div><div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;        bn.eval()</div><div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;</div><div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;        counter = [0]</div><div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;</div><div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;        <span class="keyword">def </span>bw_hook(grad):</div><div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;            counter[0] += 1</div><div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;            <span class="keywordflow">return</span> grad * 2</div><div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;</div><div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;        x = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;        z = bn(x)</div><div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;        z.register_hook(bw_hook)</div><div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;        z.sum().backward()</div><div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;</div><div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;        self.assertEqual(counter[0], 1, <span class="stringliteral">&#39;bw_hook not called&#39;</span>)</div><div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;        self.assertEqual(x.grad.data, torch.ones(5, 5) * 2)</div><div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;</div><div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;    <span class="keyword">def </span>test_hook_none(self):</div><div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;        <span class="comment"># WARNING: this is a test for autograd internals.</span></div><div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;        <span class="comment"># You should never have to use such things in your code.</span></div><div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;        <span class="keyword">class </span>NoneGradientFunction(Function):</div><div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;</div><div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;            <span class="keyword">def </span>forward(self, x, y):</div><div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;                <span class="keyword">assert</span> self.needs_input_grad[0]</div><div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;                <span class="keyword">assert</span> <span class="keywordflow">not</span> self.needs_input_grad[1]</div><div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;                <span class="keywordflow">return</span> x, y</div><div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;</div><div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;            <span class="keyword">def </span>backward(self, grad_x, grad_y):</div><div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;                <span class="keywordflow">return</span> grad_x, <span class="keywordtype">None</span></div><div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;</div><div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;        fn = NoneGradientFunction()</div><div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;        was_called = [<span class="keyword">False</span>]</div><div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;</div><div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;        <span class="keyword">def </span>hook(grad_input, grad_output):</div><div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;            self.assertIsInstance(grad_input, tuple)</div><div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;            self.assertIsInstance(grad_output, tuple)</div><div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;            self.assertIsNotNone(grad_input[0])</div><div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;            self.assertIsNotNone(grad_input[1])</div><div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;            self.assertIsNotNone(grad_output[0])</div><div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;            self.assertIsNotNone(grad_output[1])</div><div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;            was_called[0] = <span class="keyword">True</span></div><div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;        fn.register_hook(hook)</div><div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;</div><div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;        y = torch.randn(5, 5)</div><div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;        sum(fn(x, y)).sum().backward()</div><div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;        self.assertTrue(was_called[0])</div><div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;</div><div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;    <span class="keyword">def </span>test_retain_grad(self):</div><div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;        input = torch.rand(1, 3, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;        h1 = input * 3</div><div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;        out = (h1 * h1).sum()</div><div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;</div><div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;        <span class="comment"># It should be possible to call retain_grad() multiple times</span></div><div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;        h1.retain_grad()</div><div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;        h1.retain_grad()</div><div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;</div><div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;        <span class="comment"># Gradient should be accumulated</span></div><div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;        out.backward(retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;        self.assertEqual(h1.data * 2, h1.grad.data)</div><div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;        out.backward(retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;        self.assertEqual(h1.data * 4, h1.grad.data)</div><div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;</div><div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;        input.grad.data.zero_()</div><div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;        <span class="comment"># It should be a no-op for leaves</span></div><div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;        input.retain_grad()</div><div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;        input.retain_grad()</div><div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;        out.backward()</div><div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;        self.assertEqual(input.data * 18, input.grad.data)</div><div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;</div><div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;    <span class="keyword">def </span>test_retain_grad_cycle(self):</div><div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;        <span class="keyword">import</span> gc</div><div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;        <span class="keyword">import</span> weakref</div><div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;        counter = [0]</div><div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;        refs = [<span class="keywordtype">None</span>]</div><div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;</div><div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;        x = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;</div><div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;        <span class="keyword">def </span>run_test():</div><div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;            y = x * 2</div><div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;            y.retain_grad()</div><div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;</div><div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;            <span class="keyword">def </span>inc(*args):</div><div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;                counter[0] += 1</div><div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;            refs[0] = weakref.ref(y, inc)</div><div class="line"><a name="l00544"></a><span class="lineno">  544</span>&#160;            <span class="keywordflow">return</span> y / 2</div><div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;</div><div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;        z = run_test()</div><div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;        gc.collect()</div><div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;        self.assertIsNone(refs[0]())</div><div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;        self.assertEqual(counter[0], 1)</div><div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;        z.sum().backward()</div><div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;</div><div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;    <span class="keyword">def </span>test_backward(self):</div><div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;        v_t = torch.randn(5, 5)</div><div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;        x_t = torch.randn(5, 5)</div><div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;        y_t = torch.rand(5, 5) + 0.1</div><div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;        z_t = torch.randn(5, 5)</div><div class="line"><a name="l00557"></a><span class="lineno">  557</span>&#160;        grad_output = torch.randn(5, 5)</div><div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;        v = Variable(v_t, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;        x = Variable(x_t, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;        y = Variable(y_t, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;        z = Variable(z_t, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;</div><div class="line"><a name="l00563"></a><span class="lineno">  563</span>&#160;        v.backward(grad_output)</div><div class="line"><a name="l00564"></a><span class="lineno">  564</span>&#160;        self.assertEqual(v.grad.data, grad_output)</div><div class="line"><a name="l00565"></a><span class="lineno">  565</span>&#160;</div><div class="line"><a name="l00566"></a><span class="lineno">  566</span>&#160;        a = x + (y * z) + 4 * z ** 2 * x / y</div><div class="line"><a name="l00567"></a><span class="lineno">  567</span>&#160;        a.backward(grad_output)</div><div class="line"><a name="l00568"></a><span class="lineno">  568</span>&#160;        x_grad = 4 * z_t.pow(2) / y_t + 1</div><div class="line"><a name="l00569"></a><span class="lineno">  569</span>&#160;        y_grad = z_t - 4 * x_t * z_t.pow(2) / y_t.pow(2)</div><div class="line"><a name="l00570"></a><span class="lineno">  570</span>&#160;        z_grad = 8 * x_t * z_t / y_t + y_t</div><div class="line"><a name="l00571"></a><span class="lineno">  571</span>&#160;        self.assertEqual(x.grad.data, x_grad * grad_output)</div><div class="line"><a name="l00572"></a><span class="lineno">  572</span>&#160;        self.assertEqual(y.grad.data, y_grad * grad_output)</div><div class="line"><a name="l00573"></a><span class="lineno">  573</span>&#160;        self.assertEqual(z.grad.data, z_grad * grad_output)</div><div class="line"><a name="l00574"></a><span class="lineno">  574</span>&#160;</div><div class="line"><a name="l00575"></a><span class="lineno">  575</span>&#160;    <span class="keyword">def </span>test_sparse_backward(self):</div><div class="line"><a name="l00576"></a><span class="lineno">  576</span>&#160;        <span class="keyword">class </span>FixedGradientFunction(Function):</div><div class="line"><a name="l00577"></a><span class="lineno">  577</span>&#160;            <span class="keyword">def </span>__init__(self, grad):</div><div class="line"><a name="l00578"></a><span class="lineno">  578</span>&#160;                self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#acc21b5260d1198c9b945b956c60afb11">grad</a> = grad</div><div class="line"><a name="l00579"></a><span class="lineno">  579</span>&#160;</div><div class="line"><a name="l00580"></a><span class="lineno">  580</span>&#160;            <span class="keyword">def </span>forward(self, x):</div><div class="line"><a name="l00581"></a><span class="lineno">  581</span>&#160;                <span class="keywordflow">return</span> x</div><div class="line"><a name="l00582"></a><span class="lineno">  582</span>&#160;</div><div class="line"><a name="l00583"></a><span class="lineno">  583</span>&#160;            <span class="keyword">def </span>backward(self, grad_x):</div><div class="line"><a name="l00584"></a><span class="lineno">  584</span>&#160;                <span class="keywordflow">return</span> self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#acc21b5260d1198c9b945b956c60afb11">grad</a></div><div class="line"><a name="l00585"></a><span class="lineno">  585</span>&#160;</div><div class="line"><a name="l00586"></a><span class="lineno">  586</span>&#160;        size = torch.Size([6, 3, 2])</div><div class="line"><a name="l00587"></a><span class="lineno">  587</span>&#160;        i1 = torch.LongTensor([</div><div class="line"><a name="l00588"></a><span class="lineno">  588</span>&#160;            [0, 3, 4],</div><div class="line"><a name="l00589"></a><span class="lineno">  589</span>&#160;            [0, 2, 2],</div><div class="line"><a name="l00590"></a><span class="lineno">  590</span>&#160;        ])</div><div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;        v1 = torch.DoubleTensor([[1, 2], [4, 5], [7, 8]])</div><div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160;        sparse_grad1 = torch.sparse.DoubleTensor(i1, v1, size)</div><div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160;        i2 = torch.LongTensor([</div><div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160;            [0, 1, 3, 4],</div><div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160;            [0, 1, 2, 2],</div><div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;        ])</div><div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;        v2 = torch.DoubleTensor([[1, 2], [4, 3], [4, 5], [7, 8]])</div><div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;        sparse_grad2 = torch.sparse.DoubleTensor(i2, v2, size)</div><div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;        dense_grad = torch.rand(size).double()</div><div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;        sparse_fn1 = FixedGradientFunction(sparse_grad1)</div><div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;        sparse_fn2 = FixedGradientFunction(sparse_grad2)</div><div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;        dense_fn = FixedGradientFunction(dense_grad)</div><div class="line"><a name="l00603"></a><span class="lineno">  603</span>&#160;</div><div class="line"><a name="l00604"></a><span class="lineno">  604</span>&#160;        <span class="comment"># sparse first</span></div><div class="line"><a name="l00605"></a><span class="lineno">  605</span>&#160;        x = torch.randn(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00606"></a><span class="lineno">  606</span>&#160;        (sparse_fn1(x) + dense_fn(x) + sparse_fn2(x)).sum().backward()</div><div class="line"><a name="l00607"></a><span class="lineno">  607</span>&#160;        self.assertEqual(x.grad, dense_grad + sparse_grad1 + sparse_grad2)</div><div class="line"><a name="l00608"></a><span class="lineno">  608</span>&#160;        <span class="comment"># dense first</span></div><div class="line"><a name="l00609"></a><span class="lineno">  609</span>&#160;        x = torch.randn(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00610"></a><span class="lineno">  610</span>&#160;        (dense_fn(x) + sparse_fn1(x) + sparse_fn2(x)).sum().backward()</div><div class="line"><a name="l00611"></a><span class="lineno">  611</span>&#160;        self.assertEqual(x.grad, dense_grad + sparse_grad1 + sparse_grad2)</div><div class="line"><a name="l00612"></a><span class="lineno">  612</span>&#160;        <span class="comment"># sparse only</span></div><div class="line"><a name="l00613"></a><span class="lineno">  613</span>&#160;        x = torch.randn(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00614"></a><span class="lineno">  614</span>&#160;        (sparse_fn1(x) + sparse_fn2(x)).sum().backward()</div><div class="line"><a name="l00615"></a><span class="lineno">  615</span>&#160;        self.assertEqual(x.grad, sparse_grad1 + sparse_grad2)</div><div class="line"><a name="l00616"></a><span class="lineno">  616</span>&#160;</div><div class="line"><a name="l00617"></a><span class="lineno">  617</span>&#160;    <span class="keyword">def </span>test_sparse_mm_backward(self):</div><div class="line"><a name="l00618"></a><span class="lineno">  618</span>&#160;        size = (3, 3)</div><div class="line"><a name="l00619"></a><span class="lineno">  619</span>&#160;        sparse = torch.sparse_coo_tensor(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00620"></a><span class="lineno">  620</span>&#160;        dense = torch.randn(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00621"></a><span class="lineno">  621</span>&#160;</div><div class="line"><a name="l00622"></a><span class="lineno">  622</span>&#160;        z = sparse.mm(dense)</div><div class="line"><a name="l00623"></a><span class="lineno">  623</span>&#160;        with self.assertRaisesRegex(RuntimeError,</div><div class="line"><a name="l00624"></a><span class="lineno">  624</span>&#160;                                    <span class="stringliteral">&quot;calculating the gradient of a sparse Tensor argument to mm is not supported.&quot;</span>):</div><div class="line"><a name="l00625"></a><span class="lineno">  625</span>&#160;            z.sum().backward()</div><div class="line"><a name="l00626"></a><span class="lineno">  626</span>&#160;</div><div class="line"><a name="l00627"></a><span class="lineno">  627</span>&#160;        z = dense.addmm(sparse, dense)</div><div class="line"><a name="l00628"></a><span class="lineno">  628</span>&#160;        with self.assertRaisesRegex(RuntimeError,</div><div class="line"><a name="l00629"></a><span class="lineno">  629</span>&#160;                                    <span class="stringliteral">&quot;calculating the gradient of a sparse Tensor argument to mm is not supported.&quot;</span>):</div><div class="line"><a name="l00630"></a><span class="lineno">  630</span>&#160;            z.sum().backward()</div><div class="line"><a name="l00631"></a><span class="lineno">  631</span>&#160;</div><div class="line"><a name="l00632"></a><span class="lineno">  632</span>&#160;    @skipIfRocm</div><div class="line"><a name="l00633"></a><span class="lineno">  633</span>&#160;    <span class="keyword">def </span>test_sparse_ctor_getter_backward(self):</div><div class="line"><a name="l00634"></a><span class="lineno">  634</span>&#160;        <span class="comment"># See NOTE [ Sparse: autograd and API ] on the expected behavior of this test</span></div><div class="line"><a name="l00635"></a><span class="lineno">  635</span>&#160;        <span class="keyword">def </span><a class="code" href="namespacetest.html">test</a>(size, sparse_dim, nnz, device):</div><div class="line"><a name="l00636"></a><span class="lineno">  636</span>&#160;            v_size = [nnz] + list(size[sparse_dim:])</div><div class="line"><a name="l00637"></a><span class="lineno">  637</span>&#160;            i = torch.rand(sparse_dim, nnz)</div><div class="line"><a name="l00638"></a><span class="lineno">  638</span>&#160;            i.mul_(<a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>(size[:sparse_dim]).unsqueeze(1).to(i))</div><div class="line"><a name="l00639"></a><span class="lineno">  639</span>&#160;            i = i.to(torch.long)</div><div class="line"><a name="l00640"></a><span class="lineno">  640</span>&#160;</div><div class="line"><a name="l00641"></a><span class="lineno">  641</span>&#160;            inp = torch.randn(v_size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00642"></a><span class="lineno">  642</span>&#160;            other = self.genSparseTensor(size, sparse_dim, nnz, is_uncoalesced=<span class="keyword">True</span>)[0]</div><div class="line"><a name="l00643"></a><span class="lineno">  643</span>&#160;            other = other.to(device)</div><div class="line"><a name="l00644"></a><span class="lineno">  644</span>&#160;</div><div class="line"><a name="l00645"></a><span class="lineno">  645</span>&#160;            <span class="keyword">def </span>fn(v):</div><div class="line"><a name="l00646"></a><span class="lineno">  646</span>&#160;                x = torch.sparse_coo_tensor(i, v, size, device=device)</div><div class="line"><a name="l00647"></a><span class="lineno">  647</span>&#160;                y = (x + other).coalesce()</div><div class="line"><a name="l00648"></a><span class="lineno">  648</span>&#160;                yv = y.values()</div><div class="line"><a name="l00649"></a><span class="lineno">  649</span>&#160;                new_v = yv.tanh()</div><div class="line"><a name="l00650"></a><span class="lineno">  650</span>&#160;                z = torch.sparse_coo_tensor(y.indices(), new_v, y.size())</div><div class="line"><a name="l00651"></a><span class="lineno">  651</span>&#160;                <span class="keywordflow">return</span> z.coalesce().values()</div><div class="line"><a name="l00652"></a><span class="lineno">  652</span>&#160;</div><div class="line"><a name="l00653"></a><span class="lineno">  653</span>&#160;            gradcheck(fn, (inp,))</div><div class="line"><a name="l00654"></a><span class="lineno">  654</span>&#160;            <span class="comment"># FIXME: make gradgradcheck work.</span></div><div class="line"><a name="l00655"></a><span class="lineno">  655</span>&#160;            <span class="comment"># gradgradcheck(fn, (inp,))</span></div><div class="line"><a name="l00656"></a><span class="lineno">  656</span>&#160;</div><div class="line"><a name="l00657"></a><span class="lineno">  657</span>&#160;            <span class="comment"># assert that _values is non-differentiable</span></div><div class="line"><a name="l00658"></a><span class="lineno">  658</span>&#160;            with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&quot;does not have a grad_fn&quot;</span>):</div><div class="line"><a name="l00659"></a><span class="lineno">  659</span>&#160;                other.detach().requires_grad_()._values().backward(torch.ones_like(other._values()))</div><div class="line"><a name="l00660"></a><span class="lineno">  660</span>&#160;</div><div class="line"><a name="l00661"></a><span class="lineno">  661</span>&#160;        devices = [<span class="stringliteral">&#39;cpu&#39;</span>]</div><div class="line"><a name="l00662"></a><span class="lineno">  662</span>&#160;</div><div class="line"><a name="l00663"></a><span class="lineno">  663</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>():</div><div class="line"><a name="l00664"></a><span class="lineno">  664</span>&#160;            devices.append(<span class="stringliteral">&#39;cuda&#39;</span>)</div><div class="line"><a name="l00665"></a><span class="lineno">  665</span>&#160;</div><div class="line"><a name="l00666"></a><span class="lineno">  666</span>&#160;        <span class="keywordflow">for</span> empty_i, empty_v, empty_nnz <span class="keywordflow">in</span> product([<span class="keyword">True</span>, <span class="keyword">False</span>], repeat=3):</div><div class="line"><a name="l00667"></a><span class="lineno">  667</span>&#160;            sparse_size = [] <span class="keywordflow">if</span> empty_i <span class="keywordflow">else</span> [2, 1]</div><div class="line"><a name="l00668"></a><span class="lineno">  668</span>&#160;            dense_size = [1, 0, 2] <span class="keywordflow">if</span> empty_v <span class="keywordflow">else</span> [1, 2]</div><div class="line"><a name="l00669"></a><span class="lineno">  669</span>&#160;            nnz = 0 <span class="keywordflow">if</span> empty_nnz <span class="keywordflow">else</span> 5</div><div class="line"><a name="l00670"></a><span class="lineno">  670</span>&#160;            <span class="keywordflow">for</span> device <span class="keywordflow">in</span> devices:</div><div class="line"><a name="l00671"></a><span class="lineno">  671</span>&#160;                <a class="code" href="namespacetest.html">test</a>(sparse_size + dense_size, len(sparse_size), nnz, device)</div><div class="line"><a name="l00672"></a><span class="lineno">  672</span>&#160;</div><div class="line"><a name="l00673"></a><span class="lineno">  673</span>&#160;    <span class="keyword">def </span>test_multi_backward(self):</div><div class="line"><a name="l00674"></a><span class="lineno">  674</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00675"></a><span class="lineno">  675</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00676"></a><span class="lineno">  676</span>&#160;</div><div class="line"><a name="l00677"></a><span class="lineno">  677</span>&#160;        q = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00678"></a><span class="lineno">  678</span>&#160;</div><div class="line"><a name="l00679"></a><span class="lineno">  679</span>&#160;        a = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00680"></a><span class="lineno">  680</span>&#160;        b = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00681"></a><span class="lineno">  681</span>&#160;</div><div class="line"><a name="l00682"></a><span class="lineno">  682</span>&#160;        q2 = q * 2</div><div class="line"><a name="l00683"></a><span class="lineno">  683</span>&#160;        z = x + y + q2</div><div class="line"><a name="l00684"></a><span class="lineno">  684</span>&#160;        c = a * b + q2</div><div class="line"><a name="l00685"></a><span class="lineno">  685</span>&#160;        grad_z = torch.randn(5, 5)</div><div class="line"><a name="l00686"></a><span class="lineno">  686</span>&#160;        grad_c = torch.randn(5, 5)</div><div class="line"><a name="l00687"></a><span class="lineno">  687</span>&#160;        <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([z, c], [grad_z, grad_c])</div><div class="line"><a name="l00688"></a><span class="lineno">  688</span>&#160;</div><div class="line"><a name="l00689"></a><span class="lineno">  689</span>&#160;        self.assertEqual(x.grad.data, grad_z)</div><div class="line"><a name="l00690"></a><span class="lineno">  690</span>&#160;        self.assertEqual(y.grad.data, grad_z)</div><div class="line"><a name="l00691"></a><span class="lineno">  691</span>&#160;        self.assertEqual(a.grad.data, grad_c * b.data)</div><div class="line"><a name="l00692"></a><span class="lineno">  692</span>&#160;        self.assertEqual(b.grad.data, grad_c * a.data)</div><div class="line"><a name="l00693"></a><span class="lineno">  693</span>&#160;        self.assertEqual(q.grad.data, (grad_c + grad_z) * 2)</div><div class="line"><a name="l00694"></a><span class="lineno">  694</span>&#160;</div><div class="line"><a name="l00695"></a><span class="lineno">  695</span>&#160;    <span class="keyword">def </span>test_multi_backward_no_grad(self):</div><div class="line"><a name="l00696"></a><span class="lineno">  696</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00697"></a><span class="lineno">  697</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">False</span>)</div><div class="line"><a name="l00698"></a><span class="lineno">  698</span>&#160;</div><div class="line"><a name="l00699"></a><span class="lineno">  699</span>&#160;        z = x + y</div><div class="line"><a name="l00700"></a><span class="lineno">  700</span>&#160;        q = y * 2</div><div class="line"><a name="l00701"></a><span class="lineno">  701</span>&#160;</div><div class="line"><a name="l00702"></a><span class="lineno">  702</span>&#160;        <span class="comment"># NB: we currently raise an exception if any arguments to backwards</span></div><div class="line"><a name="l00703"></a><span class="lineno">  703</span>&#160;        <span class="comment"># have requires_grad=False and don&#39;t have a grad_fn. We may want to</span></div><div class="line"><a name="l00704"></a><span class="lineno">  704</span>&#160;        <span class="comment"># relax that check to a warning.</span></div><div class="line"><a name="l00705"></a><span class="lineno">  705</span>&#160;        <span class="keyword">def </span>call_backwards():</div><div class="line"><a name="l00706"></a><span class="lineno">  706</span>&#160;            <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([z, q], [torch.ones(5, 5), torch.ones(5, 5)])</div><div class="line"><a name="l00707"></a><span class="lineno">  707</span>&#160;        self.assertRaises(RuntimeError, call_backwards)</div><div class="line"><a name="l00708"></a><span class="lineno">  708</span>&#160;</div><div class="line"><a name="l00709"></a><span class="lineno">  709</span>&#160;    <span class="keyword">def </span>test_dependent_backward(self):</div><div class="line"><a name="l00710"></a><span class="lineno">  710</span>&#160;        x = torch.randn(10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00711"></a><span class="lineno">  711</span>&#160;        y = x ** 2</div><div class="line"><a name="l00712"></a><span class="lineno">  712</span>&#160;        z = y ** 3</div><div class="line"><a name="l00713"></a><span class="lineno">  713</span>&#160;</div><div class="line"><a name="l00714"></a><span class="lineno">  714</span>&#160;        go_y = torch.randn(10)</div><div class="line"><a name="l00715"></a><span class="lineno">  715</span>&#160;        go_z = torch.randn(10)</div><div class="line"><a name="l00716"></a><span class="lineno">  716</span>&#160;        <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([y, z], [go_y, go_z])</div><div class="line"><a name="l00717"></a><span class="lineno">  717</span>&#160;</div><div class="line"><a name="l00718"></a><span class="lineno">  718</span>&#160;        xd = x.data</div><div class="line"><a name="l00719"></a><span class="lineno">  719</span>&#160;        self.assertEqual(x.grad.data, 2 * xd * go_y + 6 * xd.pow(5) * go_z)</div><div class="line"><a name="l00720"></a><span class="lineno">  720</span>&#160;</div><div class="line"><a name="l00721"></a><span class="lineno">  721</span>&#160;    <span class="keyword">def </span>test_save_output_nr(self):</div><div class="line"><a name="l00722"></a><span class="lineno">  722</span>&#160;        x = torch.randn(10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00723"></a><span class="lineno">  723</span>&#160;</div><div class="line"><a name="l00724"></a><span class="lineno">  724</span>&#160;        <span class="keyword">class </span>MultiOutputFn(Function):</div><div class="line"><a name="l00725"></a><span class="lineno">  725</span>&#160;            @staticmethod</div><div class="line"><a name="l00726"></a><span class="lineno">  726</span>&#160;            <span class="keyword">def </span>forward(ctx, x):</div><div class="line"><a name="l00727"></a><span class="lineno">  727</span>&#160;                <span class="keywordflow">return</span> x[:5], x[5:]</div><div class="line"><a name="l00728"></a><span class="lineno">  728</span>&#160;</div><div class="line"><a name="l00729"></a><span class="lineno">  729</span>&#160;            @staticmethod</div><div class="line"><a name="l00730"></a><span class="lineno">  730</span>&#160;            <span class="keyword">def </span>backward(ctx, *grad):</div><div class="line"><a name="l00731"></a><span class="lineno">  731</span>&#160;                <span class="keywordflow">return</span> torch.cat(grad)</div><div class="line"><a name="l00732"></a><span class="lineno">  732</span>&#160;</div><div class="line"><a name="l00733"></a><span class="lineno">  733</span>&#160;        a, b = MultiOutputFn.apply(x)</div><div class="line"><a name="l00734"></a><span class="lineno">  734</span>&#160;        self.assertEqual(b.output_nr, 1)</div><div class="line"><a name="l00735"></a><span class="lineno">  735</span>&#160;</div><div class="line"><a name="l00736"></a><span class="lineno">  736</span>&#160;        <span class="keyword">class </span>TestFn(Function):</div><div class="line"><a name="l00737"></a><span class="lineno">  737</span>&#160;            @staticmethod</div><div class="line"><a name="l00738"></a><span class="lineno">  738</span>&#160;            <span class="keyword">def </span>forward(ctx, b):</div><div class="line"><a name="l00739"></a><span class="lineno">  739</span>&#160;                ctx.save_for_backward(b)</div><div class="line"><a name="l00740"></a><span class="lineno">  740</span>&#160;                <span class="keywordflow">return</span> b * 2</div><div class="line"><a name="l00741"></a><span class="lineno">  741</span>&#160;</div><div class="line"><a name="l00742"></a><span class="lineno">  742</span>&#160;            @staticmethod</div><div class="line"><a name="l00743"></a><span class="lineno">  743</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_b):</div><div class="line"><a name="l00744"></a><span class="lineno">  744</span>&#160;                b, = ctx.saved_tensors</div><div class="line"><a name="l00745"></a><span class="lineno">  745</span>&#160;                self.assertEqual(b.output_nr, 1)</div><div class="line"><a name="l00746"></a><span class="lineno">  746</span>&#160;</div><div class="line"><a name="l00747"></a><span class="lineno">  747</span>&#160;        TestFn.apply(b).sum().backward()</div><div class="line"><a name="l00748"></a><span class="lineno">  748</span>&#160;</div><div class="line"><a name="l00749"></a><span class="lineno">  749</span>&#160;    <span class="keyword">def </span>test_free_deep_graph(self):</div><div class="line"><a name="l00750"></a><span class="lineno">  750</span>&#160;        <span class="keyword">def </span><a class="code" href="namespacescope.html">scope</a>():</div><div class="line"><a name="l00751"></a><span class="lineno">  751</span>&#160;            depth = 150000</div><div class="line"><a name="l00752"></a><span class="lineno">  752</span>&#160;            x = torch.randn(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00753"></a><span class="lineno">  753</span>&#160;            y = x.clone()</div><div class="line"><a name="l00754"></a><span class="lineno">  754</span>&#160;</div><div class="line"><a name="l00755"></a><span class="lineno">  755</span>&#160;            <span class="comment"># build a &quot;chain&quot; computation graph</span></div><div class="line"><a name="l00756"></a><span class="lineno">  756</span>&#160;            <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(depth):</div><div class="line"><a name="l00757"></a><span class="lineno">  757</span>&#160;                y = y + y * 0.000001</div><div class="line"><a name="l00758"></a><span class="lineno">  758</span>&#160;</div><div class="line"><a name="l00759"></a><span class="lineno">  759</span>&#160;            <span class="comment"># graph deletion occurs when the above locals go out of scope.</span></div><div class="line"><a name="l00760"></a><span class="lineno">  760</span>&#160;            <span class="comment"># In this case `del y` will trigger it but it&#39;s easier to leave</span></div><div class="line"><a name="l00761"></a><span class="lineno">  761</span>&#160;            <span class="comment"># it to Python to delete the locals.</span></div><div class="line"><a name="l00762"></a><span class="lineno">  762</span>&#160;</div><div class="line"><a name="l00763"></a><span class="lineno">  763</span>&#160;        <span class="comment"># Should not stack overflow</span></div><div class="line"><a name="l00764"></a><span class="lineno">  764</span>&#160;        <a class="code" href="namespacescope.html">scope</a>()</div><div class="line"><a name="l00765"></a><span class="lineno">  765</span>&#160;</div><div class="line"><a name="l00766"></a><span class="lineno">  766</span>&#160;    <span class="keyword">def </span>test_free_deep_graph_complicated(self):</div><div class="line"><a name="l00767"></a><span class="lineno">  767</span>&#160;        <span class="keyword">def </span><a class="code" href="namespacescope.html">scope</a>():</div><div class="line"><a name="l00768"></a><span class="lineno">  768</span>&#160;            depth = 100000</div><div class="line"><a name="l00769"></a><span class="lineno">  769</span>&#160;            randchoice = torch.randint(2, [depth, 2])</div><div class="line"><a name="l00770"></a><span class="lineno">  770</span>&#160;            x = torch.randn(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00771"></a><span class="lineno">  771</span>&#160;            y = x.clone()</div><div class="line"><a name="l00772"></a><span class="lineno">  772</span>&#160;</div><div class="line"><a name="l00773"></a><span class="lineno">  773</span>&#160;            <span class="comment"># Hold the two previous values</span></div><div class="line"><a name="l00774"></a><span class="lineno">  774</span>&#160;            prev_values = [<span class="keywordtype">None</span>, <span class="keywordtype">None</span>]</div><div class="line"><a name="l00775"></a><span class="lineno">  775</span>&#160;</div><div class="line"><a name="l00776"></a><span class="lineno">  776</span>&#160;            <span class="comment"># Build a &quot;chain with skip connections&quot; graph</span></div><div class="line"><a name="l00777"></a><span class="lineno">  777</span>&#160;            <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(depth):</div><div class="line"><a name="l00778"></a><span class="lineno">  778</span>&#160;                prev_tensors = [tensor <span class="keywordflow">for</span> tensor <span class="keywordflow">in</span> prev_values[:-1]</div><div class="line"><a name="l00779"></a><span class="lineno">  779</span>&#160;                                <span class="keywordflow">if</span> tensor <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>]</div><div class="line"><a name="l00780"></a><span class="lineno">  780</span>&#160;                prev_values.append(y)</div><div class="line"><a name="l00781"></a><span class="lineno">  781</span>&#160;                prev_values.pop(0)</div><div class="line"><a name="l00782"></a><span class="lineno">  782</span>&#160;</div><div class="line"><a name="l00783"></a><span class="lineno">  783</span>&#160;                <span class="comment"># Definitely pick one tensor to add</span></div><div class="line"><a name="l00784"></a><span class="lineno">  784</span>&#160;                y += y * 0.000001</div><div class="line"><a name="l00785"></a><span class="lineno">  785</span>&#160;</div><div class="line"><a name="l00786"></a><span class="lineno">  786</span>&#160;                <span class="comment"># Possibly add other tensors</span></div><div class="line"><a name="l00787"></a><span class="lineno">  787</span>&#160;                nprev = len(prev_tensors)</div><div class="line"><a name="l00788"></a><span class="lineno">  788</span>&#160;                <span class="keywordflow">if</span> nprev == 2:</div><div class="line"><a name="l00789"></a><span class="lineno">  789</span>&#160;                    y += randchoice[depth].mul(torch.cat(prev_tensors)).sum()</div><div class="line"><a name="l00790"></a><span class="lineno">  790</span>&#160;</div><div class="line"><a name="l00791"></a><span class="lineno">  791</span>&#160;            <span class="comment"># graph deletion occurs when the above locals go out of scope.</span></div><div class="line"><a name="l00792"></a><span class="lineno">  792</span>&#160;</div><div class="line"><a name="l00793"></a><span class="lineno">  793</span>&#160;        <span class="comment"># Should not stack overflow</span></div><div class="line"><a name="l00794"></a><span class="lineno">  794</span>&#160;        <a class="code" href="namespacescope.html">scope</a>()</div><div class="line"><a name="l00795"></a><span class="lineno">  795</span>&#160;</div><div class="line"><a name="l00796"></a><span class="lineno">  796</span>&#160;    <span class="keyword">def </span>test_free_deep_graph_pyfunction(self):</div><div class="line"><a name="l00797"></a><span class="lineno">  797</span>&#160;        <span class="keyword">class </span>MyOp(Function):</div><div class="line"><a name="l00798"></a><span class="lineno">  798</span>&#160;            @staticmethod</div><div class="line"><a name="l00799"></a><span class="lineno">  799</span>&#160;            <span class="keyword">def </span>forward(ctx, tensor1, tensor2):</div><div class="line"><a name="l00800"></a><span class="lineno">  800</span>&#160;                <span class="keywordflow">return</span> tensor1 + tensor2</div><div class="line"><a name="l00801"></a><span class="lineno">  801</span>&#160;</div><div class="line"><a name="l00802"></a><span class="lineno">  802</span>&#160;            @staticmethod</div><div class="line"><a name="l00803"></a><span class="lineno">  803</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l00804"></a><span class="lineno">  804</span>&#160;                <span class="keywordflow">return</span> grad_output, grad_output</div><div class="line"><a name="l00805"></a><span class="lineno">  805</span>&#160;</div><div class="line"><a name="l00806"></a><span class="lineno">  806</span>&#160;        <span class="keyword">def </span><a class="code" href="namespacescope.html">scope</a>():</div><div class="line"><a name="l00807"></a><span class="lineno">  807</span>&#160;            depth = 150000</div><div class="line"><a name="l00808"></a><span class="lineno">  808</span>&#160;            x = torch.randn(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00809"></a><span class="lineno">  809</span>&#160;            y = x.clone()</div><div class="line"><a name="l00810"></a><span class="lineno">  810</span>&#160;</div><div class="line"><a name="l00811"></a><span class="lineno">  811</span>&#160;            <span class="comment"># build deeply nested computation graph</span></div><div class="line"><a name="l00812"></a><span class="lineno">  812</span>&#160;            <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(depth):</div><div class="line"><a name="l00813"></a><span class="lineno">  813</span>&#160;                y = MyOp.apply(y, y)</div><div class="line"><a name="l00814"></a><span class="lineno">  814</span>&#160;</div><div class="line"><a name="l00815"></a><span class="lineno">  815</span>&#160;            <span class="comment"># graph deletion occurs when the above locals go out of scope.</span></div><div class="line"><a name="l00816"></a><span class="lineno">  816</span>&#160;</div><div class="line"><a name="l00817"></a><span class="lineno">  817</span>&#160;        <span class="comment"># Should not stack overflow</span></div><div class="line"><a name="l00818"></a><span class="lineno">  818</span>&#160;        <a class="code" href="namespacescope.html">scope</a>()</div><div class="line"><a name="l00819"></a><span class="lineno">  819</span>&#160;</div><div class="line"><a name="l00820"></a><span class="lineno">  820</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> TEST_CUDA, <span class="stringliteral">&quot;need CUDA memory stats&quot;</span>)</div><div class="line"><a name="l00821"></a><span class="lineno">  821</span>&#160;    <span class="keyword">def </span>test_free_unneeded_tensor(self):</div><div class="line"><a name="l00822"></a><span class="lineno">  822</span>&#160;        x = torch.randn(2, 3, 10, 10, device=<span class="stringliteral">&#39;cuda&#39;</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00823"></a><span class="lineno">  823</span>&#160;        m = torch.randn(1, 3, 1, 1, device=<span class="stringliteral">&#39;cuda&#39;</span>)</div><div class="line"><a name="l00824"></a><span class="lineno">  824</span>&#160;</div><div class="line"><a name="l00825"></a><span class="lineno">  825</span>&#160;        z = x.sum()</div><div class="line"><a name="l00826"></a><span class="lineno">  826</span>&#160;        base_mem = <a class="code" href="torch_2cuda_2____init_____8py.html#a55f030e9a32b289b723487de9af1f5d9">torch.cuda.memory_allocated</a>()</div><div class="line"><a name="l00827"></a><span class="lineno">  827</span>&#160;        z = ((x + 2) * m).sum()</div><div class="line"><a name="l00828"></a><span class="lineno">  828</span>&#160;        end_mem = <a class="code" href="torch_2cuda_2____init_____8py.html#a55f030e9a32b289b723487de9af1f5d9">torch.cuda.memory_allocated</a>()</div><div class="line"><a name="l00829"></a><span class="lineno">  829</span>&#160;</div><div class="line"><a name="l00830"></a><span class="lineno">  830</span>&#160;        <span class="comment"># In the end the memory usage should remain equal, because neither of</span></div><div class="line"><a name="l00831"></a><span class="lineno">  831</span>&#160;        <span class="comment"># (x + 2) and ((x + 2) * m) should be kept alive for backward, while the</span></div><div class="line"><a name="l00832"></a><span class="lineno">  832</span>&#160;        <span class="comment"># previous allocation of z had the same size as the current one.</span></div><div class="line"><a name="l00833"></a><span class="lineno">  833</span>&#160;        self.assertEqual(base_mem, end_mem)</div><div class="line"><a name="l00834"></a><span class="lineno">  834</span>&#160;</div><div class="line"><a name="l00835"></a><span class="lineno">  835</span>&#160;    <span class="keyword">def </span>test_no_unnecessary_save(self):</div><div class="line"><a name="l00836"></a><span class="lineno">  836</span>&#160;        <span class="comment"># If we kept x in the derivative Function of x * 2 we would</span></div><div class="line"><a name="l00837"></a><span class="lineno">  837</span>&#160;        <span class="comment"># get an error in the backward that would complain that we&#39;ve</span></div><div class="line"><a name="l00838"></a><span class="lineno">  838</span>&#160;        <span class="comment"># modified x, which was needed for gradient computation.</span></div><div class="line"><a name="l00839"></a><span class="lineno">  839</span>&#160;        <span class="comment"># Since we should elide unnecessary saves, this test should pass.</span></div><div class="line"><a name="l00840"></a><span class="lineno">  840</span>&#160;        mu = torch.ones(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00841"></a><span class="lineno">  841</span>&#160;        x = torch.empty(1)</div><div class="line"><a name="l00842"></a><span class="lineno">  842</span>&#160;        loss = 0</div><div class="line"><a name="l00843"></a><span class="lineno">  843</span>&#160;        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(3):</div><div class="line"><a name="l00844"></a><span class="lineno">  844</span>&#160;            x.detach_()</div><div class="line"><a name="l00845"></a><span class="lineno">  845</span>&#160;            x.copy_(mu + i)</div><div class="line"><a name="l00846"></a><span class="lineno">  846</span>&#160;            loss += (x * <a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>([float(i)])).sum()</div><div class="line"><a name="l00847"></a><span class="lineno">  847</span>&#160;        loss.backward()</div><div class="line"><a name="l00848"></a><span class="lineno">  848</span>&#160;</div><div class="line"><a name="l00849"></a><span class="lineno">  849</span>&#160;    <span class="keyword">def </span>test_no_grad(self):</div><div class="line"><a name="l00850"></a><span class="lineno">  850</span>&#160;        x = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00851"></a><span class="lineno">  851</span>&#160;        y = Variable(torch.ones(5, 5) * 4)</div><div class="line"><a name="l00852"></a><span class="lineno">  852</span>&#160;        with torch.no_grad():</div><div class="line"><a name="l00853"></a><span class="lineno">  853</span>&#160;            w = x + y</div><div class="line"><a name="l00854"></a><span class="lineno">  854</span>&#160;</div><div class="line"><a name="l00855"></a><span class="lineno">  855</span>&#160;        @torch.no_grad()</div><div class="line"><a name="l00856"></a><span class="lineno">  856</span>&#160;        <span class="keyword">def </span>adder(x, y):</div><div class="line"><a name="l00857"></a><span class="lineno">  857</span>&#160;            <span class="keywordflow">return</span> x + y</div><div class="line"><a name="l00858"></a><span class="lineno">  858</span>&#160;</div><div class="line"><a name="l00859"></a><span class="lineno">  859</span>&#160;        z = adder(x, y)</div><div class="line"><a name="l00860"></a><span class="lineno">  860</span>&#160;</div><div class="line"><a name="l00861"></a><span class="lineno">  861</span>&#160;        self.assertFalse(w.requires_grad)</div><div class="line"><a name="l00862"></a><span class="lineno">  862</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: w.backward(torch.ones(5, 5)))</div><div class="line"><a name="l00863"></a><span class="lineno">  863</span>&#160;        self.assertIsNone(w.grad_fn)</div><div class="line"><a name="l00864"></a><span class="lineno">  864</span>&#160;        self.assertFalse(z.requires_grad)</div><div class="line"><a name="l00865"></a><span class="lineno">  865</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: z.backward(torch.ones(5, 5)))</div><div class="line"><a name="l00866"></a><span class="lineno">  866</span>&#160;        self.assertIsNone(z.grad_fn)</div><div class="line"><a name="l00867"></a><span class="lineno">  867</span>&#160;</div><div class="line"><a name="l00868"></a><span class="lineno">  868</span>&#160;        <span class="comment"># test nested decorator and with-statement on no_grad</span></div><div class="line"><a name="l00869"></a><span class="lineno">  869</span>&#160;        with torch.no_grad():</div><div class="line"><a name="l00870"></a><span class="lineno">  870</span>&#160;            self.assertFalse(torch.is_grad_enabled())</div><div class="line"><a name="l00871"></a><span class="lineno">  871</span>&#160;            w = adder(x, y)</div><div class="line"><a name="l00872"></a><span class="lineno">  872</span>&#160;            self.assertFalse(torch.is_grad_enabled())</div><div class="line"><a name="l00873"></a><span class="lineno">  873</span>&#160;</div><div class="line"><a name="l00874"></a><span class="lineno"><a class="line" href="classtest__autograd_1_1_test_autograd.html#aeec79a3d621dc4e50030dc79dbff7c3d">  874</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classtest__autograd_1_1_test_autograd.html#aeec79a3d621dc4e50030dc79dbff7c3d">test_no_grad_python_function</a>(self):</div><div class="line"><a name="l00875"></a><span class="lineno">  875</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Python Functions should respect grad mode.&quot;&quot;&quot;</span></div><div class="line"><a name="l00876"></a><span class="lineno">  876</span>&#160;        x = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00877"></a><span class="lineno">  877</span>&#160;</div><div class="line"><a name="l00878"></a><span class="lineno">  878</span>&#160;        <span class="keyword">class </span>MyOp(Function):</div><div class="line"><a name="l00879"></a><span class="lineno">  879</span>&#160;            @staticmethod</div><div class="line"><a name="l00880"></a><span class="lineno">  880</span>&#160;            <span class="keyword">def </span>forward(self, x):</div><div class="line"><a name="l00881"></a><span class="lineno">  881</span>&#160;                <span class="keywordflow">return</span> x + 1</div><div class="line"><a name="l00882"></a><span class="lineno">  882</span>&#160;</div><div class="line"><a name="l00883"></a><span class="lineno">  883</span>&#160;            @staticmethod</div><div class="line"><a name="l00884"></a><span class="lineno">  884</span>&#160;            <span class="keyword">def </span>backward(self, dy):</div><div class="line"><a name="l00885"></a><span class="lineno">  885</span>&#160;                <span class="keywordflow">return</span> dy</div><div class="line"><a name="l00886"></a><span class="lineno">  886</span>&#160;</div><div class="line"><a name="l00887"></a><span class="lineno">  887</span>&#160;        with torch.no_grad():</div><div class="line"><a name="l00888"></a><span class="lineno">  888</span>&#160;            y = MyOp.apply(x)</div><div class="line"><a name="l00889"></a><span class="lineno">  889</span>&#160;        self.assertFalse(y.requires_grad)</div><div class="line"><a name="l00890"></a><span class="lineno">  890</span>&#160;</div><div class="line"><a name="l00891"></a><span class="lineno">  891</span>&#160;    <span class="keyword">def </span><a class="code" href="namespacetest__indexing.html">test_indexing</a>(self):</div><div class="line"><a name="l00892"></a><span class="lineno">  892</span>&#160;        x = torch.arange(1., 17).view(4, 4)</div><div class="line"><a name="l00893"></a><span class="lineno">  893</span>&#160;        y = Variable(x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00894"></a><span class="lineno">  894</span>&#160;</div><div class="line"><a name="l00895"></a><span class="lineno">  895</span>&#160;        <span class="keyword">def </span>compare(x, y, idx, indexed_tensor, indexed_var):</div><div class="line"><a name="l00896"></a><span class="lineno">  896</span>&#160;            indexed_var_t = indexed_var.data</div><div class="line"><a name="l00897"></a><span class="lineno">  897</span>&#160;            <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(indexed_tensor, torch.Tensor):</div><div class="line"><a name="l00898"></a><span class="lineno">  898</span>&#160;                indexed_var_t = indexed_var_t[0]</div><div class="line"><a name="l00899"></a><span class="lineno">  899</span>&#160;            self.assertEqual(indexed_tensor, indexed_var_t)</div><div class="line"><a name="l00900"></a><span class="lineno">  900</span>&#160;</div><div class="line"><a name="l00901"></a><span class="lineno">  901</span>&#160;            indexed_var.sum().backward()</div><div class="line"><a name="l00902"></a><span class="lineno">  902</span>&#160;            expected_grad = torch.Tensor(x.size()).fill_(0)</div><div class="line"><a name="l00903"></a><span class="lineno">  903</span>&#160;            expected_grad[idx] = 1</div><div class="line"><a name="l00904"></a><span class="lineno">  904</span>&#160;            self.assertEqual(y.grad.data, expected_grad)</div><div class="line"><a name="l00905"></a><span class="lineno">  905</span>&#160;</div><div class="line"><a name="l00906"></a><span class="lineno">  906</span>&#160;        <span class="keyword">def </span>check_index(x, y, idx):</div><div class="line"><a name="l00907"></a><span class="lineno">  907</span>&#160;            <span class="keywordflow">if</span> y.grad <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00908"></a><span class="lineno">  908</span>&#160;                y.grad.data.zero_()</div><div class="line"><a name="l00909"></a><span class="lineno">  909</span>&#160;            indexed_tensor = x[idx]</div><div class="line"><a name="l00910"></a><span class="lineno">  910</span>&#160;            indexed_var = y[idx]</div><div class="line"><a name="l00911"></a><span class="lineno">  911</span>&#160;            compare(x, y, idx, indexed_tensor, indexed_var)</div><div class="line"><a name="l00912"></a><span class="lineno">  912</span>&#160;</div><div class="line"><a name="l00913"></a><span class="lineno">  913</span>&#160;        check_index(x, y, 1)</div><div class="line"><a name="l00914"></a><span class="lineno">  914</span>&#160;        check_index(x, y, (1, 1))</div><div class="line"><a name="l00915"></a><span class="lineno">  915</span>&#160;        check_index(x, y, slice(1, <span class="keywordtype">None</span>))</div><div class="line"><a name="l00916"></a><span class="lineno">  916</span>&#160;        check_index(x, y, slice(<span class="keywordtype">None</span>, 2))</div><div class="line"><a name="l00917"></a><span class="lineno">  917</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>, 2), 2))</div><div class="line"><a name="l00918"></a><span class="lineno">  918</span>&#160;        check_index(x, y, (slice(1, 2), 2))</div><div class="line"><a name="l00919"></a><span class="lineno">  919</span>&#160;        check_index(x, y, (1, slice(2, <span class="keywordtype">None</span>)))</div><div class="line"><a name="l00920"></a><span class="lineno">  920</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>, <span class="keywordtype">None</span>), slice(2, <span class="keywordtype">None</span>)))</div><div class="line"><a name="l00921"></a><span class="lineno">  921</span>&#160;        check_index(x, y, torch.LongTensor([0, 2]))</div><div class="line"><a name="l00922"></a><span class="lineno">  922</span>&#160;        check_index(x, y, torch.rand(4, 4).bernoulli().byte())</div><div class="line"><a name="l00923"></a><span class="lineno">  923</span>&#160;        check_index(x, y, (Ellipsis, slice(2, <span class="keywordtype">None</span>)))</div><div class="line"><a name="l00924"></a><span class="lineno">  924</span>&#160;        check_index(x, y, ([0], [0]))</div><div class="line"><a name="l00925"></a><span class="lineno">  925</span>&#160;        check_index(x, y, ([1, 2, 3], [0]))</div><div class="line"><a name="l00926"></a><span class="lineno">  926</span>&#160;        check_index(x, y, ([1, 2], [2, 1]))</div><div class="line"><a name="l00927"></a><span class="lineno">  927</span>&#160;        check_index(x, y, ([[1, 2], [3, 0]], [[0, 1], [2, 3]]))</div><div class="line"><a name="l00928"></a><span class="lineno">  928</span>&#160;        check_index(x, y, ([slice(<span class="keywordtype">None</span>), [2, 3]]))</div><div class="line"><a name="l00929"></a><span class="lineno">  929</span>&#160;        check_index(x, y, ([[2, 3], slice(<span class="keywordtype">None</span>)]))</div><div class="line"><a name="l00930"></a><span class="lineno">  930</span>&#160;</div><div class="line"><a name="l00931"></a><span class="lineno">  931</span>&#160;        <span class="comment"># advanced indexing, with less dim, or ellipsis</span></div><div class="line"><a name="l00932"></a><span class="lineno">  932</span>&#160;        check_index(x, y, ([0]))</div><div class="line"><a name="l00933"></a><span class="lineno">  933</span>&#160;        check_index(x, y, ([0], ))</div><div class="line"><a name="l00934"></a><span class="lineno">  934</span>&#160;</div><div class="line"><a name="l00935"></a><span class="lineno">  935</span>&#160;        x = torch.arange(1., 49).view(4, 3, 4)</div><div class="line"><a name="l00936"></a><span class="lineno">  936</span>&#160;        y = Variable(x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00937"></a><span class="lineno">  937</span>&#160;</div><div class="line"><a name="l00938"></a><span class="lineno">  938</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>), [0], [0]))</div><div class="line"><a name="l00939"></a><span class="lineno">  939</span>&#160;        check_index(x, y, ([0], [0], slice(<span class="keywordtype">None</span>)))</div><div class="line"><a name="l00940"></a><span class="lineno">  940</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>), [0, 1, 2], [0]))</div><div class="line"><a name="l00941"></a><span class="lineno">  941</span>&#160;        check_index(x, y, ([0, 1, 2], [0], slice(<span class="keywordtype">None</span>)))</div><div class="line"><a name="l00942"></a><span class="lineno">  942</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>), [1, 2], [2, 1]))</div><div class="line"><a name="l00943"></a><span class="lineno">  943</span>&#160;        check_index(x, y, ([1, 2], [2, 1], slice(<span class="keywordtype">None</span>)))</div><div class="line"><a name="l00944"></a><span class="lineno">  944</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>), [[1, 2], [2, 0]], [[0, 1], [2, 3]]))</div><div class="line"><a name="l00945"></a><span class="lineno">  945</span>&#160;        check_index(x, y, ([[1, 2], [3, 0]], [[0, 1], [2, 2]], slice(<span class="keywordtype">None</span>)))</div><div class="line"><a name="l00946"></a><span class="lineno">  946</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>), slice(<span class="keywordtype">None</span>), [2, 1]))</div><div class="line"><a name="l00947"></a><span class="lineno">  947</span>&#160;        check_index(x, y, (slice(<span class="keywordtype">None</span>), [2, 1], slice(<span class="keywordtype">None</span>)))</div><div class="line"><a name="l00948"></a><span class="lineno">  948</span>&#160;        check_index(x, y, ([2, 1], slice(<span class="keywordtype">None</span>), slice(<span class="keywordtype">None</span>)))</div><div class="line"><a name="l00949"></a><span class="lineno">  949</span>&#160;</div><div class="line"><a name="l00950"></a><span class="lineno">  950</span>&#160;        <span class="comment"># advanced indexing, with less dim, or ellipsis</span></div><div class="line"><a name="l00951"></a><span class="lineno">  951</span>&#160;        check_index(x, y, ([0], ))</div><div class="line"><a name="l00952"></a><span class="lineno">  952</span>&#160;        check_index(x, y, ([0], slice(<span class="keywordtype">None</span>)))</div><div class="line"><a name="l00953"></a><span class="lineno">  953</span>&#160;        check_index(x, y, ([0], Ellipsis))</div><div class="line"><a name="l00954"></a><span class="lineno">  954</span>&#160;        check_index(x, y, ([1, 2], [0, 1]))</div><div class="line"><a name="l00955"></a><span class="lineno">  955</span>&#160;        check_index(x, y, ([1, 2], [0, 1], Ellipsis))</div><div class="line"><a name="l00956"></a><span class="lineno">  956</span>&#160;        check_index(x, y, (Ellipsis, [1, 2], [0, 1]))</div><div class="line"><a name="l00957"></a><span class="lineno">  957</span>&#160;</div><div class="line"><a name="l00958"></a><span class="lineno">  958</span>&#160;        <span class="comment"># advanced indexing, with a tensor wrapped in a variable</span></div><div class="line"><a name="l00959"></a><span class="lineno">  959</span>&#160;        z = torch.LongTensor([0, 1])</div><div class="line"><a name="l00960"></a><span class="lineno">  960</span>&#160;        zv = Variable(z, requires_grad=<span class="keyword">False</span>)</div><div class="line"><a name="l00961"></a><span class="lineno">  961</span>&#160;        seq = [z, Ellipsis]</div><div class="line"><a name="l00962"></a><span class="lineno">  962</span>&#160;        seqv = [zv, Ellipsis]</div><div class="line"><a name="l00963"></a><span class="lineno">  963</span>&#160;</div><div class="line"><a name="l00964"></a><span class="lineno">  964</span>&#160;        <span class="keywordflow">if</span> y.grad <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00965"></a><span class="lineno">  965</span>&#160;            y.grad.data.zero_()</div><div class="line"><a name="l00966"></a><span class="lineno">  966</span>&#160;        indexed_tensor = x[seq]</div><div class="line"><a name="l00967"></a><span class="lineno">  967</span>&#160;        indexed_var = y[seqv]</div><div class="line"><a name="l00968"></a><span class="lineno">  968</span>&#160;        compare(x, y, seq, indexed_tensor, indexed_var)</div><div class="line"><a name="l00969"></a><span class="lineno">  969</span>&#160;</div><div class="line"><a name="l00970"></a><span class="lineno">  970</span>&#160;    <span class="keyword">def </span>test_indexing_duplicates(self):</div><div class="line"><a name="l00971"></a><span class="lineno">  971</span>&#160;        x = torch.arange(1., 17).view(4, 4)</div><div class="line"><a name="l00972"></a><span class="lineno">  972</span>&#160;        y = Variable(x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00973"></a><span class="lineno">  973</span>&#160;</div><div class="line"><a name="l00974"></a><span class="lineno">  974</span>&#160;        idx = torch.LongTensor([1, 1, 3, 2, 1, 2])</div><div class="line"><a name="l00975"></a><span class="lineno">  975</span>&#160;        y[idx].sum().backward()</div><div class="line"><a name="l00976"></a><span class="lineno">  976</span>&#160;        expected_grad = torch.zeros(4, 4)</div><div class="line"><a name="l00977"></a><span class="lineno">  977</span>&#160;        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> idx:</div><div class="line"><a name="l00978"></a><span class="lineno">  978</span>&#160;            expected_grad[i] += 1</div><div class="line"><a name="l00979"></a><span class="lineno">  979</span>&#160;        self.assertEqual(y.grad.data, expected_grad)</div><div class="line"><a name="l00980"></a><span class="lineno">  980</span>&#160;</div><div class="line"><a name="l00981"></a><span class="lineno">  981</span>&#160;        <span class="comment"># with advanced indexing</span></div><div class="line"><a name="l00982"></a><span class="lineno">  982</span>&#160;        x = torch.arange(1., 17).view(4, 4)</div><div class="line"><a name="l00983"></a><span class="lineno">  983</span>&#160;        y = Variable(x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00984"></a><span class="lineno">  984</span>&#160;</div><div class="line"><a name="l00985"></a><span class="lineno">  985</span>&#160;        idx = [[1, 1, 3, 2, 1, 2], [0]]</div><div class="line"><a name="l00986"></a><span class="lineno">  986</span>&#160;        y[idx].sum().backward()</div><div class="line"><a name="l00987"></a><span class="lineno">  987</span>&#160;        expected_grad = torch.zeros(4, 4)</div><div class="line"><a name="l00988"></a><span class="lineno">  988</span>&#160;        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> idx[0]:</div><div class="line"><a name="l00989"></a><span class="lineno">  989</span>&#160;            <span class="keywordflow">for</span> j <span class="keywordflow">in</span> idx[1]:</div><div class="line"><a name="l00990"></a><span class="lineno">  990</span>&#160;                expected_grad[i][j] += 1</div><div class="line"><a name="l00991"></a><span class="lineno">  991</span>&#160;</div><div class="line"><a name="l00992"></a><span class="lineno">  992</span>&#160;        self.assertEqual(y.grad.data, expected_grad)</div><div class="line"><a name="l00993"></a><span class="lineno">  993</span>&#160;</div><div class="line"><a name="l00994"></a><span class="lineno">  994</span>&#160;        x = torch.arange(1., 17).view(4, 4)</div><div class="line"><a name="l00995"></a><span class="lineno">  995</span>&#160;        y = Variable(x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l00996"></a><span class="lineno">  996</span>&#160;        idx = [[[1, 2], [0, 0]], [[0, 1], [1, 1]]]</div><div class="line"><a name="l00997"></a><span class="lineno">  997</span>&#160;        y[idx].sum().backward()</div><div class="line"><a name="l00998"></a><span class="lineno">  998</span>&#160;        expected_grad = torch.Tensor([[0, 2, 0, 0],</div><div class="line"><a name="l00999"></a><span class="lineno">  999</span>&#160;                                      [1, 0, 0, 0],</div><div class="line"><a name="l01000"></a><span class="lineno"> 1000</span>&#160;                                      [0, 1, 0, 0],</div><div class="line"><a name="l01001"></a><span class="lineno"> 1001</span>&#160;                                      [0, 0, 0, 0]])</div><div class="line"><a name="l01002"></a><span class="lineno"> 1002</span>&#160;        self.assertEqual(y.grad.data, expected_grad)</div><div class="line"><a name="l01003"></a><span class="lineno"> 1003</span>&#160;</div><div class="line"><a name="l01004"></a><span class="lineno"> 1004</span>&#160;        x = torch.arange(1., 65).view(4, 4, 4)</div><div class="line"><a name="l01005"></a><span class="lineno"> 1005</span>&#160;        y = Variable(x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01006"></a><span class="lineno"> 1006</span>&#160;</div><div class="line"><a name="l01007"></a><span class="lineno"> 1007</span>&#160;        idx = [[1, 1, 1], slice(<span class="keywordtype">None</span>), slice(<span class="keywordtype">None</span>)]</div><div class="line"><a name="l01008"></a><span class="lineno"> 1008</span>&#160;        y[idx].sum().backward()</div><div class="line"><a name="l01009"></a><span class="lineno"> 1009</span>&#160;        expected_grad = torch.Tensor(4, 4, 4).zero_()</div><div class="line"><a name="l01010"></a><span class="lineno"> 1010</span>&#160;        expected_grad[1].fill_(3)</div><div class="line"><a name="l01011"></a><span class="lineno"> 1011</span>&#160;        self.assertEqual(y.grad.data, expected_grad)</div><div class="line"><a name="l01012"></a><span class="lineno"> 1012</span>&#160;</div><div class="line"><a name="l01013"></a><span class="lineno"> 1013</span>&#160;    <span class="keyword">def </span>test_volatile_deprecated(self):</div><div class="line"><a name="l01014"></a><span class="lineno"> 1014</span>&#160;        v = torch.autograd.torch.randn(3, 3)</div><div class="line"><a name="l01015"></a><span class="lineno"> 1015</span>&#160;        with warnings.catch_warnings(record=<span class="keyword">True</span>) <span class="keyword">as</span> w:</div><div class="line"><a name="l01016"></a><span class="lineno"> 1016</span>&#160;            self.assertFalse(v.volatile)</div><div class="line"><a name="l01017"></a><span class="lineno"> 1017</span>&#160;        self.assertIn(<span class="stringliteral">&#39;volatile&#39;</span>, str(w[0].message))</div><div class="line"><a name="l01018"></a><span class="lineno"> 1018</span>&#160;</div><div class="line"><a name="l01019"></a><span class="lineno"> 1019</span>&#160;    <span class="keyword">def </span>test_saved_variables_deprecated(self):</div><div class="line"><a name="l01020"></a><span class="lineno"> 1020</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l01021"></a><span class="lineno"> 1021</span>&#160;            @staticmethod</div><div class="line"><a name="l01022"></a><span class="lineno"> 1022</span>&#160;            <span class="keyword">def </span>forward(ctx, tensor1, tensor2):</div><div class="line"><a name="l01023"></a><span class="lineno"> 1023</span>&#160;                ctx.save_for_backward(tensor1, tensor2)</div><div class="line"><a name="l01024"></a><span class="lineno"> 1024</span>&#160;                <span class="keywordflow">return</span> tensor1 + tensor2</div><div class="line"><a name="l01025"></a><span class="lineno"> 1025</span>&#160;</div><div class="line"><a name="l01026"></a><span class="lineno"> 1026</span>&#160;            @staticmethod</div><div class="line"><a name="l01027"></a><span class="lineno"> 1027</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l01028"></a><span class="lineno"> 1028</span>&#160;                var1, var2 = ctx.saved_variables</div><div class="line"><a name="l01029"></a><span class="lineno"> 1029</span>&#160;                <span class="keywordflow">return</span> (grad_output, grad_output)</div><div class="line"><a name="l01030"></a><span class="lineno"> 1030</span>&#160;</div><div class="line"><a name="l01031"></a><span class="lineno"> 1031</span>&#160;        with warnings.catch_warnings(record=<span class="keyword">True</span>) <span class="keyword">as</span> warns:</div><div class="line"><a name="l01032"></a><span class="lineno"> 1032</span>&#160;            warnings.simplefilter(<span class="stringliteral">&quot;always&quot;</span>)</div><div class="line"><a name="l01033"></a><span class="lineno"> 1033</span>&#160;            x = torch.randn((3, 3), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01034"></a><span class="lineno"> 1034</span>&#160;            y = torch.randn((3, 3), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01035"></a><span class="lineno"> 1035</span>&#160;            model = MyFunction()</div><div class="line"><a name="l01036"></a><span class="lineno"> 1036</span>&#160;            model.apply(x, y).sum().backward()</div><div class="line"><a name="l01037"></a><span class="lineno"> 1037</span>&#160;</div><div class="line"><a name="l01038"></a><span class="lineno"> 1038</span>&#160;            has_deprecated = map(<span class="keyword">lambda</span> warn:</div><div class="line"><a name="l01039"></a><span class="lineno"> 1039</span>&#160;                                 <span class="stringliteral">&#39;deprecated&#39;</span> <span class="keywordflow">in</span> str(warn) <span class="keywordflow">and</span></div><div class="line"><a name="l01040"></a><span class="lineno"> 1040</span>&#160;                                 <span class="stringliteral">&#39;saved_variables&#39;</span> <span class="keywordflow">in</span> str(warn),</div><div class="line"><a name="l01041"></a><span class="lineno"> 1041</span>&#160;                                 warns)</div><div class="line"><a name="l01042"></a><span class="lineno"> 1042</span>&#160;            has_deprecated = reduce(<span class="keyword">lambda</span> x, y: x <span class="keywordflow">or</span> y, has_deprecated)</div><div class="line"><a name="l01043"></a><span class="lineno"> 1043</span>&#160;            self.assertTrue(has_deprecated)</div><div class="line"><a name="l01044"></a><span class="lineno"> 1044</span>&#160;</div><div class="line"><a name="l01045"></a><span class="lineno"> 1045</span>&#160;    <span class="keyword">def </span>test_requires_grad(self):</div><div class="line"><a name="l01046"></a><span class="lineno"> 1046</span>&#160;        x = torch.randn(5, 5)</div><div class="line"><a name="l01047"></a><span class="lineno"> 1047</span>&#160;        y = torch.randn(5, 5)</div><div class="line"><a name="l01048"></a><span class="lineno"> 1048</span>&#160;        z = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01049"></a><span class="lineno"> 1049</span>&#160;        a = x + y</div><div class="line"><a name="l01050"></a><span class="lineno"> 1050</span>&#160;        self.assertFalse(a.requires_grad)</div><div class="line"><a name="l01051"></a><span class="lineno"> 1051</span>&#160;        b = a + z</div><div class="line"><a name="l01052"></a><span class="lineno"> 1052</span>&#160;        self.assertTrue(b.requires_grad)</div><div class="line"><a name="l01053"></a><span class="lineno"> 1053</span>&#160;</div><div class="line"><a name="l01054"></a><span class="lineno"> 1054</span>&#160;        <span class="keyword">def </span>error():</div><div class="line"><a name="l01055"></a><span class="lineno"> 1055</span>&#160;            <span class="keywordflow">raise</span> RuntimeError</div><div class="line"><a name="l01056"></a><span class="lineno"> 1056</span>&#160;        <span class="comment"># Make sure backward isn&#39;t called on these</span></div><div class="line"><a name="l01057"></a><span class="lineno"> 1057</span>&#160;        a._backward_hooks = OrderedDict()</div><div class="line"><a name="l01058"></a><span class="lineno"> 1058</span>&#160;        x._backward_hooks = OrderedDict()</div><div class="line"><a name="l01059"></a><span class="lineno"> 1059</span>&#160;        y._backward_hooks = OrderedDict()</div><div class="line"><a name="l01060"></a><span class="lineno"> 1060</span>&#160;        a._backward_hooks[<span class="stringliteral">&#39;test&#39;</span>] = error</div><div class="line"><a name="l01061"></a><span class="lineno"> 1061</span>&#160;        x._backward_hooks[<span class="stringliteral">&#39;test&#39;</span>] = error</div><div class="line"><a name="l01062"></a><span class="lineno"> 1062</span>&#160;        y._backward_hooks[<span class="stringliteral">&#39;test&#39;</span>] = error</div><div class="line"><a name="l01063"></a><span class="lineno"> 1063</span>&#160;        b.backward(torch.ones(5, 5))</div><div class="line"><a name="l01064"></a><span class="lineno"> 1064</span>&#160;</div><div class="line"><a name="l01065"></a><span class="lineno"> 1065</span>&#160;    <span class="keyword">def </span>test_requires_grad_(self):</div><div class="line"><a name="l01066"></a><span class="lineno"> 1066</span>&#160;        x = torch.randn(5, 5)</div><div class="line"><a name="l01067"></a><span class="lineno"> 1067</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01068"></a><span class="lineno"> 1068</span>&#160;        self.assertIs(x, x.requires_grad_())</div><div class="line"><a name="l01069"></a><span class="lineno"> 1069</span>&#160;        self.assertTrue(x.requires_grad)</div><div class="line"><a name="l01070"></a><span class="lineno"> 1070</span>&#160;        self.assertIs(y, y.requires_grad_())</div><div class="line"><a name="l01071"></a><span class="lineno"> 1071</span>&#160;        self.assertTrue(y.requires_grad)</div><div class="line"><a name="l01072"></a><span class="lineno"> 1072</span>&#160;        self.assertIs(x, x.requires_grad_(<span class="keyword">True</span>))</div><div class="line"><a name="l01073"></a><span class="lineno"> 1073</span>&#160;        self.assertTrue(x.requires_grad)</div><div class="line"><a name="l01074"></a><span class="lineno"> 1074</span>&#160;        self.assertIs(y, y.requires_grad_(<span class="keyword">True</span>))</div><div class="line"><a name="l01075"></a><span class="lineno"> 1075</span>&#160;        self.assertTrue(y.requires_grad)</div><div class="line"><a name="l01076"></a><span class="lineno"> 1076</span>&#160;        z = x * y</div><div class="line"><a name="l01077"></a><span class="lineno"> 1077</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: z.requires_grad_(<span class="keyword">False</span>))</div><div class="line"><a name="l01078"></a><span class="lineno"> 1078</span>&#160;        self.assertIs(z, z.requires_grad_())</div><div class="line"><a name="l01079"></a><span class="lineno"> 1079</span>&#160;        self.assertTrue(z.requires_grad)</div><div class="line"><a name="l01080"></a><span class="lineno"> 1080</span>&#160;        self.assertIs(z, z.requires_grad_(<span class="keyword">True</span>))</div><div class="line"><a name="l01081"></a><span class="lineno"> 1081</span>&#160;        self.assertTrue(z.requires_grad)</div><div class="line"><a name="l01082"></a><span class="lineno"> 1082</span>&#160;</div><div class="line"><a name="l01083"></a><span class="lineno"> 1083</span>&#160;        self.assertIs(x, x.requires_grad_(<span class="keyword">False</span>))</div><div class="line"><a name="l01084"></a><span class="lineno"> 1084</span>&#160;        self.assertFalse(x.requires_grad)</div><div class="line"><a name="l01085"></a><span class="lineno"> 1085</span>&#160;        self.assertIs(y, y.requires_grad_(<span class="keyword">False</span>))</div><div class="line"><a name="l01086"></a><span class="lineno"> 1086</span>&#160;        self.assertFalse(y.requires_grad)</div><div class="line"><a name="l01087"></a><span class="lineno"> 1087</span>&#160;</div><div class="line"><a name="l01088"></a><span class="lineno"> 1088</span>&#160;    <span class="keyword">def </span>test_requires_grad_inplace(self):</div><div class="line"><a name="l01089"></a><span class="lineno"> 1089</span>&#160;        a = torch.randn(5, 5)</div><div class="line"><a name="l01090"></a><span class="lineno"> 1090</span>&#160;        b = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01091"></a><span class="lineno"> 1091</span>&#160;        a += b</div><div class="line"><a name="l01092"></a><span class="lineno"> 1092</span>&#160;        self.assertTrue(a.requires_grad)</div><div class="line"><a name="l01093"></a><span class="lineno"> 1093</span>&#160;</div><div class="line"><a name="l01094"></a><span class="lineno"> 1094</span>&#160;        <span class="comment"># non-leaf Variable</span></div><div class="line"><a name="l01095"></a><span class="lineno"> 1095</span>&#160;        a = torch.randn(5, 5) + 0</div><div class="line"><a name="l01096"></a><span class="lineno"> 1096</span>&#160;        b = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01097"></a><span class="lineno"> 1097</span>&#160;        a += b</div><div class="line"><a name="l01098"></a><span class="lineno"> 1098</span>&#160;        self.assertTrue(a.requires_grad)</div><div class="line"><a name="l01099"></a><span class="lineno"> 1099</span>&#160;</div><div class="line"><a name="l01100"></a><span class="lineno"> 1100</span>&#160;    <span class="keyword">def </span>test_no_requires_grad_inplace(self):</div><div class="line"><a name="l01101"></a><span class="lineno"> 1101</span>&#160;        <span class="comment"># basic case, should be able to modify inplace while requires_grad is False</span></div><div class="line"><a name="l01102"></a><span class="lineno"> 1102</span>&#160;        a = torch.randn(2, 3)</div><div class="line"><a name="l01103"></a><span class="lineno"> 1103</span>&#160;        a.add_(5)</div><div class="line"><a name="l01104"></a><span class="lineno"> 1104</span>&#160;        a.requires_grad = <span class="keyword">True</span></div><div class="line"><a name="l01105"></a><span class="lineno"> 1105</span>&#160;        a.sum().backward()</div><div class="line"><a name="l01106"></a><span class="lineno"> 1106</span>&#160;        self.assertEqual(a.grad.data, torch.ones(2, 3))</div><div class="line"><a name="l01107"></a><span class="lineno"> 1107</span>&#160;</div><div class="line"><a name="l01108"></a><span class="lineno"> 1108</span>&#160;        <span class="comment"># same but with a view</span></div><div class="line"><a name="l01109"></a><span class="lineno"> 1109</span>&#160;        a = torch.randn(2, 3)</div><div class="line"><a name="l01110"></a><span class="lineno"> 1110</span>&#160;        b = a[:]</div><div class="line"><a name="l01111"></a><span class="lineno"> 1111</span>&#160;        b.add_(5)</div><div class="line"><a name="l01112"></a><span class="lineno"> 1112</span>&#160;        a.requires_grad = <span class="keyword">True</span></div><div class="line"><a name="l01113"></a><span class="lineno"> 1113</span>&#160;        a.sum().backward()</div><div class="line"><a name="l01114"></a><span class="lineno"> 1114</span>&#160;        self.assertEqual(a.grad.data, torch.ones(2, 3))</div><div class="line"><a name="l01115"></a><span class="lineno"> 1115</span>&#160;</div><div class="line"><a name="l01116"></a><span class="lineno"> 1116</span>&#160;        <span class="comment"># should fail if requires_grad = True when we modify inplace</span></div><div class="line"><a name="l01117"></a><span class="lineno"> 1117</span>&#160;        a = torch.randn(2, 3)</div><div class="line"><a name="l01118"></a><span class="lineno"> 1118</span>&#160;        b = a[:]</div><div class="line"><a name="l01119"></a><span class="lineno"> 1119</span>&#160;        a.requires_grad = <span class="keyword">True</span></div><div class="line"><a name="l01120"></a><span class="lineno"> 1120</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01121"></a><span class="lineno"> 1121</span>&#160;            a.add_(5)</div><div class="line"><a name="l01122"></a><span class="lineno"> 1122</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01123"></a><span class="lineno"> 1123</span>&#160;            b.add_(5)</div><div class="line"><a name="l01124"></a><span class="lineno"> 1124</span>&#160;</div><div class="line"><a name="l01125"></a><span class="lineno"> 1125</span>&#160;    <span class="keyword">def </span>test_requires_grad_factory(self):</div><div class="line"><a name="l01126"></a><span class="lineno"> 1126</span>&#160;        x = torch.randn(2, 3)</div><div class="line"><a name="l01127"></a><span class="lineno"> 1127</span>&#160;        fns = [torch.ones_like, torch.testing.randn_like]</div><div class="line"><a name="l01128"></a><span class="lineno"> 1128</span>&#160;        dtypes = [torch.float32, torch.float64]</div><div class="line"><a name="l01129"></a><span class="lineno"> 1129</span>&#160;        <span class="keywordflow">for</span> fn <span class="keywordflow">in</span> fns:</div><div class="line"><a name="l01130"></a><span class="lineno"> 1130</span>&#160;            <span class="keywordflow">for</span> requires_grad <span class="keywordflow">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line"><a name="l01131"></a><span class="lineno"> 1131</span>&#160;                <span class="keywordflow">for</span> dtype <span class="keywordflow">in</span> dtypes:</div><div class="line"><a name="l01132"></a><span class="lineno"> 1132</span>&#160;                    <span class="keywordflow">for</span> use_cuda <span class="keywordflow">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div><div class="line"><a name="l01133"></a><span class="lineno"> 1133</span>&#160;                        <span class="keywordflow">if</span> <span class="keywordflow">not</span> use_cuda:</div><div class="line"><a name="l01134"></a><span class="lineno"> 1134</span>&#160;                            output = fn(x, dtype=dtype, requires_grad=requires_grad)</div><div class="line"><a name="l01135"></a><span class="lineno"> 1135</span>&#160;                            self.assertEqual(requires_grad, output.requires_grad)</div><div class="line"><a name="l01136"></a><span class="lineno"> 1136</span>&#160;                            self.assertIs(dtype, output.dtype)</div><div class="line"><a name="l01137"></a><span class="lineno"> 1137</span>&#160;                        <span class="keywordflow">elif</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>() <span class="keywordflow">and</span> <a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>() &gt; 1:</div><div class="line"><a name="l01138"></a><span class="lineno"> 1138</span>&#160;                            output = fn(x, dtype=dtype, device=1, requires_grad=requires_grad)</div><div class="line"><a name="l01139"></a><span class="lineno"> 1139</span>&#160;                            self.assertEqual(requires_grad, output.requires_grad)</div><div class="line"><a name="l01140"></a><span class="lineno"> 1140</span>&#160;                            self.assertIs(dtype, output.dtype)</div><div class="line"><a name="l01141"></a><span class="lineno"> 1141</span>&#160;                            self.assertEqual(1, output.get_device())</div><div class="line"><a name="l01142"></a><span class="lineno"> 1142</span>&#160;</div><div class="line"><a name="l01143"></a><span class="lineno"> 1143</span>&#160;    <span class="keyword">def </span>test_attribute_deletion(self):</div><div class="line"><a name="l01144"></a><span class="lineno"> 1144</span>&#160;        x = torch.randn((5, 5), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01145"></a><span class="lineno"> 1145</span>&#160;        del x.grad</div><div class="line"><a name="l01146"></a><span class="lineno"> 1146</span>&#160;        self.assertIsNone(x.grad)</div><div class="line"><a name="l01147"></a><span class="lineno"> 1147</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01148"></a><span class="lineno"> 1148</span>&#160;            del x.data</div><div class="line"><a name="l01149"></a><span class="lineno"> 1149</span>&#160;        with self.assertRaises(TypeError):</div><div class="line"><a name="l01150"></a><span class="lineno"> 1150</span>&#160;            x.data = <span class="keywordtype">None</span></div><div class="line"><a name="l01151"></a><span class="lineno"> 1151</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01152"></a><span class="lineno"> 1152</span>&#160;            del x.requires_grad</div><div class="line"><a name="l01153"></a><span class="lineno"> 1153</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01154"></a><span class="lineno"> 1154</span>&#160;            del x._grad_fn</div><div class="line"><a name="l01155"></a><span class="lineno"> 1155</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01156"></a><span class="lineno"> 1156</span>&#160;            del x._backward_hooks</div><div class="line"><a name="l01157"></a><span class="lineno"> 1157</span>&#160;</div><div class="line"><a name="l01158"></a><span class="lineno"> 1158</span>&#160;    <span class="keyword">def </span>test_grad_assignment(self):</div><div class="line"><a name="l01159"></a><span class="lineno"> 1159</span>&#160;        x = torch.randn(5, 5)</div><div class="line"><a name="l01160"></a><span class="lineno"> 1160</span>&#160;</div><div class="line"><a name="l01161"></a><span class="lineno"> 1161</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01162"></a><span class="lineno"> 1162</span>&#160;            x.grad = torch.randn(2, 2)</div><div class="line"><a name="l01163"></a><span class="lineno"> 1163</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01164"></a><span class="lineno"> 1164</span>&#160;            x.grad = Variable(torch.randn(5, 5).long())</div><div class="line"><a name="l01165"></a><span class="lineno"> 1165</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01166"></a><span class="lineno"> 1166</span>&#160;            x.grad = x</div><div class="line"><a name="l01167"></a><span class="lineno"> 1167</span>&#160;</div><div class="line"><a name="l01168"></a><span class="lineno"> 1168</span>&#160;        <span class="keywordflow">if</span> <span class="keywordflow">not</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>():</div><div class="line"><a name="l01169"></a><span class="lineno"> 1169</span>&#160;            <span class="keywordflow">raise</span> unittest.SkipTest(<span class="stringliteral">&quot;CUDA not available&quot;</span>)</div><div class="line"><a name="l01170"></a><span class="lineno"> 1170</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01171"></a><span class="lineno"> 1171</span>&#160;            x.grad = Variable(torch.randn(5, 5).cuda())</div><div class="line"><a name="l01172"></a><span class="lineno"> 1172</span>&#160;        x = x.cuda().half()</div><div class="line"><a name="l01173"></a><span class="lineno"> 1173</span>&#160;        x.grad = torch.zeros_like(x)  <span class="comment"># would raise an error unless sparse type is properly handled</span></div><div class="line"><a name="l01174"></a><span class="lineno"> 1174</span>&#160;</div><div class="line"><a name="l01175"></a><span class="lineno"> 1175</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>() &lt; 2:</div><div class="line"><a name="l01176"></a><span class="lineno"> 1176</span>&#160;            <span class="keywordflow">raise</span> unittest.SkipTest(<span class="stringliteral">&quot;At least 2 CUDA devices needed&quot;</span>)</div><div class="line"><a name="l01177"></a><span class="lineno"> 1177</span>&#160;        x = Variable(torch.randn(5, 5).cuda(0))</div><div class="line"><a name="l01178"></a><span class="lineno"> 1178</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01179"></a><span class="lineno"> 1179</span>&#160;            x.grad = Variable(torch.randn(5, 5).cuda(1))</div><div class="line"><a name="l01180"></a><span class="lineno"> 1180</span>&#160;</div><div class="line"><a name="l01181"></a><span class="lineno"> 1181</span>&#160;    <span class="keyword">def </span>test_duplicate_backward_root(self):</div><div class="line"><a name="l01182"></a><span class="lineno"> 1182</span>&#160;        a = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01183"></a><span class="lineno"> 1183</span>&#160;        b = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01184"></a><span class="lineno"> 1184</span>&#160;</div><div class="line"><a name="l01185"></a><span class="lineno"> 1185</span>&#160;        x = a * b</div><div class="line"><a name="l01186"></a><span class="lineno"> 1186</span>&#160;        grad_output = torch.randn_like(x)</div><div class="line"><a name="l01187"></a><span class="lineno"> 1187</span>&#160;        <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([x, x], [grad_output, grad_output])</div><div class="line"><a name="l01188"></a><span class="lineno"> 1188</span>&#160;</div><div class="line"><a name="l01189"></a><span class="lineno"> 1189</span>&#160;        self.assertEqual(a.grad.data, b.data * grad_output * 2)</div><div class="line"><a name="l01190"></a><span class="lineno"> 1190</span>&#160;        self.assertEqual(b.grad.data, a.data * grad_output * 2)</div><div class="line"><a name="l01191"></a><span class="lineno"> 1191</span>&#160;</div><div class="line"><a name="l01192"></a><span class="lineno"> 1192</span>&#160;    <span class="keyword">def </span>test_backward_no_grad(self):</div><div class="line"><a name="l01193"></a><span class="lineno"> 1193</span>&#160;        a = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01194"></a><span class="lineno"> 1194</span>&#160;        b = a + 2</div><div class="line"><a name="l01195"></a><span class="lineno"> 1195</span>&#160;        with self.assertRaises(RuntimeError):</div><div class="line"><a name="l01196"></a><span class="lineno"> 1196</span>&#160;            <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([b], [<span class="keywordtype">None</span>])</div><div class="line"><a name="l01197"></a><span class="lineno"> 1197</span>&#160;</div><div class="line"><a name="l01198"></a><span class="lineno"> 1198</span>&#160;    <span class="keyword">def </span>test_next_functions(self):</div><div class="line"><a name="l01199"></a><span class="lineno"> 1199</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01200"></a><span class="lineno"> 1200</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01201"></a><span class="lineno"> 1201</span>&#160;</div><div class="line"><a name="l01202"></a><span class="lineno"> 1202</span>&#160;        a = x + y</div><div class="line"><a name="l01203"></a><span class="lineno"> 1203</span>&#160;        self.assertIsNotNone(a.grad_fn)</div><div class="line"><a name="l01204"></a><span class="lineno"> 1204</span>&#160;        next_functions = a.grad_fn.next_functions</div><div class="line"><a name="l01205"></a><span class="lineno"> 1205</span>&#160;        self.assertEqual(len(next_functions), 2)</div><div class="line"><a name="l01206"></a><span class="lineno"> 1206</span>&#160;        self.assertIsInstance(next_functions[0][0], torch._C._functions.AccumulateGrad)</div><div class="line"><a name="l01207"></a><span class="lineno"> 1207</span>&#160;        self.assertEqual(next_functions[0][1], 0)</div><div class="line"><a name="l01208"></a><span class="lineno"> 1208</span>&#160;        self.assertIsInstance(next_functions[1][0], torch._C._functions.AccumulateGrad)</div><div class="line"><a name="l01209"></a><span class="lineno"> 1209</span>&#160;        self.assertEqual(next_functions[1][1], 0)</div><div class="line"><a name="l01210"></a><span class="lineno"> 1210</span>&#160;</div><div class="line"><a name="l01211"></a><span class="lineno"> 1211</span>&#160;        b = a + 5</div><div class="line"><a name="l01212"></a><span class="lineno"> 1212</span>&#160;        next_functions = b.grad_fn.next_functions</div><div class="line"><a name="l01213"></a><span class="lineno"> 1213</span>&#160;        self.assertEqual(len(next_functions), 2)</div><div class="line"><a name="l01214"></a><span class="lineno"> 1214</span>&#160;        self.assertIs(next_functions[0][0], a.grad_fn)</div><div class="line"><a name="l01215"></a><span class="lineno"> 1215</span>&#160;        self.assertIs(next_functions[1][0], <span class="keywordtype">None</span>)</div><div class="line"><a name="l01216"></a><span class="lineno"> 1216</span>&#160;</div><div class="line"><a name="l01217"></a><span class="lineno"> 1217</span>&#160;    <span class="keyword">def </span>test_inplace(self):</div><div class="line"><a name="l01218"></a><span class="lineno"> 1218</span>&#160;        x = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01219"></a><span class="lineno"> 1219</span>&#160;        y = Variable(torch.ones(5, 5) * 4, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01220"></a><span class="lineno"> 1220</span>&#160;</div><div class="line"><a name="l01221"></a><span class="lineno"> 1221</span>&#160;        z = x * y</div><div class="line"><a name="l01222"></a><span class="lineno"> 1222</span>&#160;        q = z + y</div><div class="line"><a name="l01223"></a><span class="lineno"> 1223</span>&#160;        w = z * y</div><div class="line"><a name="l01224"></a><span class="lineno"> 1224</span>&#160;        z.add_(2)</div><div class="line"><a name="l01225"></a><span class="lineno"> 1225</span>&#160;        <span class="comment"># Add doesn&#39;t need it&#39;s inputs to do backward, so it shouldn&#39;t raise</span></div><div class="line"><a name="l01226"></a><span class="lineno"> 1226</span>&#160;        q.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l01227"></a><span class="lineno"> 1227</span>&#160;        <span class="comment"># Mul saves both inputs in forward, so it should raise</span></div><div class="line"><a name="l01228"></a><span class="lineno"> 1228</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: w.backward(torch.ones(5, 5)))</div><div class="line"><a name="l01229"></a><span class="lineno"> 1229</span>&#160;</div><div class="line"><a name="l01230"></a><span class="lineno"> 1230</span>&#160;        z = x * y</div><div class="line"><a name="l01231"></a><span class="lineno"> 1231</span>&#160;        q = z * y</div><div class="line"><a name="l01232"></a><span class="lineno"> 1232</span>&#160;        r = z + y</div><div class="line"><a name="l01233"></a><span class="lineno"> 1233</span>&#160;        w = z.add_(y)</div><div class="line"><a name="l01234"></a><span class="lineno"> 1234</span>&#160;        <span class="comment"># w is a the last expression, so this should succeed</span></div><div class="line"><a name="l01235"></a><span class="lineno"> 1235</span>&#160;        w.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l01236"></a><span class="lineno"> 1236</span>&#160;        <span class="comment"># r doesn&#39;t use the modified value in backward, so it should succeed</span></div><div class="line"><a name="l01237"></a><span class="lineno"> 1237</span>&#160;        r.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l01238"></a><span class="lineno"> 1238</span>&#160;        <span class="comment"># q uses dirty z, so it should raise</span></div><div class="line"><a name="l01239"></a><span class="lineno"> 1239</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: q.backward(torch.ones(5, 5)))</div><div class="line"><a name="l01240"></a><span class="lineno"> 1240</span>&#160;</div><div class="line"><a name="l01241"></a><span class="lineno"> 1241</span>&#160;        x.grad.data.zero_()</div><div class="line"><a name="l01242"></a><span class="lineno"> 1242</span>&#160;        m = x / 2</div><div class="line"><a name="l01243"></a><span class="lineno"> 1243</span>&#160;        z = m + y / 8</div><div class="line"><a name="l01244"></a><span class="lineno"> 1244</span>&#160;        q = z * y</div><div class="line"><a name="l01245"></a><span class="lineno"> 1245</span>&#160;        r = z + y</div><div class="line"><a name="l01246"></a><span class="lineno"> 1246</span>&#160;        prev_version = z._version</div><div class="line"><a name="l01247"></a><span class="lineno"> 1247</span>&#160;        w = z.exp_()</div><div class="line"><a name="l01248"></a><span class="lineno"> 1248</span>&#160;        self.assertNotEqual(z._version, prev_version)</div><div class="line"><a name="l01249"></a><span class="lineno"> 1249</span>&#160;        r.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l01250"></a><span class="lineno"> 1250</span>&#160;        self.assertEqual(x.grad.data, torch.ones(5, 5) / 2)</div><div class="line"><a name="l01251"></a><span class="lineno"> 1251</span>&#160;        w.backward(torch.ones(5, 5), retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l01252"></a><span class="lineno"> 1252</span>&#160;        self.assertEqual(x.grad.data, torch.Tensor(5, 5).fill_((1 + math.e) / 2))</div><div class="line"><a name="l01253"></a><span class="lineno"> 1253</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: q.backward(torch.ones(5, 5)))</div><div class="line"><a name="l01254"></a><span class="lineno"> 1254</span>&#160;</div><div class="line"><a name="l01255"></a><span class="lineno"> 1255</span>&#160;        leaf = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01256"></a><span class="lineno"> 1256</span>&#160;        x = leaf.clone()</div><div class="line"><a name="l01257"></a><span class="lineno"> 1257</span>&#160;        x.add_(10)</div><div class="line"><a name="l01258"></a><span class="lineno"> 1258</span>&#160;        self.assertEqual(x.data, torch.ones(5, 5) * 11)</div><div class="line"><a name="l01259"></a><span class="lineno"> 1259</span>&#160;        <span class="comment"># x should be still usable</span></div><div class="line"><a name="l01260"></a><span class="lineno"> 1260</span>&#160;        y = x + 2</div><div class="line"><a name="l01261"></a><span class="lineno"> 1261</span>&#160;        y.backward(torch.ones(5, 5))</div><div class="line"><a name="l01262"></a><span class="lineno"> 1262</span>&#160;        self.assertEqual(leaf.grad.data, torch.ones(5, 5))</div><div class="line"><a name="l01263"></a><span class="lineno"> 1263</span>&#160;        z = x * y</div><div class="line"><a name="l01264"></a><span class="lineno"> 1264</span>&#160;        x.add_(2)</div><div class="line"><a name="l01265"></a><span class="lineno"> 1265</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: z.backward(torch.ones(5, 5)))</div><div class="line"><a name="l01266"></a><span class="lineno"> 1266</span>&#160;</div><div class="line"><a name="l01267"></a><span class="lineno"> 1267</span>&#160;    <span class="keyword">def </span>test_mark_non_differentiable(self):</div><div class="line"><a name="l01268"></a><span class="lineno"> 1268</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l01269"></a><span class="lineno"> 1269</span>&#160;            @staticmethod</div><div class="line"><a name="l01270"></a><span class="lineno"> 1270</span>&#160;            <span class="keyword">def </span>forward(ctx, input):</div><div class="line"><a name="l01271"></a><span class="lineno"> 1271</span>&#160;                output = input &gt; 0</div><div class="line"><a name="l01272"></a><span class="lineno"> 1272</span>&#160;                ctx.mark_non_differentiable(output)</div><div class="line"><a name="l01273"></a><span class="lineno"> 1273</span>&#160;                <span class="keywordflow">return</span> output</div><div class="line"><a name="l01274"></a><span class="lineno"> 1274</span>&#160;</div><div class="line"><a name="l01275"></a><span class="lineno"> 1275</span>&#160;            @staticmethod</div><div class="line"><a name="l01276"></a><span class="lineno"> 1276</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l01277"></a><span class="lineno"> 1277</span>&#160;                <span class="keywordflow">return</span> (grad_output * 0).type(torch.DoubleTensor)</div><div class="line"><a name="l01278"></a><span class="lineno"> 1278</span>&#160;</div><div class="line"><a name="l01279"></a><span class="lineno"> 1279</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01280"></a><span class="lineno"> 1280</span>&#160;        mask = MyFunction.apply(x)</div><div class="line"><a name="l01281"></a><span class="lineno"> 1281</span>&#160;        self.assertFalse(mask.requires_grad)</div><div class="line"><a name="l01282"></a><span class="lineno"> 1282</span>&#160;        y = x.masked_fill(mask, 0)</div><div class="line"><a name="l01283"></a><span class="lineno"> 1283</span>&#160;        y.sum().backward()</div><div class="line"><a name="l01284"></a><span class="lineno"> 1284</span>&#160;</div><div class="line"><a name="l01285"></a><span class="lineno"> 1285</span>&#160;    <span class="keyword">def </span>test_mark_non_differentiable_mixed(self):</div><div class="line"><a name="l01286"></a><span class="lineno"> 1286</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l01287"></a><span class="lineno"> 1287</span>&#160;            @staticmethod</div><div class="line"><a name="l01288"></a><span class="lineno"> 1288</span>&#160;            <span class="keyword">def </span>forward(ctx, input):</div><div class="line"><a name="l01289"></a><span class="lineno"> 1289</span>&#160;                a = input + 1</div><div class="line"><a name="l01290"></a><span class="lineno"> 1290</span>&#160;                b = input + 2</div><div class="line"><a name="l01291"></a><span class="lineno"> 1291</span>&#160;                ctx.mark_non_differentiable(a)</div><div class="line"><a name="l01292"></a><span class="lineno"> 1292</span>&#160;                <span class="keywordflow">return</span> a, b</div><div class="line"><a name="l01293"></a><span class="lineno"> 1293</span>&#160;</div><div class="line"><a name="l01294"></a><span class="lineno"> 1294</span>&#160;            @staticmethod</div><div class="line"><a name="l01295"></a><span class="lineno"> 1295</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_a, grad_b):</div><div class="line"><a name="l01296"></a><span class="lineno"> 1296</span>&#160;                self.assertTrue((grad_a == 0).all())</div><div class="line"><a name="l01297"></a><span class="lineno"> 1297</span>&#160;                self.assertTrue((grad_b == 1).all())</div><div class="line"><a name="l01298"></a><span class="lineno"> 1298</span>&#160;                <span class="keywordflow">return</span> grad_b</div><div class="line"><a name="l01299"></a><span class="lineno"> 1299</span>&#160;</div><div class="line"><a name="l01300"></a><span class="lineno"> 1300</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01301"></a><span class="lineno"> 1301</span>&#160;        a, b = MyFunction.apply(x)</div><div class="line"><a name="l01302"></a><span class="lineno"> 1302</span>&#160;        self.assertFalse(a.requires_grad)</div><div class="line"><a name="l01303"></a><span class="lineno"> 1303</span>&#160;        self.assertTrue(b.requires_grad)</div><div class="line"><a name="l01304"></a><span class="lineno"> 1304</span>&#160;        b.sum().backward()</div><div class="line"><a name="l01305"></a><span class="lineno"> 1305</span>&#160;        self.assertEqual(x.grad.data, torch.ones(5, 5))</div><div class="line"><a name="l01306"></a><span class="lineno"> 1306</span>&#160;</div><div class="line"><a name="l01307"></a><span class="lineno"> 1307</span>&#160;    <span class="keyword">def </span>test_mark_non_differentiable_none(self):</div><div class="line"><a name="l01308"></a><span class="lineno"> 1308</span>&#160;        <span class="comment"># This used to segfault because MyFunction would send back null</span></div><div class="line"><a name="l01309"></a><span class="lineno"> 1309</span>&#160;        <span class="comment"># gradients to MulBackward, which is implemented in C++. C++</span></div><div class="line"><a name="l01310"></a><span class="lineno"> 1310</span>&#160;        <span class="comment"># implemented functions expect incoming  grad_ouptuts to be non-null.</span></div><div class="line"><a name="l01311"></a><span class="lineno"> 1311</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l01312"></a><span class="lineno"> 1312</span>&#160;            @staticmethod</div><div class="line"><a name="l01313"></a><span class="lineno"> 1313</span>&#160;            <span class="keyword">def </span>forward(ctx, input):</div><div class="line"><a name="l01314"></a><span class="lineno"> 1314</span>&#160;                output = input.clone()</div><div class="line"><a name="l01315"></a><span class="lineno"> 1315</span>&#160;                ctx.mark_non_differentiable(output)</div><div class="line"><a name="l01316"></a><span class="lineno"> 1316</span>&#160;                <span class="keywordflow">return</span> output</div><div class="line"><a name="l01317"></a><span class="lineno"> 1317</span>&#160;</div><div class="line"><a name="l01318"></a><span class="lineno"> 1318</span>&#160;            @staticmethod</div><div class="line"><a name="l01319"></a><span class="lineno"> 1319</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l01320"></a><span class="lineno"> 1320</span>&#160;                <span class="keywordflow">return</span> <span class="keywordtype">None</span></div><div class="line"><a name="l01321"></a><span class="lineno"> 1321</span>&#160;</div><div class="line"><a name="l01322"></a><span class="lineno"> 1322</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01323"></a><span class="lineno"> 1323</span>&#160;        r = MyFunction.apply(x * x)</div><div class="line"><a name="l01324"></a><span class="lineno"> 1324</span>&#160;        (r * x).sum().backward()</div><div class="line"><a name="l01325"></a><span class="lineno"> 1325</span>&#160;</div><div class="line"><a name="l01326"></a><span class="lineno"> 1326</span>&#160;    <span class="keyword">def </span>test_return_duplicate(self):</div><div class="line"><a name="l01327"></a><span class="lineno"> 1327</span>&#160;        <span class="keyword">class </span>DoubleDuplicate(Function):</div><div class="line"><a name="l01328"></a><span class="lineno"> 1328</span>&#160;            @staticmethod</div><div class="line"><a name="l01329"></a><span class="lineno"> 1329</span>&#160;            <span class="keyword">def </span>forward(ctx, x):</div><div class="line"><a name="l01330"></a><span class="lineno"> 1330</span>&#160;                output = x * 2</div><div class="line"><a name="l01331"></a><span class="lineno"> 1331</span>&#160;                <span class="keywordflow">return</span> output, output</div><div class="line"><a name="l01332"></a><span class="lineno"> 1332</span>&#160;</div><div class="line"><a name="l01333"></a><span class="lineno"> 1333</span>&#160;            @staticmethod</div><div class="line"><a name="l01334"></a><span class="lineno"> 1334</span>&#160;            <span class="keyword">def </span>backward(ctx, grad1, grad2):</div><div class="line"><a name="l01335"></a><span class="lineno"> 1335</span>&#160;                <span class="keywordflow">return</span> grad1 * 2 + grad2 * 2</div><div class="line"><a name="l01336"></a><span class="lineno"> 1336</span>&#160;</div><div class="line"><a name="l01337"></a><span class="lineno"> 1337</span>&#160;        <span class="keyword">def </span>fn(x):</div><div class="line"><a name="l01338"></a><span class="lineno"> 1338</span>&#160;            a, b = DoubleDuplicate.apply(x)</div><div class="line"><a name="l01339"></a><span class="lineno"> 1339</span>&#160;            self.assertIs(a, b)</div><div class="line"><a name="l01340"></a><span class="lineno"> 1340</span>&#160;            <span class="keywordflow">return</span> a + b</div><div class="line"><a name="l01341"></a><span class="lineno"> 1341</span>&#160;</div><div class="line"><a name="l01342"></a><span class="lineno"> 1342</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01343"></a><span class="lineno"> 1343</span>&#160;        gradcheck(fn, [x])</div><div class="line"><a name="l01344"></a><span class="lineno"> 1344</span>&#160;        gradgradcheck(fn, [x])</div><div class="line"><a name="l01345"></a><span class="lineno"> 1345</span>&#160;</div><div class="line"><a name="l01346"></a><span class="lineno"> 1346</span>&#160;    <span class="keyword">def </span>test_return_duplicate_inplace(self):</div><div class="line"><a name="l01347"></a><span class="lineno"> 1347</span>&#160;        <span class="keyword">class </span>DoubleInplace(Function):</div><div class="line"><a name="l01348"></a><span class="lineno"> 1348</span>&#160;            @staticmethod</div><div class="line"><a name="l01349"></a><span class="lineno"> 1349</span>&#160;            <span class="keyword">def </span>forward(ctx, x):</div><div class="line"><a name="l01350"></a><span class="lineno"> 1350</span>&#160;                x.mul_(2)</div><div class="line"><a name="l01351"></a><span class="lineno"> 1351</span>&#160;                ctx.mark_dirty(x)</div><div class="line"><a name="l01352"></a><span class="lineno"> 1352</span>&#160;                <span class="keywordflow">return</span> x, x</div><div class="line"><a name="l01353"></a><span class="lineno"> 1353</span>&#160;</div><div class="line"><a name="l01354"></a><span class="lineno"> 1354</span>&#160;            @staticmethod</div><div class="line"><a name="l01355"></a><span class="lineno"> 1355</span>&#160;            <span class="keyword">def </span>backward(ctx, grad1, grad2):</div><div class="line"><a name="l01356"></a><span class="lineno"> 1356</span>&#160;                <span class="keywordflow">return</span> grad1 * 2 + grad2 * 2</div><div class="line"><a name="l01357"></a><span class="lineno"> 1357</span>&#160;</div><div class="line"><a name="l01358"></a><span class="lineno"> 1358</span>&#160;        <span class="keyword">def </span>inplace_fn(x):</div><div class="line"><a name="l01359"></a><span class="lineno"> 1359</span>&#160;            a, b = DoubleInplace.apply(x.clone())</div><div class="line"><a name="l01360"></a><span class="lineno"> 1360</span>&#160;            self.assertIs(a, b)</div><div class="line"><a name="l01361"></a><span class="lineno"> 1361</span>&#160;            <span class="keywordflow">return</span> a + b</div><div class="line"><a name="l01362"></a><span class="lineno"> 1362</span>&#160;</div><div class="line"><a name="l01363"></a><span class="lineno"> 1363</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01364"></a><span class="lineno"> 1364</span>&#160;        gradcheck(inplace_fn, [x])</div><div class="line"><a name="l01365"></a><span class="lineno"> 1365</span>&#160;        gradgradcheck(inplace_fn, [x])</div><div class="line"><a name="l01366"></a><span class="lineno"> 1366</span>&#160;</div><div class="line"><a name="l01367"></a><span class="lineno"> 1367</span>&#160;        <span class="comment"># Can&#39;t modify leaf variables in-place</span></div><div class="line"><a name="l01368"></a><span class="lineno"> 1368</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: InplaceFunction.apply(x))</div><div class="line"><a name="l01369"></a><span class="lineno"> 1369</span>&#160;        <span class="comment"># Functions which modify views in-place must return only one output</span></div><div class="line"><a name="l01370"></a><span class="lineno"> 1370</span>&#160;        self.assertRaises(RuntimeError, <span class="keyword">lambda</span>: InplaceFunction.apply(x.clone()[0]))</div><div class="line"><a name="l01371"></a><span class="lineno"> 1371</span>&#160;</div><div class="line"><a name="l01372"></a><span class="lineno"> 1372</span>&#160;    @suppress_warnings</div><div class="line"><a name="l01373"></a><span class="lineno"> 1373</span>&#160;    <span class="keyword">def </span>test_resize(self):</div><div class="line"><a name="l01374"></a><span class="lineno"> 1374</span>&#160;        x = torch.ones(2, 3)</div><div class="line"><a name="l01375"></a><span class="lineno"> 1375</span>&#160;        self.assertTrue(x.resize(3, 2).size() == (3, 2))</div><div class="line"><a name="l01376"></a><span class="lineno"> 1376</span>&#160;</div><div class="line"><a name="l01377"></a><span class="lineno"> 1377</span>&#160;    <span class="keyword">def </span>_test_setitem(self, size, index):</div><div class="line"><a name="l01378"></a><span class="lineno"> 1378</span>&#160;        x = torch.ones(*size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01379"></a><span class="lineno"> 1379</span>&#160;        y = x + 2</div><div class="line"><a name="l01380"></a><span class="lineno"> 1380</span>&#160;        y_version = y._version</div><div class="line"><a name="l01381"></a><span class="lineno"> 1381</span>&#160;        y[index] = 2</div><div class="line"><a name="l01382"></a><span class="lineno"> 1382</span>&#160;        self.assertNotEqual(y._version, y_version)</div><div class="line"><a name="l01383"></a><span class="lineno"> 1383</span>&#160;        y.backward(torch.ones(*size))</div><div class="line"><a name="l01384"></a><span class="lineno"> 1384</span>&#160;        expected_grad = torch.ones(*size)</div><div class="line"><a name="l01385"></a><span class="lineno"> 1385</span>&#160;        expected_grad[index] = 0</div><div class="line"><a name="l01386"></a><span class="lineno"> 1386</span>&#160;        self.assertEqual(x.grad, expected_grad)</div><div class="line"><a name="l01387"></a><span class="lineno"> 1387</span>&#160;</div><div class="line"><a name="l01388"></a><span class="lineno"> 1388</span>&#160;    <span class="keyword">def </span>_test_setitem_tensor(self, size, index):</div><div class="line"><a name="l01389"></a><span class="lineno"> 1389</span>&#160;        x = torch.ones(*size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01390"></a><span class="lineno"> 1390</span>&#160;        y = x + 2</div><div class="line"><a name="l01391"></a><span class="lineno"> 1391</span>&#160;        y_version = y._version</div><div class="line"><a name="l01392"></a><span class="lineno"> 1392</span>&#160;        value = x.new(x[index].size()).fill_(7)</div><div class="line"><a name="l01393"></a><span class="lineno"> 1393</span>&#160;        value.requires_grad = <span class="keyword">True</span></div><div class="line"><a name="l01394"></a><span class="lineno"> 1394</span>&#160;        y[index] = value</div><div class="line"><a name="l01395"></a><span class="lineno"> 1395</span>&#160;        self.assertNotEqual(y._version, y_version)</div><div class="line"><a name="l01396"></a><span class="lineno"> 1396</span>&#160;        y.backward(torch.ones(*size))</div><div class="line"><a name="l01397"></a><span class="lineno"> 1397</span>&#160;        expected_grad_input = torch.ones(*size)</div><div class="line"><a name="l01398"></a><span class="lineno"> 1398</span>&#160;        expected_grad_input[index] = 0</div><div class="line"><a name="l01399"></a><span class="lineno"> 1399</span>&#160;        self.assertEqual(x.grad, expected_grad_input)</div><div class="line"><a name="l01400"></a><span class="lineno"> 1400</span>&#160;        self.assertEqual(value.grad, torch.ones_like(value))</div><div class="line"><a name="l01401"></a><span class="lineno"> 1401</span>&#160;</div><div class="line"><a name="l01402"></a><span class="lineno"> 1402</span>&#160;        <span class="comment"># case when x broadcasts to as y[1]</span></div><div class="line"><a name="l01403"></a><span class="lineno"> 1403</span>&#160;        x = torch.randn(4, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01404"></a><span class="lineno"> 1404</span>&#160;        y = torch.zeros(2, 3, 4)</div><div class="line"><a name="l01405"></a><span class="lineno"> 1405</span>&#160;        y[1] = x</div><div class="line"><a name="l01406"></a><span class="lineno"> 1406</span>&#160;        y.backward(torch.randn(2, 3, 4))</div><div class="line"><a name="l01407"></a><span class="lineno"> 1407</span>&#160;        self.assertEqual(x.size(), x.grad.size())</div><div class="line"><a name="l01408"></a><span class="lineno"> 1408</span>&#160;</div><div class="line"><a name="l01409"></a><span class="lineno"> 1409</span>&#160;    <span class="keyword">def </span>test_setitem(self):</div><div class="line"><a name="l01410"></a><span class="lineno"> 1410</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5), 1)</div><div class="line"><a name="l01411"></a><span class="lineno"> 1411</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5,), 1)</div><div class="line"><a name="l01412"></a><span class="lineno"> 1412</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((1,), 0)</div><div class="line"><a name="l01413"></a><span class="lineno"> 1413</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((10,), [[0, 4, 2]])</div><div class="line"><a name="l01414"></a><span class="lineno"> 1414</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5), [[0, 4], [2, 2]])</div><div class="line"><a name="l01415"></a><span class="lineno"> 1415</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5, 5), [slice(<span class="keywordtype">None</span>), slice(<span class="keywordtype">None</span>), [1, 3]])</div><div class="line"><a name="l01416"></a><span class="lineno"> 1416</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5, 5), [slice(<span class="keywordtype">None</span>), [1, 3], slice(<span class="keywordtype">None</span>)])</div><div class="line"><a name="l01417"></a><span class="lineno"> 1417</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5, 5), [[1, 3], slice(<span class="keywordtype">None</span>), slice(<span class="keywordtype">None</span>)])</div><div class="line"><a name="l01418"></a><span class="lineno"> 1418</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5, 5), [slice(<span class="keywordtype">None</span>), [2, 4], [1, 3]])</div><div class="line"><a name="l01419"></a><span class="lineno"> 1419</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5, 5), [[1, 3], [2, 4], slice(<span class="keywordtype">None</span>)])</div><div class="line"><a name="l01420"></a><span class="lineno"> 1420</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5), 3)</div><div class="line"><a name="l01421"></a><span class="lineno"> 1421</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5), [[0, 1], [1, 0]])</div><div class="line"><a name="l01422"></a><span class="lineno"> 1422</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5,), 3)</div><div class="line"><a name="l01423"></a><span class="lineno"> 1423</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5,), Variable(torch.LongTensor([3]), requires_grad=<span class="keyword">False</span>).sum())</div><div class="line"><a name="l01424"></a><span class="lineno"> 1424</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5,), [[0, 1, 2, 3]])</div><div class="line"><a name="l01425"></a><span class="lineno"> 1425</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5, 5), [slice(<span class="keywordtype">None</span>), slice(<span class="keywordtype">None</span>), [1, 3]])</div><div class="line"><a name="l01426"></a><span class="lineno"> 1426</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5, 5), [slice(<span class="keywordtype">None</span>), [1, 3], slice(<span class="keywordtype">None</span>)])</div><div class="line"><a name="l01427"></a><span class="lineno"> 1427</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5, 5), [[1, 3], slice(<span class="keywordtype">None</span>), slice(<span class="keywordtype">None</span>)])</div><div class="line"><a name="l01428"></a><span class="lineno"> 1428</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5, 5), [slice(<span class="keywordtype">None</span>), [2, 4], [1, 3]])</div><div class="line"><a name="l01429"></a><span class="lineno"> 1429</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5, 5), [[1, 3], [2, 4], slice(<span class="keywordtype">None</span>)])</div><div class="line"><a name="l01430"></a><span class="lineno"> 1430</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5, 5), [Variable(torch.LongTensor([1,</div><div class="line"><a name="l01431"></a><span class="lineno"> 1431</span>&#160;                                              3]), requires_grad=<span class="keyword">False</span>), [2, 4], slice(<span class="keywordtype">None</span>)])</div><div class="line"><a name="l01432"></a><span class="lineno"> 1432</span>&#160;</div><div class="line"><a name="l01433"></a><span class="lineno"> 1433</span>&#160;    <span class="keyword">def </span>test_setitem_mask(self):</div><div class="line"><a name="l01434"></a><span class="lineno"> 1434</span>&#160;        mask = torch.ByteTensor(5, 5).bernoulli_()</div><div class="line"><a name="l01435"></a><span class="lineno"> 1435</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5, 5), Variable(mask))</div><div class="line"><a name="l01436"></a><span class="lineno"> 1436</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((5,), Variable(mask[0]))</div><div class="line"><a name="l01437"></a><span class="lineno"> 1437</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">_test_setitem</a>((1,), Variable(mask[0, 0:1]))</div><div class="line"><a name="l01438"></a><span class="lineno"> 1438</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5, 5), Variable(mask))</div><div class="line"><a name="l01439"></a><span class="lineno"> 1439</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">_test_setitem_tensor</a>((5,), Variable(mask[0]))</div><div class="line"><a name="l01440"></a><span class="lineno"> 1440</span>&#160;</div><div class="line"><a name="l01441"></a><span class="lineno"> 1441</span>&#160;    <span class="keyword">def </span>test_select_sum(self):</div><div class="line"><a name="l01442"></a><span class="lineno"> 1442</span>&#160;        <span class="comment"># both select and sum return Scalars in ATen; ensure they work together.</span></div><div class="line"><a name="l01443"></a><span class="lineno"> 1443</span>&#160;        x = torch.randn(10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01444"></a><span class="lineno"> 1444</span>&#160;</div><div class="line"><a name="l01445"></a><span class="lineno"> 1445</span>&#160;        <span class="keyword">def </span>func(x):</div><div class="line"><a name="l01446"></a><span class="lineno"> 1446</span>&#160;            <span class="keywordflow">return</span> x.select(0, 1).sum()</div><div class="line"><a name="l01447"></a><span class="lineno"> 1447</span>&#160;</div><div class="line"><a name="l01448"></a><span class="lineno"> 1448</span>&#160;        gradcheck(func, [x])</div><div class="line"><a name="l01449"></a><span class="lineno"> 1449</span>&#160;        gradgradcheck(func, [x])</div><div class="line"><a name="l01450"></a><span class="lineno"> 1450</span>&#160;</div><div class="line"><a name="l01451"></a><span class="lineno"> 1451</span>&#160;    <span class="keyword">def </span>test_stack(self):</div><div class="line"><a name="l01452"></a><span class="lineno"> 1452</span>&#160;        x = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01453"></a><span class="lineno"> 1453</span>&#160;        y = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01454"></a><span class="lineno"> 1454</span>&#160;        z = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01455"></a><span class="lineno"> 1455</span>&#160;        stacked = torch.stack([x, y, z], 0)</div><div class="line"><a name="l01456"></a><span class="lineno"> 1456</span>&#160;        grad = torch.randn(3, 10, 10)</div><div class="line"><a name="l01457"></a><span class="lineno"> 1457</span>&#160;        stacked.backward(grad)</div><div class="line"><a name="l01458"></a><span class="lineno"> 1458</span>&#160;        self.assertEqual(x.grad.data, grad[0])</div><div class="line"><a name="l01459"></a><span class="lineno"> 1459</span>&#160;        self.assertEqual(y.grad.data, grad[1])</div><div class="line"><a name="l01460"></a><span class="lineno"> 1460</span>&#160;        self.assertEqual(z.grad.data, grad[2])</div><div class="line"><a name="l01461"></a><span class="lineno"> 1461</span>&#160;</div><div class="line"><a name="l01462"></a><span class="lineno"> 1462</span>&#160;    <span class="keyword">def </span>test_unbind(self):</div><div class="line"><a name="l01463"></a><span class="lineno"> 1463</span>&#160;        stacked = torch.randn(3, 10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01464"></a><span class="lineno"> 1464</span>&#160;        x, y, z = stacked.unbind()</div><div class="line"><a name="l01465"></a><span class="lineno"> 1465</span>&#160;        grad = torch.randn(3, 10, 10)</div><div class="line"><a name="l01466"></a><span class="lineno"> 1466</span>&#160;        <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([x, y, z], grad.unbind())</div><div class="line"><a name="l01467"></a><span class="lineno"> 1467</span>&#160;        self.assertEqual(stacked.grad.data, grad)</div><div class="line"><a name="l01468"></a><span class="lineno"> 1468</span>&#160;        <span class="comment"># check that it works with only one gradient provided (#9977)</span></div><div class="line"><a name="l01469"></a><span class="lineno"> 1469</span>&#160;        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(3):</div><div class="line"><a name="l01470"></a><span class="lineno"> 1470</span>&#160;            stacked = torch.randn(3, 10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01471"></a><span class="lineno"> 1471</span>&#160;            outs = stacked.unbind()</div><div class="line"><a name="l01472"></a><span class="lineno"> 1472</span>&#160;            gi = grad.unbind()[i]</div><div class="line"><a name="l01473"></a><span class="lineno"> 1473</span>&#160;            g, = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(outs[i], stacked, gi)</div><div class="line"><a name="l01474"></a><span class="lineno"> 1474</span>&#160;            g_expected = torch.stack([gi <span class="keywordflow">if</span> j == i <span class="keywordflow">else</span> torch.zeros_like(gi)</div><div class="line"><a name="l01475"></a><span class="lineno"> 1475</span>&#160;                                      <span class="keywordflow">for</span> j <span class="keywordflow">in</span> range(3)], dim=0)</div><div class="line"><a name="l01476"></a><span class="lineno"> 1476</span>&#160;            self.assertEqual(g, g_expected)</div><div class="line"><a name="l01477"></a><span class="lineno"> 1477</span>&#160;</div><div class="line"><a name="l01478"></a><span class="lineno"> 1478</span>&#160;    <span class="keyword">def </span>test_put(self):</div><div class="line"><a name="l01479"></a><span class="lineno"> 1479</span>&#160;        root = torch.randn(4, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01480"></a><span class="lineno"> 1480</span>&#160;        values = torch.randn(6, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01481"></a><span class="lineno"> 1481</span>&#160;        idx = Variable(torch.LongTensor([1, 2, 3, -1, -2, -3]))</div><div class="line"><a name="l01482"></a><span class="lineno"> 1482</span>&#160;</div><div class="line"><a name="l01483"></a><span class="lineno"> 1483</span>&#160;        <span class="keyword">def </span>func(root, values):</div><div class="line"><a name="l01484"></a><span class="lineno"> 1484</span>&#160;            x = root.clone()</div><div class="line"><a name="l01485"></a><span class="lineno"> 1485</span>&#160;            x.put_(idx, values)</div><div class="line"><a name="l01486"></a><span class="lineno"> 1486</span>&#160;            <span class="keywordflow">return</span> x</div><div class="line"><a name="l01487"></a><span class="lineno"> 1487</span>&#160;</div><div class="line"><a name="l01488"></a><span class="lineno"> 1488</span>&#160;        gradcheck(func, [root, values])</div><div class="line"><a name="l01489"></a><span class="lineno"> 1489</span>&#160;        gradgradcheck(func, [root, values])</div><div class="line"><a name="l01490"></a><span class="lineno"> 1490</span>&#160;</div><div class="line"><a name="l01491"></a><span class="lineno"> 1491</span>&#160;    <span class="keyword">def </span>test_put_accumulate(self):</div><div class="line"><a name="l01492"></a><span class="lineno"> 1492</span>&#160;        root = torch.randn(4, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01493"></a><span class="lineno"> 1493</span>&#160;        values = torch.randn(6, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01494"></a><span class="lineno"> 1494</span>&#160;        idx = Variable(torch.LongTensor([1, 2, 3, 1, 2, 3]))</div><div class="line"><a name="l01495"></a><span class="lineno"> 1495</span>&#160;</div><div class="line"><a name="l01496"></a><span class="lineno"> 1496</span>&#160;        <span class="keyword">def </span>func(root, values):</div><div class="line"><a name="l01497"></a><span class="lineno"> 1497</span>&#160;            x = root.clone()</div><div class="line"><a name="l01498"></a><span class="lineno"> 1498</span>&#160;            x.put_(idx, values, accumulate=<span class="keyword">True</span>)</div><div class="line"><a name="l01499"></a><span class="lineno"> 1499</span>&#160;            <span class="keywordflow">return</span> x</div><div class="line"><a name="l01500"></a><span class="lineno"> 1500</span>&#160;</div><div class="line"><a name="l01501"></a><span class="lineno"> 1501</span>&#160;        gradcheck(func, [root, values])</div><div class="line"><a name="l01502"></a><span class="lineno"> 1502</span>&#160;        gradgradcheck(func, [root, values])</div><div class="line"><a name="l01503"></a><span class="lineno"> 1503</span>&#160;</div><div class="line"><a name="l01504"></a><span class="lineno"> 1504</span>&#160;    <span class="keyword">def </span>test_fill(self):</div><div class="line"><a name="l01505"></a><span class="lineno"> 1505</span>&#160;        root = torch.randn(4, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01506"></a><span class="lineno"> 1506</span>&#160;</div><div class="line"><a name="l01507"></a><span class="lineno"> 1507</span>&#160;        <span class="keyword">def </span>func(root):</div><div class="line"><a name="l01508"></a><span class="lineno"> 1508</span>&#160;            x = root.clone()</div><div class="line"><a name="l01509"></a><span class="lineno"> 1509</span>&#160;            x.fill_(2)</div><div class="line"><a name="l01510"></a><span class="lineno"> 1510</span>&#160;            <span class="keywordflow">return</span> x</div><div class="line"><a name="l01511"></a><span class="lineno"> 1511</span>&#160;</div><div class="line"><a name="l01512"></a><span class="lineno"> 1512</span>&#160;        gradcheck(func, [root])</div><div class="line"><a name="l01513"></a><span class="lineno"> 1513</span>&#160;        gradgradcheck(func, [root])</div><div class="line"><a name="l01514"></a><span class="lineno"> 1514</span>&#160;</div><div class="line"><a name="l01515"></a><span class="lineno"> 1515</span>&#160;    <span class="keyword">def </span>test_unused_output(self):</div><div class="line"><a name="l01516"></a><span class="lineno"> 1516</span>&#160;        x = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01517"></a><span class="lineno"> 1517</span>&#160;        outputs = x.chunk(5)</div><div class="line"><a name="l01518"></a><span class="lineno"> 1518</span>&#160;        o = outputs[2]</div><div class="line"><a name="l01519"></a><span class="lineno"> 1519</span>&#160;        o = o * 4 + 2</div><div class="line"><a name="l01520"></a><span class="lineno"> 1520</span>&#160;        o.sum().backward()</div><div class="line"><a name="l01521"></a><span class="lineno"> 1521</span>&#160;        expected_grad = torch.zeros(10, 10)</div><div class="line"><a name="l01522"></a><span class="lineno"> 1522</span>&#160;        expected_grad[4:6] = 4</div><div class="line"><a name="l01523"></a><span class="lineno"> 1523</span>&#160;        self.assertEqual(x.grad.data, expected_grad)</div><div class="line"><a name="l01524"></a><span class="lineno"> 1524</span>&#160;</div><div class="line"><a name="l01525"></a><span class="lineno"> 1525</span>&#160;        x.grad.data.zero_()</div><div class="line"><a name="l01526"></a><span class="lineno"> 1526</span>&#160;        grad_output = torch.randn(2, 10)</div><div class="line"><a name="l01527"></a><span class="lineno"> 1527</span>&#160;        outputs = x.chunk(5)</div><div class="line"><a name="l01528"></a><span class="lineno"> 1528</span>&#160;        outputs[0].backward(grad_output)</div><div class="line"><a name="l01529"></a><span class="lineno"> 1529</span>&#160;        expected_grad = torch.zeros(10, 10)</div><div class="line"><a name="l01530"></a><span class="lineno"> 1530</span>&#160;        expected_grad[:2] = grad_output</div><div class="line"><a name="l01531"></a><span class="lineno"> 1531</span>&#160;        self.assertEqual(x.grad.data, expected_grad)</div><div class="line"><a name="l01532"></a><span class="lineno"> 1532</span>&#160;</div><div class="line"><a name="l01533"></a><span class="lineno"> 1533</span>&#160;    @skipIfRocm</div><div class="line"><a name="l01534"></a><span class="lineno"> 1534</span>&#160;    <span class="keyword">def </span>test_ctc_loss(self):</div><div class="line"><a name="l01535"></a><span class="lineno"> 1535</span>&#160;        batch_size = 64</div><div class="line"><a name="l01536"></a><span class="lineno"> 1536</span>&#160;        num_labels = 101</div><div class="line"><a name="l01537"></a><span class="lineno"> 1537</span>&#160;        target_length = 15</div><div class="line"><a name="l01538"></a><span class="lineno"> 1538</span>&#160;        gradcheck_input_size = 10</div><div class="line"><a name="l01539"></a><span class="lineno"> 1539</span>&#160;</div><div class="line"><a name="l01540"></a><span class="lineno"> 1540</span>&#160;        <span class="comment"># device, input_length</span></div><div class="line"><a name="l01541"></a><span class="lineno"> 1541</span>&#160;        tests = [(<span class="stringliteral">&#39;cpu&#39;</span>, 150, <span class="keyword">False</span>),</div><div class="line"><a name="l01542"></a><span class="lineno"> 1542</span>&#160;                 (<span class="stringliteral">&#39;cpu&#39;</span>, 150, <span class="keyword">True</span>)]</div><div class="line"><a name="l01543"></a><span class="lineno"> 1543</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>():</div><div class="line"><a name="l01544"></a><span class="lineno"> 1544</span>&#160;            tests += [(<span class="stringliteral">&#39;cuda&#39;</span>, 50, <span class="keyword">False</span>),</div><div class="line"><a name="l01545"></a><span class="lineno"> 1545</span>&#160;                      (<span class="stringliteral">&#39;cuda&#39;</span>, 150, <span class="keyword">False</span>),</div><div class="line"><a name="l01546"></a><span class="lineno"> 1546</span>&#160;                      (<span class="stringliteral">&#39;cuda&#39;</span>, 50, <span class="keyword">True</span>),</div><div class="line"><a name="l01547"></a><span class="lineno"> 1547</span>&#160;                      (<span class="stringliteral">&#39;cuda&#39;</span>, 150, <span class="keyword">True</span>)]</div><div class="line"><a name="l01548"></a><span class="lineno"> 1548</span>&#160;</div><div class="line"><a name="l01549"></a><span class="lineno"> 1549</span>&#160;        <span class="keywordflow">for</span> device, input_length, vary_lengths <span class="keywordflow">in</span> tests:</div><div class="line"><a name="l01550"></a><span class="lineno"> 1550</span>&#160;            targets = torch.randint(1, num_labels, (batch_size, target_length),</div><div class="line"><a name="l01551"></a><span class="lineno"> 1551</span>&#160;                                    device=device, dtype=torch.long)</div><div class="line"><a name="l01552"></a><span class="lineno"> 1552</span>&#160;            x = torch.randn(gradcheck_input_size, device=device, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01553"></a><span class="lineno"> 1553</span>&#160;            tile_factors = torch.randn(input_length * batch_size * num_labels // gradcheck_input_size + 1,</div><div class="line"><a name="l01554"></a><span class="lineno"> 1554</span>&#160;                                       device=device)</div><div class="line"><a name="l01555"></a><span class="lineno"> 1555</span>&#160;            input_lengths = [(torch.randint(input_length // 2, input_length + 1, ()).item()</div><div class="line"><a name="l01556"></a><span class="lineno"> 1556</span>&#160;                              <span class="keywordflow">if</span> vary_lengths <span class="keywordflow">or</span> i == 0 <span class="keywordflow">else</span> input_length) <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(batch_size)]</div><div class="line"><a name="l01557"></a><span class="lineno"> 1557</span>&#160;            target_lengths = [(torch.randint(target_length // 2, target_length + 1, ()).item()</div><div class="line"><a name="l01558"></a><span class="lineno"> 1558</span>&#160;                               <span class="keywordflow">if</span> vary_lengths <span class="keywordflow">or</span> i == 0 <span class="keywordflow">else</span> target_length) <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(batch_size)]</div><div class="line"><a name="l01559"></a><span class="lineno"> 1559</span>&#160;</div><div class="line"><a name="l01560"></a><span class="lineno"> 1560</span>&#160;            <span class="keyword">def </span>ctc_after_softmax(x):</div><div class="line"><a name="l01561"></a><span class="lineno"> 1561</span>&#160;                x_full = ((x[:, <span class="keywordtype">None</span>] * tile_factors[<span class="keywordtype">None</span>, :]).view(-1)[:input_length * batch_size * num_labels]</div><div class="line"><a name="l01562"></a><span class="lineno"> 1562</span>&#160;                          .view(input_length, batch_size, num_labels))</div><div class="line"><a name="l01563"></a><span class="lineno"> 1563</span>&#160;                log_probs = torch.log_softmax(x_full, 2)</div><div class="line"><a name="l01564"></a><span class="lineno"> 1564</span>&#160;                <span class="keywordflow">return</span> <a class="code" href="torch_2nn_2functional_8py.html#af6226b2d76538bc3946825155e260ef9">torch.nn.functional.ctc_loss</a>(log_probs, targets, input_lengths, target_lengths)</div><div class="line"><a name="l01565"></a><span class="lineno"> 1565</span>&#160;</div><div class="line"><a name="l01566"></a><span class="lineno"> 1566</span>&#160;            gradcheck(ctc_after_softmax, [x])</div><div class="line"><a name="l01567"></a><span class="lineno"> 1567</span>&#160;</div><div class="line"><a name="l01568"></a><span class="lineno"> 1568</span>&#160;    <span class="keyword">def </span>_test_sparse_gather(self, size_x, size_ind, dim):</div><div class="line"><a name="l01569"></a><span class="lineno"> 1569</span>&#160;        x = torch.randn(size_x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01570"></a><span class="lineno"> 1570</span>&#160;        <span class="keywordflow">if</span> len(size_ind) &gt; 0 <span class="keywordflow">and</span> len(size_x) &gt; 0:</div><div class="line"><a name="l01571"></a><span class="lineno"> 1571</span>&#160;            ind = torch.randint(x.size(dim), size_ind)</div><div class="line"><a name="l01572"></a><span class="lineno"> 1572</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l01573"></a><span class="lineno"> 1573</span>&#160;            ind = torch.zeros(size_ind, dtype=torch.int64)</div><div class="line"><a name="l01574"></a><span class="lineno"> 1574</span>&#160;        out = torch.gather(x, dim, ind, sparse_grad=<span class="keyword">False</span>)</div><div class="line"><a name="l01575"></a><span class="lineno"> 1575</span>&#160;        grad = torch.rand_like(out)</div><div class="line"><a name="l01576"></a><span class="lineno"> 1576</span>&#160;        out.backward(grad)</div><div class="line"><a name="l01577"></a><span class="lineno"> 1577</span>&#160;        grad_dense = x.grad.clone()</div><div class="line"><a name="l01578"></a><span class="lineno"> 1578</span>&#160;        x.grad = <span class="keywordtype">None</span></div><div class="line"><a name="l01579"></a><span class="lineno"> 1579</span>&#160;        out = torch.gather(x, dim, ind, sparse_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01580"></a><span class="lineno"> 1580</span>&#160;        out.backward(grad)</div><div class="line"><a name="l01581"></a><span class="lineno"> 1581</span>&#160;        self.assertEqual(grad_dense, x.grad.to_dense())</div><div class="line"><a name="l01582"></a><span class="lineno"> 1582</span>&#160;</div><div class="line"><a name="l01583"></a><span class="lineno"> 1583</span>&#160;    <span class="keyword">def </span>test_sparse_gather_dim0(self):</div><div class="line"><a name="l01584"></a><span class="lineno"> 1584</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#add0f247ddbbb614bed6116f90e84c7c5">_test_sparse_gather</a>((10, 10), (5, 10), 0)</div><div class="line"><a name="l01585"></a><span class="lineno"> 1585</span>&#160;</div><div class="line"><a name="l01586"></a><span class="lineno"> 1586</span>&#160;    <span class="keyword">def </span>test_sparse_gather_dim1(self):</div><div class="line"><a name="l01587"></a><span class="lineno"> 1587</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#add0f247ddbbb614bed6116f90e84c7c5">_test_sparse_gather</a>((10, 10, 5), (10, 5, 5), 1)</div><div class="line"><a name="l01588"></a><span class="lineno"> 1588</span>&#160;</div><div class="line"><a name="l01589"></a><span class="lineno"> 1589</span>&#160;    <span class="keyword">def </span>test_sparse_gather_dim_neg(self):</div><div class="line"><a name="l01590"></a><span class="lineno"> 1590</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#add0f247ddbbb614bed6116f90e84c7c5">_test_sparse_gather</a>((10, 10, 5), (10, 10, 2), -1)</div><div class="line"><a name="l01591"></a><span class="lineno"> 1591</span>&#160;</div><div class="line"><a name="l01592"></a><span class="lineno"> 1592</span>&#160;    <span class="keyword">def </span>test_sparse_gather_ind_scalar(self):</div><div class="line"><a name="l01593"></a><span class="lineno"> 1593</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#add0f247ddbbb614bed6116f90e84c7c5">_test_sparse_gather</a>((10,), (), 0)</div><div class="line"><a name="l01594"></a><span class="lineno"> 1594</span>&#160;</div><div class="line"><a name="l01595"></a><span class="lineno"> 1595</span>&#160;    <span class="keyword">def </span>test_sparse_gather_x_scalar(self):</div><div class="line"><a name="l01596"></a><span class="lineno"> 1596</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#add0f247ddbbb614bed6116f90e84c7c5">_test_sparse_gather</a>((), (2,), 0)</div><div class="line"><a name="l01597"></a><span class="lineno"> 1597</span>&#160;</div><div class="line"><a name="l01598"></a><span class="lineno"> 1598</span>&#160;    <span class="keyword">def </span>test_sparse_gather_both_scalar(self):</div><div class="line"><a name="l01599"></a><span class="lineno"> 1599</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#add0f247ddbbb614bed6116f90e84c7c5">_test_sparse_gather</a>((), (), 0)</div><div class="line"><a name="l01600"></a><span class="lineno"> 1600</span>&#160;</div><div class="line"><a name="l01601"></a><span class="lineno"><a class="line" href="classtest__autograd_1_1_test_autograd.html#adb8918803c42299b58ef47d07a9f33e5"> 1601</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classtest__autograd_1_1_test_autograd.html#adb8918803c42299b58ef47d07a9f33e5">test_gc_in_destructor</a>(self):</div><div class="line"><a name="l01602"></a><span class="lineno"> 1602</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l01603"></a><span class="lineno"> 1603</span>&#160;<span class="stringliteral">        Previously, if a Function destructor triggered a garbage collection,</span></div><div class="line"><a name="l01604"></a><span class="lineno"> 1604</span>&#160;<span class="stringliteral">        the Variable&#39;s tp_dealloc handler would get called twice leading to a</span></div><div class="line"><a name="l01605"></a><span class="lineno"> 1605</span>&#160;<span class="stringliteral">        segfault.</span></div><div class="line"><a name="l01606"></a><span class="lineno"> 1606</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l01607"></a><span class="lineno"> 1607</span>&#160;        <span class="keyword">class </span>CollectOnDelete(Function):</div><div class="line"><a name="l01608"></a><span class="lineno"> 1608</span>&#160;</div><div class="line"><a name="l01609"></a><span class="lineno"> 1609</span>&#160;            <span class="keyword">def </span>__del__(self):</div><div class="line"><a name="l01610"></a><span class="lineno"> 1610</span>&#160;                gc.collect()</div><div class="line"><a name="l01611"></a><span class="lineno"> 1611</span>&#160;</div><div class="line"><a name="l01612"></a><span class="lineno"> 1612</span>&#160;        <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(10):</div><div class="line"><a name="l01613"></a><span class="lineno"> 1613</span>&#160;            Variable(torch.randn(10, 10), _grad_fn=CollectOnDelete())</div><div class="line"><a name="l01614"></a><span class="lineno"> 1614</span>&#160;</div><div class="line"><a name="l01615"></a><span class="lineno"> 1615</span>&#160;    @unittest.skipIf(<a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>() &lt; 2, <span class="stringliteral">&quot;no multi-GPU&quot;</span>)</div><div class="line"><a name="l01616"></a><span class="lineno"> 1616</span>&#160;    @skipIfRocm</div><div class="line"><a name="l01617"></a><span class="lineno"> 1617</span>&#160;    <span class="keyword">def </span>test_unused_output_gpu(self):</div><div class="line"><a name="l01618"></a><span class="lineno"> 1618</span>&#160;        <span class="keyword">from</span> <a class="code" href="namespacetorch_1_1nn_1_1parallel_1_1__functions.html">torch.nn.parallel._functions</a> <span class="keyword">import</span> Broadcast</div><div class="line"><a name="l01619"></a><span class="lineno"> 1619</span>&#160;        x = Variable(torch.randn(5, 5).float().cuda(), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01620"></a><span class="lineno"> 1620</span>&#160;        outputs = Broadcast.apply(list(range(<a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>())), x)</div><div class="line"><a name="l01621"></a><span class="lineno"> 1621</span>&#160;        y = outputs[-1] * 2</div><div class="line"><a name="l01622"></a><span class="lineno"> 1622</span>&#160;        y.sum().backward()</div><div class="line"><a name="l01623"></a><span class="lineno"> 1623</span>&#160;        self.assertEqual(x.grad.data, torch.ones(5, 5) * 2)</div><div class="line"><a name="l01624"></a><span class="lineno"> 1624</span>&#160;</div><div class="line"><a name="l01625"></a><span class="lineno"> 1625</span>&#160;    @unittest.skipIf(<a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>() &lt; 2, <span class="stringliteral">&quot;no multi-GPU&quot;</span>)</div><div class="line"><a name="l01626"></a><span class="lineno"> 1626</span>&#160;    <span class="keyword">def </span>test_backward_device(self):</div><div class="line"><a name="l01627"></a><span class="lineno"> 1627</span>&#160;        <span class="comment"># check that current device matches the variable&#39;s device</span></div><div class="line"><a name="l01628"></a><span class="lineno"> 1628</span>&#160;        device = [<span class="keywordtype">None</span>]</div><div class="line"><a name="l01629"></a><span class="lineno"> 1629</span>&#160;</div><div class="line"><a name="l01630"></a><span class="lineno"> 1630</span>&#160;        <span class="keyword">class </span>Identity(torch.autograd.Function):</div><div class="line"><a name="l01631"></a><span class="lineno"> 1631</span>&#160;            @staticmethod</div><div class="line"><a name="l01632"></a><span class="lineno"> 1632</span>&#160;            <span class="keyword">def </span>forward(ctx, x):</div><div class="line"><a name="l01633"></a><span class="lineno"> 1633</span>&#160;                <span class="keywordflow">return</span> x.clone()</div><div class="line"><a name="l01634"></a><span class="lineno"> 1634</span>&#160;</div><div class="line"><a name="l01635"></a><span class="lineno"> 1635</span>&#160;            @staticmethod</div><div class="line"><a name="l01636"></a><span class="lineno"> 1636</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l01637"></a><span class="lineno"> 1637</span>&#160;                device[0] = <a class="code" href="torch_2cuda_2____init_____8py.html#a1d0466ead04bf87515421e836c858d74">torch.cuda.current_device</a>()</div><div class="line"><a name="l01638"></a><span class="lineno"> 1638</span>&#160;                <span class="keywordflow">return</span> grad_output.clone()</div><div class="line"><a name="l01639"></a><span class="lineno"> 1639</span>&#160;</div><div class="line"><a name="l01640"></a><span class="lineno"> 1640</span>&#160;        v = Variable(torch.randn(1).cuda(1), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01641"></a><span class="lineno"> 1641</span>&#160;        Identity.apply(v).backward()</div><div class="line"><a name="l01642"></a><span class="lineno"> 1642</span>&#160;        self.assertEqual(device[0], 1)</div><div class="line"><a name="l01643"></a><span class="lineno"> 1643</span>&#160;</div><div class="line"><a name="l01644"></a><span class="lineno"> 1644</span>&#160;    @unittest.skipIf(<a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>() &lt; 2, <span class="stringliteral">&quot;no multi-GPU&quot;</span>)</div><div class="line"><a name="l01645"></a><span class="lineno"> 1645</span>&#160;    @skipIfRocm</div><div class="line"><a name="l01646"></a><span class="lineno"> 1646</span>&#160;    <span class="keyword">def </span>test_inputbuffer_add_multigpu(self):</div><div class="line"><a name="l01647"></a><span class="lineno"> 1647</span>&#160;        input = torch.randn(1).cuda(0).requires_grad_()</div><div class="line"><a name="l01648"></a><span class="lineno"> 1648</span>&#160;        output = input.cuda(1) + input.cuda(1)</div><div class="line"><a name="l01649"></a><span class="lineno"> 1649</span>&#160;        output.backward()</div><div class="line"><a name="l01650"></a><span class="lineno"> 1650</span>&#160;</div><div class="line"><a name="l01651"></a><span class="lineno"> 1651</span>&#160;    <span class="keyword">def </span>test_detach(self):</div><div class="line"><a name="l01652"></a><span class="lineno"> 1652</span>&#160;        x = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01653"></a><span class="lineno"> 1653</span>&#160;        y = x + 2</div><div class="line"><a name="l01654"></a><span class="lineno"> 1654</span>&#160;        y = y.detach()</div><div class="line"><a name="l01655"></a><span class="lineno"> 1655</span>&#160;        z = y * 4 + 2</div><div class="line"><a name="l01656"></a><span class="lineno"> 1656</span>&#160;        self.assertFalse(y.requires_grad)</div><div class="line"><a name="l01657"></a><span class="lineno"> 1657</span>&#160;        self.assertFalse(z.requires_grad)</div><div class="line"><a name="l01658"></a><span class="lineno"> 1658</span>&#160;</div><div class="line"><a name="l01659"></a><span class="lineno"> 1659</span>&#160;        x = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01660"></a><span class="lineno"> 1660</span>&#160;        y = x * 2</div><div class="line"><a name="l01661"></a><span class="lineno"> 1661</span>&#160;        y = y.detach()</div><div class="line"><a name="l01662"></a><span class="lineno"> 1662</span>&#160;        self.assertFalse(y.requires_grad)</div><div class="line"><a name="l01663"></a><span class="lineno"> 1663</span>&#160;        self.assertIsNone(y.grad_fn)</div><div class="line"><a name="l01664"></a><span class="lineno"> 1664</span>&#160;        z = x + y</div><div class="line"><a name="l01665"></a><span class="lineno"> 1665</span>&#160;        z.sum().backward()</div><div class="line"><a name="l01666"></a><span class="lineno"> 1666</span>&#160;        <span class="comment"># This is an incorrect gradient, but we assume that&#39;s what the user</span></div><div class="line"><a name="l01667"></a><span class="lineno"> 1667</span>&#160;        <span class="comment"># wanted. detach() is an advanced option.</span></div><div class="line"><a name="l01668"></a><span class="lineno"> 1668</span>&#160;        self.assertEqual(x.grad.data, torch.ones(10, 10))</div><div class="line"><a name="l01669"></a><span class="lineno"> 1669</span>&#160;</div><div class="line"><a name="l01670"></a><span class="lineno"> 1670</span>&#160;        <span class="comment"># in-place detach</span></div><div class="line"><a name="l01671"></a><span class="lineno"> 1671</span>&#160;        x = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01672"></a><span class="lineno"> 1672</span>&#160;        y = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01673"></a><span class="lineno"> 1673</span>&#160;        a = x * 2</div><div class="line"><a name="l01674"></a><span class="lineno"> 1674</span>&#160;        (y + a).sum().backward(retain_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l01675"></a><span class="lineno"> 1675</span>&#160;        a.detach_()</div><div class="line"><a name="l01676"></a><span class="lineno"> 1676</span>&#160;        self.assertFalse(a.requires_grad)</div><div class="line"><a name="l01677"></a><span class="lineno"> 1677</span>&#160;        (y + a).sum().backward()  <span class="comment"># this won&#39;t backprop to x</span></div><div class="line"><a name="l01678"></a><span class="lineno"> 1678</span>&#160;        self.assertEqual(x.grad.data, torch.ones(10, 10) * 2)</div><div class="line"><a name="l01679"></a><span class="lineno"> 1679</span>&#160;        self.assertEqual(y.grad.data, torch.ones(10, 10) * 2)</div><div class="line"><a name="l01680"></a><span class="lineno"> 1680</span>&#160;</div><div class="line"><a name="l01681"></a><span class="lineno"> 1681</span>&#160;        <span class="comment"># in-place deatch on a view raises an exception</span></div><div class="line"><a name="l01682"></a><span class="lineno"> 1682</span>&#160;        view = x.narrow(0, 1, 4)</div><div class="line"><a name="l01683"></a><span class="lineno"> 1683</span>&#160;        self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;view&#39;</span>, <span class="keyword">lambda</span>: view.detach_())</div><div class="line"><a name="l01684"></a><span class="lineno"> 1684</span>&#160;</div><div class="line"><a name="l01685"></a><span class="lineno"> 1685</span>&#160;    <span class="keyword">def </span>test_detach_base(self):</div><div class="line"><a name="l01686"></a><span class="lineno"> 1686</span>&#160;        <span class="stringliteral">&quot;detaching base does not detach view&quot;</span></div><div class="line"><a name="l01687"></a><span class="lineno"> 1687</span>&#160;        x = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01688"></a><span class="lineno"> 1688</span>&#160;        view = x.narrow(0, 1, 4)</div><div class="line"><a name="l01689"></a><span class="lineno"> 1689</span>&#160;        x.detach_()</div><div class="line"><a name="l01690"></a><span class="lineno"> 1690</span>&#160;        self.assertFalse(x.requires_grad)</div><div class="line"><a name="l01691"></a><span class="lineno"> 1691</span>&#160;        self.assertTrue(view.requires_grad)</div><div class="line"><a name="l01692"></a><span class="lineno"> 1692</span>&#160;        self.assertIsNotNone(view.grad_fn)</div><div class="line"><a name="l01693"></a><span class="lineno"> 1693</span>&#160;        self.assertIs(view._base, x)</div><div class="line"><a name="l01694"></a><span class="lineno"> 1694</span>&#160;</div><div class="line"><a name="l01695"></a><span class="lineno"> 1695</span>&#160;    <span class="keyword">def </span>_test_type_conversion_backward(self, t, ):</div><div class="line"><a name="l01696"></a><span class="lineno"> 1696</span>&#160;        fvar = Variable(t(torch.randn(5, 5).float()), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01697"></a><span class="lineno"> 1697</span>&#160;        fvar.double().sum().backward()</div><div class="line"><a name="l01698"></a><span class="lineno"> 1698</span>&#160;        self.assertEqual(fvar.grad, torch.ones_like(fvar))</div><div class="line"><a name="l01699"></a><span class="lineno"> 1699</span>&#160;        self.assertEqual(type(fvar.grad.data), type(fvar.data))</div><div class="line"><a name="l01700"></a><span class="lineno"> 1700</span>&#160;        dvar = Variable(t(torch.randn(5, 5).double()), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01701"></a><span class="lineno"> 1701</span>&#160;        dvar.float().sum().backward()</div><div class="line"><a name="l01702"></a><span class="lineno"> 1702</span>&#160;        self.assertEqual(dvar.grad, torch.ones_like(dvar))</div><div class="line"><a name="l01703"></a><span class="lineno"> 1703</span>&#160;        self.assertEqual(type(dvar.grad.data), type(dvar.data))</div><div class="line"><a name="l01704"></a><span class="lineno"> 1704</span>&#160;</div><div class="line"><a name="l01705"></a><span class="lineno"> 1705</span>&#160;    <span class="keyword">def </span>test_type_conversions(self):</div><div class="line"><a name="l01706"></a><span class="lineno"> 1706</span>&#160;        x = torch.randn(5, 5)</div><div class="line"><a name="l01707"></a><span class="lineno"> 1707</span>&#160;        self.assertIsInstance(x.float(), torch.FloatTensor)</div><div class="line"><a name="l01708"></a><span class="lineno"> 1708</span>&#160;        self.assertIsInstance(x.int(), torch.IntTensor)</div><div class="line"><a name="l01709"></a><span class="lineno"> 1709</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>():</div><div class="line"><a name="l01710"></a><span class="lineno"> 1710</span>&#160;            self.assertIsInstance(x.float().cuda(), torch.cuda.FloatTensor)</div><div class="line"><a name="l01711"></a><span class="lineno"> 1711</span>&#160;            self.assertIsInstance(x.int().cuda(), torch.cuda.IntTensor)</div><div class="line"><a name="l01712"></a><span class="lineno"> 1712</span>&#160;            self.assertIsInstance(x.int().cuda().cpu(), torch.IntTensor)</div><div class="line"><a name="l01713"></a><span class="lineno"> 1713</span>&#160;            <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>() &gt;= 2:</div><div class="line"><a name="l01714"></a><span class="lineno"> 1714</span>&#160;                x2 = x.float().cuda(1)</div><div class="line"><a name="l01715"></a><span class="lineno"> 1715</span>&#160;                self.assertIsInstance(x2, torch.cuda.FloatTensor)</div><div class="line"><a name="l01716"></a><span class="lineno"> 1716</span>&#160;                self.assertIs(x2.get_device(), 1)</div><div class="line"><a name="l01717"></a><span class="lineno"> 1717</span>&#160;                x2 = x.float().cuda()</div><div class="line"><a name="l01718"></a><span class="lineno"> 1718</span>&#160;                self.assertIsInstance(x2.data, torch.cuda.FloatTensor)</div><div class="line"><a name="l01719"></a><span class="lineno"> 1719</span>&#160;                self.assertIs(x2.get_device(), 0)</div><div class="line"><a name="l01720"></a><span class="lineno"> 1720</span>&#160;                x2 = x2.cuda(1)</div><div class="line"><a name="l01721"></a><span class="lineno"> 1721</span>&#160;                self.assertIsInstance(x2, torch.cuda.FloatTensor)</div><div class="line"><a name="l01722"></a><span class="lineno"> 1722</span>&#160;                self.assertIs(x2.get_device(), 1)</div><div class="line"><a name="l01723"></a><span class="lineno"> 1723</span>&#160;                y = Variable(torch.randn(5).cuda(1), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01724"></a><span class="lineno"> 1724</span>&#160;                y.cpu().sum().backward()</div><div class="line"><a name="l01725"></a><span class="lineno"> 1725</span>&#160;                self.assertIs(y.grad.get_device(), 1)</div><div class="line"><a name="l01726"></a><span class="lineno"> 1726</span>&#160;                self.assertIs(y.long().data.get_device(), 1)</div><div class="line"><a name="l01727"></a><span class="lineno"> 1727</span>&#160;</div><div class="line"><a name="l01728"></a><span class="lineno"> 1728</span>&#160;        <span class="keywordflow">for</span> t <span class="keywordflow">in</span> [torch.DoubleTensor, torch.FloatTensor, torch.IntTensor, torch.ByteTensor]:</div><div class="line"><a name="l01729"></a><span class="lineno"> 1729</span>&#160;            <span class="keywordflow">for</span> y_var <span class="keywordflow">in</span> (<span class="keyword">True</span>, <span class="keyword">False</span>):</div><div class="line"><a name="l01730"></a><span class="lineno"> 1730</span>&#160;                y = torch.randint(5, (5, 5), dtype=t.dtype)</div><div class="line"><a name="l01731"></a><span class="lineno"> 1731</span>&#160;                y = Variable(y) <span class="keywordflow">if</span> y_var <span class="keywordflow">else</span> y</div><div class="line"><a name="l01732"></a><span class="lineno"> 1732</span>&#160;                self.assertIsInstance(x.type(t), t)</div><div class="line"><a name="l01733"></a><span class="lineno"> 1733</span>&#160;                self.assertIsInstance(x.type_as(y), t)</div><div class="line"><a name="l01734"></a><span class="lineno"> 1734</span>&#160;                <span class="comment"># TODO: t.dtype should work</span></div><div class="line"><a name="l01735"></a><span class="lineno"> 1735</span>&#160;                t_dtype = t().dtype</div><div class="line"><a name="l01736"></a><span class="lineno"> 1736</span>&#160;                self.assertIsInstance(x.type(t_dtype), t)</div><div class="line"><a name="l01737"></a><span class="lineno"> 1737</span>&#160;                self.assertIs(t_dtype, x.type(t_dtype).dtype)</div><div class="line"><a name="l01738"></a><span class="lineno"> 1738</span>&#160;                self.assertEqual(y.data_ptr(), y.type(t).data_ptr())</div><div class="line"><a name="l01739"></a><span class="lineno"> 1739</span>&#160;                <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>():</div><div class="line"><a name="l01740"></a><span class="lineno"> 1740</span>&#160;                    <span class="keywordflow">for</span> x_cuda <span class="keywordflow">in</span> (<span class="keyword">True</span>, <span class="keyword">False</span>):</div><div class="line"><a name="l01741"></a><span class="lineno"> 1741</span>&#160;                        <span class="keywordflow">for</span> y_cuda <span class="keywordflow">in</span> (<span class="keyword">True</span>, <span class="keyword">False</span>):</div><div class="line"><a name="l01742"></a><span class="lineno"> 1742</span>&#160;                            x_c = x.cuda() <span class="keywordflow">if</span> x_cuda <span class="keywordflow">else</span> x</div><div class="line"><a name="l01743"></a><span class="lineno"> 1743</span>&#160;                            y_c = y.cuda() <span class="keywordflow">if</span> y_cuda <span class="keywordflow">else</span> y</div><div class="line"><a name="l01744"></a><span class="lineno"> 1744</span>&#160;                            _, y_type = y_c.type().rsplit(<span class="stringliteral">&#39;.&#39;</span>, 1)</div><div class="line"><a name="l01745"></a><span class="lineno"> 1745</span>&#160;                            y_typestr = (<span class="stringliteral">&#39;torch.cuda.&#39;</span> <span class="keywordflow">if</span> y_cuda <span class="keywordflow">else</span> <span class="stringliteral">&#39;torch.&#39;</span>) + y_type</div><div class="line"><a name="l01746"></a><span class="lineno"> 1746</span>&#160;                            self.assertEqual(y_c.type(), x_c.type(y_typestr).type())</div><div class="line"><a name="l01747"></a><span class="lineno"> 1747</span>&#160;                            self.assertIs(y_c.dtype, x_c.type(y_c.dtype).dtype)</div><div class="line"><a name="l01748"></a><span class="lineno"> 1748</span>&#160;                            self.assertEqual(y_c.data_ptr(), y_c.cuda().data_ptr() <span class="keywordflow">if</span> y_cuda <span class="keywordflow">else</span> y_c.data_ptr())</div><div class="line"><a name="l01749"></a><span class="lineno"> 1749</span>&#160;</div><div class="line"><a name="l01750"></a><span class="lineno"> 1750</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#ad46368b6d7f3b992d8e20d9d196bc7b7">_test_type_conversion_backward</a>(<span class="keyword">lambda</span> x: x)</div><div class="line"><a name="l01751"></a><span class="lineno"> 1751</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>():</div><div class="line"><a name="l01752"></a><span class="lineno"> 1752</span>&#160;            self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#ad46368b6d7f3b992d8e20d9d196bc7b7">_test_type_conversion_backward</a>(<span class="keyword">lambda</span> x: x.cuda())</div><div class="line"><a name="l01753"></a><span class="lineno"> 1753</span>&#160;            <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a>() &gt;= 2:</div><div class="line"><a name="l01754"></a><span class="lineno"> 1754</span>&#160;                <span class="comment"># one of these has to be the non-default device</span></div><div class="line"><a name="l01755"></a><span class="lineno"> 1755</span>&#160;                self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#ad46368b6d7f3b992d8e20d9d196bc7b7">_test_type_conversion_backward</a>(<span class="keyword">lambda</span> x: x.cuda(0))</div><div class="line"><a name="l01756"></a><span class="lineno"> 1756</span>&#160;                self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#ad46368b6d7f3b992d8e20d9d196bc7b7">_test_type_conversion_backward</a>(<span class="keyword">lambda</span> x: x.cuda(1))</div><div class="line"><a name="l01757"></a><span class="lineno"> 1757</span>&#160;</div><div class="line"><a name="l01758"></a><span class="lineno"> 1758</span>&#160;    <span class="keyword">def </span>_test_pyscalar_conversions(self, t, integral_conv):</div><div class="line"><a name="l01759"></a><span class="lineno"> 1759</span>&#160;        <span class="comment"># integral -&gt; integral</span></div><div class="line"><a name="l01760"></a><span class="lineno"> 1760</span>&#160;        l = t(torch.zeros(1, 1, 1, dtype=torch.long))</div><div class="line"><a name="l01761"></a><span class="lineno"> 1761</span>&#160;        pyscalar = -12345</div><div class="line"><a name="l01762"></a><span class="lineno"> 1762</span>&#160;        l[0] = pyscalar</div><div class="line"><a name="l01763"></a><span class="lineno"> 1763</span>&#160;        self.assertEqual(integral_conv(l), pyscalar)</div><div class="line"><a name="l01764"></a><span class="lineno"> 1764</span>&#160;</div><div class="line"><a name="l01765"></a><span class="lineno"> 1765</span>&#160;        <span class="comment"># floating point -&gt; floating point</span></div><div class="line"><a name="l01766"></a><span class="lineno"> 1766</span>&#160;        f = Variable(t(torch.randn(1, 1)))</div><div class="line"><a name="l01767"></a><span class="lineno"> 1767</span>&#160;        pyscalar = -12345.1</div><div class="line"><a name="l01768"></a><span class="lineno"> 1768</span>&#160;        f[0] = pyscalar</div><div class="line"><a name="l01769"></a><span class="lineno"> 1769</span>&#160;        self.assertEqual(float(f), pyscalar)</div><div class="line"><a name="l01770"></a><span class="lineno"> 1770</span>&#160;        f[0] = nan</div><div class="line"><a name="l01771"></a><span class="lineno"> 1771</span>&#160;        self.assertTrue(math.isnan(float(f)))</div><div class="line"><a name="l01772"></a><span class="lineno"> 1772</span>&#160;        f[0] = inf</div><div class="line"><a name="l01773"></a><span class="lineno"> 1773</span>&#160;        self.assertEqual(float(f), inf, allow_inf=<span class="keyword">True</span>)</div><div class="line"><a name="l01774"></a><span class="lineno"> 1774</span>&#160;        f[0] = -inf</div><div class="line"><a name="l01775"></a><span class="lineno"> 1775</span>&#160;        self.assertEqual(float(f), -inf, allow_inf=<span class="keyword">True</span>)</div><div class="line"><a name="l01776"></a><span class="lineno"> 1776</span>&#160;</div><div class="line"><a name="l01777"></a><span class="lineno"> 1777</span>&#160;        <span class="comment"># integral -&gt; floating point</span></div><div class="line"><a name="l01778"></a><span class="lineno"> 1778</span>&#160;        <span class="comment"># check we can convert something that loses precision</span></div><div class="line"><a name="l01779"></a><span class="lineno"> 1779</span>&#160;        pyscalar = 1234567890123456789</div><div class="line"><a name="l01780"></a><span class="lineno"> 1780</span>&#160;        self.assertNotEqual(pyscalar, integral_conv(float(pyscalar)))</div><div class="line"><a name="l01781"></a><span class="lineno"> 1781</span>&#160;        l[0] = pyscalar</div><div class="line"><a name="l01782"></a><span class="lineno"> 1782</span>&#160;        self.assertEqual(float(l), float(pyscalar))</div><div class="line"><a name="l01783"></a><span class="lineno"> 1783</span>&#160;</div><div class="line"><a name="l01784"></a><span class="lineno"> 1784</span>&#160;        <span class="comment"># floating point -&gt; integral</span></div><div class="line"><a name="l01785"></a><span class="lineno"> 1785</span>&#160;        f[0] = nan</div><div class="line"><a name="l01786"></a><span class="lineno"> 1786</span>&#160;        self.assertRaises(ValueError, <span class="keyword">lambda</span>: integral_conv(f[0]))</div><div class="line"><a name="l01787"></a><span class="lineno"> 1787</span>&#160;        f[0] = inf</div><div class="line"><a name="l01788"></a><span class="lineno"> 1788</span>&#160;        self.assertRaises(OverflowError, <span class="keyword">lambda</span>: integral_conv(f[0]))</div><div class="line"><a name="l01789"></a><span class="lineno"> 1789</span>&#160;        f[0] = -inf</div><div class="line"><a name="l01790"></a><span class="lineno"> 1790</span>&#160;        self.assertRaises(OverflowError, <span class="keyword">lambda</span>: integral_conv(f[0]))</div><div class="line"><a name="l01791"></a><span class="lineno"> 1791</span>&#160;        f[0] = sys.float_info.max</div><div class="line"><a name="l01792"></a><span class="lineno"> 1792</span>&#160;        self.assertEqual(integral_conv(f), sys.float_info.max)</div><div class="line"><a name="l01793"></a><span class="lineno"> 1793</span>&#160;</div><div class="line"><a name="l01794"></a><span class="lineno"> 1794</span>&#160;        <span class="comment"># bool, nonzero</span></div><div class="line"><a name="l01795"></a><span class="lineno"> 1795</span>&#160;        <span class="keyword">def </span>test_nonzero(tensor, value, expected):</div><div class="line"><a name="l01796"></a><span class="lineno"> 1796</span>&#160;            tensor[0] = value</div><div class="line"><a name="l01797"></a><span class="lineno"> 1797</span>&#160;            self.assertEqual(expected, bool(tensor))</div><div class="line"><a name="l01798"></a><span class="lineno"> 1798</span>&#160;            self.assertEqual(expected, <span class="keyword">True</span> <span class="keywordflow">if</span> tensor <span class="keywordflow">else</span> <span class="keyword">False</span>)</div><div class="line"><a name="l01799"></a><span class="lineno"> 1799</span>&#160;</div><div class="line"><a name="l01800"></a><span class="lineno"> 1800</span>&#160;        test_nonzero(l, 0, <span class="keyword">False</span>)</div><div class="line"><a name="l01801"></a><span class="lineno"> 1801</span>&#160;        test_nonzero(l, -2, <span class="keyword">True</span>)</div><div class="line"><a name="l01802"></a><span class="lineno"> 1802</span>&#160;        test_nonzero(f, 0.0, <span class="keyword">False</span>)</div><div class="line"><a name="l01803"></a><span class="lineno"> 1803</span>&#160;        test_nonzero(f, sys.float_info.min, <span class="keyword">True</span>)</div><div class="line"><a name="l01804"></a><span class="lineno"> 1804</span>&#160;        test_nonzero(f, nan, bool(nan))</div><div class="line"><a name="l01805"></a><span class="lineno"> 1805</span>&#160;        test_nonzero(f, inf, bool(inf))</div><div class="line"><a name="l01806"></a><span class="lineno"> 1806</span>&#160;        test_nonzero(f, -inf, bool(-inf))</div><div class="line"><a name="l01807"></a><span class="lineno"> 1807</span>&#160;</div><div class="line"><a name="l01808"></a><span class="lineno"> 1808</span>&#160;    <span class="keyword">def </span>test_pyscalar_conversions(self):</div><div class="line"><a name="l01809"></a><span class="lineno"> 1809</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a6b73c08752701bcfd98c59de3ce91550">_test_pyscalar_conversions</a>(<span class="keyword">lambda</span> x: x, <span class="keyword">lambda</span> x: int(x))</div><div class="line"><a name="l01810"></a><span class="lineno"> 1810</span>&#160;        <span class="keywordflow">if</span> sys.version_info[0] == 2:</div><div class="line"><a name="l01811"></a><span class="lineno"> 1811</span>&#160;            self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a6b73c08752701bcfd98c59de3ce91550">_test_pyscalar_conversions</a>(<span class="keyword">lambda</span> x: x, <span class="keyword">lambda</span> x: long(x))</div><div class="line"><a name="l01812"></a><span class="lineno"> 1812</span>&#160;        <span class="keywordflow">if</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>():</div><div class="line"><a name="l01813"></a><span class="lineno"> 1813</span>&#160;            self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a6b73c08752701bcfd98c59de3ce91550">_test_pyscalar_conversions</a>(<span class="keyword">lambda</span> x: x.cuda(), <span class="keyword">lambda</span> x: int(x))</div><div class="line"><a name="l01814"></a><span class="lineno"> 1814</span>&#160;            <span class="keywordflow">if</span> sys.version_info[0] == 2:</div><div class="line"><a name="l01815"></a><span class="lineno"> 1815</span>&#160;                self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a6b73c08752701bcfd98c59de3ce91550">_test_pyscalar_conversions</a>(<span class="keyword">lambda</span> x: x.cuda(), <span class="keyword">lambda</span> x: long(x))</div><div class="line"><a name="l01816"></a><span class="lineno"> 1816</span>&#160;</div><div class="line"><a name="l01817"></a><span class="lineno"> 1817</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>(), <span class="stringliteral">&quot;CUDA unavailable&quot;</span>)</div><div class="line"><a name="l01818"></a><span class="lineno"> 1818</span>&#160;    <span class="keyword">def </span>test_pin_memory(self):</div><div class="line"><a name="l01819"></a><span class="lineno"> 1819</span>&#160;        x = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01820"></a><span class="lineno"> 1820</span>&#160;        self.assertEqual(x, x.pin_memory())</div><div class="line"><a name="l01821"></a><span class="lineno"> 1821</span>&#160;        self.assertIsNot(x, x.pin_memory())</div><div class="line"><a name="l01822"></a><span class="lineno"> 1822</span>&#160;        self.assertTrue(x.pin_memory().requires_grad)</div><div class="line"><a name="l01823"></a><span class="lineno"> 1823</span>&#160;        gradcheck(<span class="keyword">lambda</span> x: x.pin_memory(), [x])</div><div class="line"><a name="l01824"></a><span class="lineno"> 1824</span>&#160;        gradgradcheck(<span class="keyword">lambda</span> x: x.pin_memory(), [x])</div><div class="line"><a name="l01825"></a><span class="lineno"> 1825</span>&#160;</div><div class="line"><a name="l01826"></a><span class="lineno"> 1826</span>&#160;    <span class="keyword">def </span>test_isolated_node(self):</div><div class="line"><a name="l01827"></a><span class="lineno"> 1827</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01828"></a><span class="lineno"> 1828</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01829"></a><span class="lineno"> 1829</span>&#160;</div><div class="line"><a name="l01830"></a><span class="lineno"> 1830</span>&#160;        a = x + y</div><div class="line"><a name="l01831"></a><span class="lineno"> 1831</span>&#160;        b = torch.max(a, 1, <span class="keyword">True</span>)[1].repeat(1, 5).double()</div><div class="line"><a name="l01832"></a><span class="lineno"> 1832</span>&#160;        o = (b + a).sum()</div><div class="line"><a name="l01833"></a><span class="lineno"> 1833</span>&#160;        o.backward()</div><div class="line"><a name="l01834"></a><span class="lineno"> 1834</span>&#160;</div><div class="line"><a name="l01835"></a><span class="lineno"> 1835</span>&#160;    <span class="keyword">def </span>test_shape(self):</div><div class="line"><a name="l01836"></a><span class="lineno"> 1836</span>&#160;        x = torch.randn(3, 4)</div><div class="line"><a name="l01837"></a><span class="lineno"> 1837</span>&#160;        self.assertEqual(2, len(x.shape))</div><div class="line"><a name="l01838"></a><span class="lineno"> 1838</span>&#160;        self.assertEqual(x.shape[0], 3)</div><div class="line"><a name="l01839"></a><span class="lineno"> 1839</span>&#160;        self.assertEqual(x.shape[1], 4)</div><div class="line"><a name="l01840"></a><span class="lineno"> 1840</span>&#160;</div><div class="line"><a name="l01841"></a><span class="lineno"> 1841</span>&#160;    <span class="keyword">def </span>test_numpy_requires_grad(self):</div><div class="line"><a name="l01842"></a><span class="lineno"> 1842</span>&#160;        x = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01843"></a><span class="lineno"> 1843</span>&#160;        self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;requires grad&#39;</span>, <span class="keyword">lambda</span>: x.numpy())</div><div class="line"><a name="l01844"></a><span class="lineno"> 1844</span>&#160;</div><div class="line"><a name="l01845"></a><span class="lineno"> 1845</span>&#160;    <span class="keyword">def </span>test_return_leaf(self):</div><div class="line"><a name="l01846"></a><span class="lineno"> 1846</span>&#160;        <span class="keyword">class </span>Identity(Function):</div><div class="line"><a name="l01847"></a><span class="lineno"> 1847</span>&#160;</div><div class="line"><a name="l01848"></a><span class="lineno"> 1848</span>&#160;            <span class="keyword">def </span>forward(self, a, b):</div><div class="line"><a name="l01849"></a><span class="lineno"> 1849</span>&#160;                <span class="keywordflow">return</span> a, a + b</div><div class="line"><a name="l01850"></a><span class="lineno"> 1850</span>&#160;</div><div class="line"><a name="l01851"></a><span class="lineno"> 1851</span>&#160;            <span class="keyword">def </span>backward(self, grad_a, grad_b):</div><div class="line"><a name="l01852"></a><span class="lineno"> 1852</span>&#160;                <span class="keywordflow">return</span> grad_a + grad_b, grad_b</div><div class="line"><a name="l01853"></a><span class="lineno"> 1853</span>&#160;</div><div class="line"><a name="l01854"></a><span class="lineno"> 1854</span>&#160;        hook_called = [<span class="keyword">False</span>]</div><div class="line"><a name="l01855"></a><span class="lineno"> 1855</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01856"></a><span class="lineno"> 1856</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01857"></a><span class="lineno"> 1857</span>&#160;</div><div class="line"><a name="l01858"></a><span class="lineno"> 1858</span>&#160;        q, p = Identity()(x, y)</div><div class="line"><a name="l01859"></a><span class="lineno"> 1859</span>&#160;</div><div class="line"><a name="l01860"></a><span class="lineno"> 1860</span>&#160;        <span class="comment"># Make sure hooks only receive grad from usage of q, not x.</span></div><div class="line"><a name="l01861"></a><span class="lineno"> 1861</span>&#160;        <span class="keyword">def </span>hook(grad):</div><div class="line"><a name="l01862"></a><span class="lineno"> 1862</span>&#160;            hook_called[0] = <span class="keyword">True</span></div><div class="line"><a name="l01863"></a><span class="lineno"> 1863</span>&#160;            self.assertEqual(grad.data, torch.ones(5, 5))</div><div class="line"><a name="l01864"></a><span class="lineno"> 1864</span>&#160;</div><div class="line"><a name="l01865"></a><span class="lineno"> 1865</span>&#160;        q.register_hook(hook)</div><div class="line"><a name="l01866"></a><span class="lineno"> 1866</span>&#160;        (q + p + x).sum().backward()</div><div class="line"><a name="l01867"></a><span class="lineno"> 1867</span>&#160;        self.assertEqual(x.grad.data, torch.ones(5, 5) * 3)</div><div class="line"><a name="l01868"></a><span class="lineno"> 1868</span>&#160;        self.assertEqual(y.grad.data, torch.ones(5, 5))</div><div class="line"><a name="l01869"></a><span class="lineno"> 1869</span>&#160;        self.assertTrue(hook_called[0])</div><div class="line"><a name="l01870"></a><span class="lineno"> 1870</span>&#160;</div><div class="line"><a name="l01871"></a><span class="lineno"> 1871</span>&#160;    <span class="keyword">def </span>test_return_leaf_inplace(self):</div><div class="line"><a name="l01872"></a><span class="lineno"> 1872</span>&#160;        <span class="keyword">class </span>Inplace(<a class="code" href="classtorch_1_1autograd_1_1function_1_1_inplace_function.html">InplaceFunction</a>):</div><div class="line"><a name="l01873"></a><span class="lineno"> 1873</span>&#160;</div><div class="line"><a name="l01874"></a><span class="lineno"> 1874</span>&#160;            <span class="keyword">def </span>forward(self, a, b):</div><div class="line"><a name="l01875"></a><span class="lineno"> 1875</span>&#160;                self.mark_dirty(a)</div><div class="line"><a name="l01876"></a><span class="lineno"> 1876</span>&#160;                <span class="keywordflow">return</span> a.add_(b), b + 2</div><div class="line"><a name="l01877"></a><span class="lineno"> 1877</span>&#160;</div><div class="line"><a name="l01878"></a><span class="lineno"> 1878</span>&#160;            <span class="keyword">def </span>backward(self, grad_a, grad_b):</div><div class="line"><a name="l01879"></a><span class="lineno"> 1879</span>&#160;                <span class="keywordflow">return</span> grad_a, grad_a + grad_b</div><div class="line"><a name="l01880"></a><span class="lineno"> 1880</span>&#160;</div><div class="line"><a name="l01881"></a><span class="lineno"> 1881</span>&#160;        x = torch.randn(5, 5)</div><div class="line"><a name="l01882"></a><span class="lineno"> 1882</span>&#160;        y = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01883"></a><span class="lineno"> 1883</span>&#160;</div><div class="line"><a name="l01884"></a><span class="lineno"> 1884</span>&#160;        fn = Inplace(<span class="keyword">True</span>)</div><div class="line"><a name="l01885"></a><span class="lineno"> 1885</span>&#160;        q, p = fn(x, y)</div><div class="line"><a name="l01886"></a><span class="lineno"> 1886</span>&#160;        self.assertIs(q, x)</div><div class="line"><a name="l01887"></a><span class="lineno"> 1887</span>&#160;        self.assertIs(q.grad_fn, fn)</div><div class="line"><a name="l01888"></a><span class="lineno"> 1888</span>&#160;        self.assertTrue(q.requires_grad)</div><div class="line"><a name="l01889"></a><span class="lineno"> 1889</span>&#160;        q.sum().backward()</div><div class="line"><a name="l01890"></a><span class="lineno"> 1890</span>&#160;        self.assertEqual(y.grad.data, torch.ones(5, 5))</div><div class="line"><a name="l01891"></a><span class="lineno"> 1891</span>&#160;</div><div class="line"><a name="l01892"></a><span class="lineno"> 1892</span>&#160;    <span class="keyword">def </span>test_leaf_assignment(self):</div><div class="line"><a name="l01893"></a><span class="lineno"> 1893</span>&#160;        x = torch.randn(5, 5)</div><div class="line"><a name="l01894"></a><span class="lineno"> 1894</span>&#160;        y = torch.randn(5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01895"></a><span class="lineno"> 1895</span>&#160;        z = torch.randn(5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01896"></a><span class="lineno"> 1896</span>&#160;</div><div class="line"><a name="l01897"></a><span class="lineno"> 1897</span>&#160;        x[0] = y</div><div class="line"><a name="l01898"></a><span class="lineno"> 1898</span>&#160;        x[1] = 2 * z</div><div class="line"><a name="l01899"></a><span class="lineno"> 1899</span>&#160;        self.assertTrue(x.requires_grad)</div><div class="line"><a name="l01900"></a><span class="lineno"> 1900</span>&#160;        self.assertIsNot(x.grad_fn, <span class="keywordtype">None</span>)</div><div class="line"><a name="l01901"></a><span class="lineno"> 1901</span>&#160;        x.sum().backward()</div><div class="line"><a name="l01902"></a><span class="lineno"> 1902</span>&#160;        self.assertEqual(y.grad.data, torch.ones(5))</div><div class="line"><a name="l01903"></a><span class="lineno"> 1903</span>&#160;        self.assertEqual(z.grad.data, torch.ones(5) * 2)</div><div class="line"><a name="l01904"></a><span class="lineno"> 1904</span>&#160;</div><div class="line"><a name="l01905"></a><span class="lineno"> 1905</span>&#160;    <span class="keyword">def </span>test_no_grad_assignment(self):</div><div class="line"><a name="l01906"></a><span class="lineno"> 1906</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01907"></a><span class="lineno"> 1907</span>&#160;        y = torch.randn(5)</div><div class="line"><a name="l01908"></a><span class="lineno"> 1908</span>&#160;        with torch.no_grad():</div><div class="line"><a name="l01909"></a><span class="lineno"> 1909</span>&#160;            x[0] = y</div><div class="line"><a name="l01910"></a><span class="lineno"> 1910</span>&#160;</div><div class="line"><a name="l01911"></a><span class="lineno"> 1911</span>&#160;        self.assertTrue(x.requires_grad)</div><div class="line"><a name="l01912"></a><span class="lineno"> 1912</span>&#160;        self.assertIsNone(x.grad_fn)</div><div class="line"><a name="l01913"></a><span class="lineno"> 1913</span>&#160;</div><div class="line"><a name="l01914"></a><span class="lineno"> 1914</span>&#160;    <span class="keyword">def </span>test_no_grad_modifies_version(self):</div><div class="line"><a name="l01915"></a><span class="lineno"> 1915</span>&#160;        x = torch.randn(5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01916"></a><span class="lineno"> 1916</span>&#160;        y = torch.randn(5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01917"></a><span class="lineno"> 1917</span>&#160;        z = (x * y).sum()</div><div class="line"><a name="l01918"></a><span class="lineno"> 1918</span>&#160;        with torch.no_grad():</div><div class="line"><a name="l01919"></a><span class="lineno"> 1919</span>&#160;            x *= 2</div><div class="line"><a name="l01920"></a><span class="lineno"> 1920</span>&#160;        self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;modified by an inplace operation&#39;</span>,</div><div class="line"><a name="l01921"></a><span class="lineno"> 1921</span>&#160;                               <span class="keyword">lambda</span>: z.backward())</div><div class="line"><a name="l01922"></a><span class="lineno"> 1922</span>&#160;</div><div class="line"><a name="l01923"></a><span class="lineno"> 1923</span>&#160;    <span class="keyword">def </span>test_no_grad_input(self):</div><div class="line"><a name="l01924"></a><span class="lineno"> 1924</span>&#160;        <span class="keyword">class </span>MyFunction(Function):</div><div class="line"><a name="l01925"></a><span class="lineno"> 1925</span>&#160;            @staticmethod</div><div class="line"><a name="l01926"></a><span class="lineno"> 1926</span>&#160;            <span class="keyword">def </span>forward(self, x):</div><div class="line"><a name="l01927"></a><span class="lineno"> 1927</span>&#160;                <span class="keywordflow">return</span> x</div><div class="line"><a name="l01928"></a><span class="lineno"> 1928</span>&#160;</div><div class="line"><a name="l01929"></a><span class="lineno"> 1929</span>&#160;            @staticmethod</div><div class="line"><a name="l01930"></a><span class="lineno"> 1930</span>&#160;            <span class="keyword">def </span>backward(self, grad_output):</div><div class="line"><a name="l01931"></a><span class="lineno"> 1931</span>&#160;                <span class="keywordflow">return</span> grad_output</div><div class="line"><a name="l01932"></a><span class="lineno"> 1932</span>&#160;</div><div class="line"><a name="l01933"></a><span class="lineno"> 1933</span>&#160;        x = torch.randn(5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01934"></a><span class="lineno"> 1934</span>&#160;        with torch.no_grad():</div><div class="line"><a name="l01935"></a><span class="lineno"> 1935</span>&#160;            y = MyFunction.apply(x)</div><div class="line"><a name="l01936"></a><span class="lineno"> 1936</span>&#160;</div><div class="line"><a name="l01937"></a><span class="lineno"> 1937</span>&#160;        self.assertTrue(x.requires_grad)</div><div class="line"><a name="l01938"></a><span class="lineno"> 1938</span>&#160;        self.assertIsNone(y.grad_fn)</div><div class="line"><a name="l01939"></a><span class="lineno"> 1939</span>&#160;</div><div class="line"><a name="l01940"></a><span class="lineno"> 1940</span>&#160;    <span class="keyword">def </span>test_backward_copy(self):</div><div class="line"><a name="l01941"></a><span class="lineno"> 1941</span>&#160;        <span class="comment"># This tests checks backward engine for a very subtle bug that appreared</span></div><div class="line"><a name="l01942"></a><span class="lineno"> 1942</span>&#160;        <span class="comment"># in one of the initial versions of autograd. Gradients tensors were</span></div><div class="line"><a name="l01943"></a><span class="lineno"> 1943</span>&#160;        <span class="comment"># simply stored in lists while the function waited for all its gradients</span></div><div class="line"><a name="l01944"></a><span class="lineno"> 1944</span>&#160;        <span class="comment"># to be computed. However, sometimes an output was used multiple times,</span></div><div class="line"><a name="l01945"></a><span class="lineno"> 1945</span>&#160;        <span class="comment"># so the gradients needed to be summed. Engine used to keep a need_copy</span></div><div class="line"><a name="l01946"></a><span class="lineno"> 1946</span>&#160;        <span class="comment"># set of tensors that will need a clone upon next addition and removed</span></div><div class="line"><a name="l01947"></a><span class="lineno"> 1947</span>&#160;        <span class="comment"># them from the set as soon as the clone was performed. However, this</span></div><div class="line"><a name="l01948"></a><span class="lineno"> 1948</span>&#160;        <span class="comment"># could lead to incorrect results if the same gradient tensor was</span></div><div class="line"><a name="l01949"></a><span class="lineno"> 1949</span>&#160;        <span class="comment"># buffered in three places in the graph:</span></div><div class="line"><a name="l01950"></a><span class="lineno"> 1950</span>&#160;        <span class="comment"># 1. When accumulating gradients in one of these places it was cloned</span></div><div class="line"><a name="l01951"></a><span class="lineno"> 1951</span>&#160;        <span class="comment">#    and removed from need_copy set.</span></div><div class="line"><a name="l01952"></a><span class="lineno"> 1952</span>&#160;        <span class="comment"># 2. When accumulating in second place, it wasn&#39;t in the need_copy set,</span></div><div class="line"><a name="l01953"></a><span class="lineno"> 1953</span>&#160;        <span class="comment">#    so the gradients were simply accumulated in-place (which already</span></div><div class="line"><a name="l01954"></a><span class="lineno"> 1954</span>&#160;        <span class="comment">#    modified the grad in 3rd place)</span></div><div class="line"><a name="l01955"></a><span class="lineno"> 1955</span>&#160;        <span class="comment"># 3. When accumulating in the third place, it wasn&#39;t in the need_copy set</span></div><div class="line"><a name="l01956"></a><span class="lineno"> 1956</span>&#160;        <span class="comment">#    as well, so the incoming gradient was summed in-place, yielding</span></div><div class="line"><a name="l01957"></a><span class="lineno"> 1957</span>&#160;        <span class="comment">#    incorrect results in all functions, except the first one.</span></div><div class="line"><a name="l01958"></a><span class="lineno"> 1958</span>&#160;        x = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01959"></a><span class="lineno"> 1959</span>&#160;        y = torch.ones(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01960"></a><span class="lineno"> 1960</span>&#160;        <span class="comment"># Simulate that we&#39;re in the middle of the graph</span></div><div class="line"><a name="l01961"></a><span class="lineno"> 1961</span>&#160;        a = x + 2</div><div class="line"><a name="l01962"></a><span class="lineno"> 1962</span>&#160;        b = y + 2</div><div class="line"><a name="l01963"></a><span class="lineno"> 1963</span>&#160;        c = x + 2</div><div class="line"><a name="l01964"></a><span class="lineno"> 1964</span>&#160;        <span class="comment"># This op will just return grad_output two times in backward</span></div><div class="line"><a name="l01965"></a><span class="lineno"> 1965</span>&#160;        add1 = a + b</div><div class="line"><a name="l01966"></a><span class="lineno"> 1966</span>&#160;        add2 = add1 + c</div><div class="line"><a name="l01967"></a><span class="lineno"> 1967</span>&#160;        <span class="comment"># Simulate a long branch, so grad_output will get buffered.</span></div><div class="line"><a name="l01968"></a><span class="lineno"> 1968</span>&#160;        <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(4):</div><div class="line"><a name="l01969"></a><span class="lineno"> 1969</span>&#160;            a = a * 2</div><div class="line"><a name="l01970"></a><span class="lineno"> 1970</span>&#160;            b = b * 2</div><div class="line"><a name="l01971"></a><span class="lineno"> 1971</span>&#160;            c = c * 2</div><div class="line"><a name="l01972"></a><span class="lineno"> 1972</span>&#160;        branch = a + b + c</div><div class="line"><a name="l01973"></a><span class="lineno"> 1973</span>&#160;        out = add2 + branch</div><div class="line"><a name="l01974"></a><span class="lineno"> 1974</span>&#160;        <span class="comment"># expected gradients are:</span></div><div class="line"><a name="l01975"></a><span class="lineno"> 1975</span>&#160;        <span class="comment"># for x: 34 (16 from final a, 16 from final c, 2 from add2)</span></div><div class="line"><a name="l01976"></a><span class="lineno"> 1976</span>&#160;        <span class="comment"># for y: 17 (16 from final b, 1 from add2)</span></div><div class="line"><a name="l01977"></a><span class="lineno"> 1977</span>&#160;        grad_output = torch.ones(5, 5)</div><div class="line"><a name="l01978"></a><span class="lineno"> 1978</span>&#160;        out.backward(grad_output)</div><div class="line"><a name="l01979"></a><span class="lineno"> 1979</span>&#160;        self.assertEqual(x.grad, torch.ones(5, 5) * 34)</div><div class="line"><a name="l01980"></a><span class="lineno"> 1980</span>&#160;        self.assertEqual(y.grad, torch.ones(5, 5) * 17)</div><div class="line"><a name="l01981"></a><span class="lineno"> 1981</span>&#160;</div><div class="line"><a name="l01982"></a><span class="lineno"> 1982</span>&#160;    <span class="keyword">def </span>test_save_none_for_backward(self):</div><div class="line"><a name="l01983"></a><span class="lineno"> 1983</span>&#160;        test_case = self</div><div class="line"><a name="l01984"></a><span class="lineno"> 1984</span>&#160;</div><div class="line"><a name="l01985"></a><span class="lineno"> 1985</span>&#160;        <span class="keyword">class </span>MyFn(Function):</div><div class="line"><a name="l01986"></a><span class="lineno"> 1986</span>&#160;</div><div class="line"><a name="l01987"></a><span class="lineno"> 1987</span>&#160;            <span class="keyword">def </span>forward(self, input):</div><div class="line"><a name="l01988"></a><span class="lineno"> 1988</span>&#160;                self.save_for_backward(<span class="keywordtype">None</span>, input, <span class="keywordtype">None</span>)</div><div class="line"><a name="l01989"></a><span class="lineno"> 1989</span>&#160;                <span class="keywordflow">return</span> input * input</div><div class="line"><a name="l01990"></a><span class="lineno"> 1990</span>&#160;</div><div class="line"><a name="l01991"></a><span class="lineno"> 1991</span>&#160;            <span class="keyword">def </span>backward(self, grad_output):</div><div class="line"><a name="l01992"></a><span class="lineno"> 1992</span>&#160;                n1, input, n2 = self.saved_tensors</div><div class="line"><a name="l01993"></a><span class="lineno"> 1993</span>&#160;                test_case.assertIsNone(n1)</div><div class="line"><a name="l01994"></a><span class="lineno"> 1994</span>&#160;                test_case.assertIsNone(n2)</div><div class="line"><a name="l01995"></a><span class="lineno"> 1995</span>&#160;                <span class="keywordflow">return</span> 2 * input * grad_output</div><div class="line"><a name="l01996"></a><span class="lineno"> 1996</span>&#160;</div><div class="line"><a name="l01997"></a><span class="lineno"> 1997</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l01998"></a><span class="lineno"> 1998</span>&#160;        y = MyFn()(x)</div><div class="line"><a name="l01999"></a><span class="lineno"> 1999</span>&#160;        y.sum().backward()</div><div class="line"><a name="l02000"></a><span class="lineno"> 2000</span>&#160;        self.assertEqual(x.grad, 2 * x)</div><div class="line"><a name="l02001"></a><span class="lineno"> 2001</span>&#160;</div><div class="line"><a name="l02002"></a><span class="lineno"> 2002</span>&#160;    <span class="keyword">def </span>test_too_many_grads(self):</div><div class="line"><a name="l02003"></a><span class="lineno"> 2003</span>&#160;        <span class="keyword">class </span>MyFn(Function):</div><div class="line"><a name="l02004"></a><span class="lineno"> 2004</span>&#160;</div><div class="line"><a name="l02005"></a><span class="lineno"> 2005</span>&#160;            <span class="keyword">def </span>forward(self, input):</div><div class="line"><a name="l02006"></a><span class="lineno"> 2006</span>&#160;                <span class="keywordflow">return</span> input</div><div class="line"><a name="l02007"></a><span class="lineno"> 2007</span>&#160;</div><div class="line"><a name="l02008"></a><span class="lineno"> 2008</span>&#160;            <span class="keyword">def </span>backward(self, grad_output):</div><div class="line"><a name="l02009"></a><span class="lineno"> 2009</span>&#160;                <span class="keywordflow">return</span> grad_output, <span class="keywordtype">None</span>, <span class="keywordtype">None</span></div><div class="line"><a name="l02010"></a><span class="lineno"> 2010</span>&#160;</div><div class="line"><a name="l02011"></a><span class="lineno"> 2011</span>&#160;        x = torch.randn(5, 5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02012"></a><span class="lineno"> 2012</span>&#160;        y = MyFn()(x)</div><div class="line"><a name="l02013"></a><span class="lineno"> 2013</span>&#160;        y.sum().backward()</div><div class="line"><a name="l02014"></a><span class="lineno"> 2014</span>&#160;        self.assertEqual(x.grad, torch.ones_like(x))</div><div class="line"><a name="l02015"></a><span class="lineno"> 2015</span>&#160;</div><div class="line"><a name="l02016"></a><span class="lineno"> 2016</span>&#160;    <span class="keyword">def </span>test_pickle(self):</div><div class="line"><a name="l02017"></a><span class="lineno"> 2017</span>&#160;        x = torch.randn(10, 10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02018"></a><span class="lineno"> 2018</span>&#160;        y = torch.randn(10, 10, requires_grad=<span class="keyword">False</span>)</div><div class="line"><a name="l02019"></a><span class="lineno"> 2019</span>&#160;</div><div class="line"><a name="l02020"></a><span class="lineno"> 2020</span>&#160;        <span class="keyword">def </span>assert_strict_equal(var1, var2):</div><div class="line"><a name="l02021"></a><span class="lineno"> 2021</span>&#160;            self.assertEqual(var1.data, var2.data)</div><div class="line"><a name="l02022"></a><span class="lineno"> 2022</span>&#160;            self.assertEqual(var1.requires_grad, var2.requires_grad)</div><div class="line"><a name="l02023"></a><span class="lineno"> 2023</span>&#160;</div><div class="line"><a name="l02024"></a><span class="lineno"> 2024</span>&#160;        serialized = [pickle.dumps([x, y], protocol=p) <span class="keywordflow">for</span> p <span class="keywordflow">in</span> range(3)]</div><div class="line"><a name="l02025"></a><span class="lineno"> 2025</span>&#160;        <span class="keywordflow">for</span> dump <span class="keywordflow">in</span> serialized:</div><div class="line"><a name="l02026"></a><span class="lineno"> 2026</span>&#160;            xc, yc = pickle.loads(dump)</div><div class="line"><a name="l02027"></a><span class="lineno"> 2027</span>&#160;            assert_strict_equal(xc, x)</div><div class="line"><a name="l02028"></a><span class="lineno"> 2028</span>&#160;            assert_strict_equal(yc, y)</div><div class="line"><a name="l02029"></a><span class="lineno"> 2029</span>&#160;</div><div class="line"><a name="l02030"></a><span class="lineno"> 2030</span>&#160;    <span class="keyword">def </span>test_dep_nograd(self):</div><div class="line"><a name="l02031"></a><span class="lineno"> 2031</span>&#160;        <span class="keyword">class </span>F1(Function):</div><div class="line"><a name="l02032"></a><span class="lineno"> 2032</span>&#160;</div><div class="line"><a name="l02033"></a><span class="lineno"> 2033</span>&#160;            <span class="keyword">def </span>forward(self, input):</div><div class="line"><a name="l02034"></a><span class="lineno"> 2034</span>&#160;                out = torch.randn(input.size())</div><div class="line"><a name="l02035"></a><span class="lineno"> 2035</span>&#160;                self.mark_non_differentiable(out)</div><div class="line"><a name="l02036"></a><span class="lineno"> 2036</span>&#160;                <span class="keywordflow">return</span> input, out</div><div class="line"><a name="l02037"></a><span class="lineno"> 2037</span>&#160;</div><div class="line"><a name="l02038"></a><span class="lineno"> 2038</span>&#160;            <span class="keyword">def </span>backward(self, grad_output, ignored):</div><div class="line"><a name="l02039"></a><span class="lineno"> 2039</span>&#160;                <span class="keywordflow">return</span> grad_output</div><div class="line"><a name="l02040"></a><span class="lineno"> 2040</span>&#160;</div><div class="line"><a name="l02041"></a><span class="lineno"> 2041</span>&#160;        <span class="keyword">class </span>F2(Function):</div><div class="line"><a name="l02042"></a><span class="lineno"> 2042</span>&#160;</div><div class="line"><a name="l02043"></a><span class="lineno"> 2043</span>&#160;            <span class="keyword">def </span>forward(self, input, ignored):</div><div class="line"><a name="l02044"></a><span class="lineno"> 2044</span>&#160;                <span class="keywordflow">return</span> input</div><div class="line"><a name="l02045"></a><span class="lineno"> 2045</span>&#160;</div><div class="line"><a name="l02046"></a><span class="lineno"> 2046</span>&#160;            <span class="keyword">def </span>backward(self, grad_output):</div><div class="line"><a name="l02047"></a><span class="lineno"> 2047</span>&#160;                <span class="keywordflow">return</span> grad_output, <span class="keywordtype">None</span></div><div class="line"><a name="l02048"></a><span class="lineno"> 2048</span>&#160;</div><div class="line"><a name="l02049"></a><span class="lineno"> 2049</span>&#160;        x = torch.randn(5, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02050"></a><span class="lineno"> 2050</span>&#160;        a, b = F1()(x)</div><div class="line"><a name="l02051"></a><span class="lineno"> 2051</span>&#160;        b = b + 1  <span class="comment"># separate F1 from F2 by another op</span></div><div class="line"><a name="l02052"></a><span class="lineno"> 2052</span>&#160;        self.assertTrue(a.requires_grad)</div><div class="line"><a name="l02053"></a><span class="lineno"> 2053</span>&#160;        self.assertFalse(b.requires_grad)</div><div class="line"><a name="l02054"></a><span class="lineno"> 2054</span>&#160;        c = F2()(a, b)</div><div class="line"><a name="l02055"></a><span class="lineno"> 2055</span>&#160;        c.backward(torch.ones(c.size()))</div><div class="line"><a name="l02056"></a><span class="lineno"> 2056</span>&#160;        self.assertEqual(x.grad.data, torch.ones(x.size()))</div><div class="line"><a name="l02057"></a><span class="lineno"> 2057</span>&#160;</div><div class="line"><a name="l02058"></a><span class="lineno"> 2058</span>&#160;    <span class="keyword">def </span>test_set_grad_enabled(self):</div><div class="line"><a name="l02059"></a><span class="lineno"> 2059</span>&#160;        x = <a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>([1.], requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02060"></a><span class="lineno"> 2060</span>&#160;        with torch.set_grad_enabled(<span class="keyword">False</span>):</div><div class="line"><a name="l02061"></a><span class="lineno"> 2061</span>&#160;            y = x * 2</div><div class="line"><a name="l02062"></a><span class="lineno"> 2062</span>&#160;        self.assertFalse(y.requires_grad)</div><div class="line"><a name="l02063"></a><span class="lineno"> 2063</span>&#160;        with torch.set_grad_enabled(<span class="keyword">True</span>):</div><div class="line"><a name="l02064"></a><span class="lineno"> 2064</span>&#160;            y = x * 2</div><div class="line"><a name="l02065"></a><span class="lineno"> 2065</span>&#160;        self.assertTrue(y.requires_grad)</div><div class="line"><a name="l02066"></a><span class="lineno"> 2066</span>&#160;        with torch.set_grad_enabled(<span class="keyword">False</span>):</div><div class="line"><a name="l02067"></a><span class="lineno"> 2067</span>&#160;            torch.set_grad_enabled(<span class="keyword">True</span>)</div><div class="line"><a name="l02068"></a><span class="lineno"> 2068</span>&#160;            y = x * 2</div><div class="line"><a name="l02069"></a><span class="lineno"> 2069</span>&#160;        self.assertTrue(y.requires_grad)</div><div class="line"><a name="l02070"></a><span class="lineno"> 2070</span>&#160;</div><div class="line"><a name="l02071"></a><span class="lineno"> 2071</span>&#160;    <span class="keyword">def </span>test_reentrant(self):</div><div class="line"><a name="l02072"></a><span class="lineno"> 2072</span>&#160;        y_data = torch.randn(2, 2)</div><div class="line"><a name="l02073"></a><span class="lineno"> 2073</span>&#160;</div><div class="line"><a name="l02074"></a><span class="lineno"> 2074</span>&#160;        <span class="keyword">class </span>Reenter(Function):</div><div class="line"><a name="l02075"></a><span class="lineno"> 2075</span>&#160;            @staticmethod</div><div class="line"><a name="l02076"></a><span class="lineno"> 2076</span>&#160;            <span class="keyword">def </span>forward(ctx, x):</div><div class="line"><a name="l02077"></a><span class="lineno"> 2077</span>&#160;                with torch.enable_grad():</div><div class="line"><a name="l02078"></a><span class="lineno"> 2078</span>&#160;                    ctx.x = Variable(x.data, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02079"></a><span class="lineno"> 2079</span>&#160;                    ctx.y = Variable(y_data, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02080"></a><span class="lineno"> 2080</span>&#160;                    ctx.output_var = ctx.x * ctx.y</div><div class="line"><a name="l02081"></a><span class="lineno"> 2081</span>&#160;                <span class="keywordflow">return</span> ctx.output_var.detach()</div><div class="line"><a name="l02082"></a><span class="lineno"> 2082</span>&#160;</div><div class="line"><a name="l02083"></a><span class="lineno"> 2083</span>&#160;            @staticmethod</div><div class="line"><a name="l02084"></a><span class="lineno"> 2084</span>&#160;            <span class="keyword">def </span>backward(ctx, grad_output):</div><div class="line"><a name="l02085"></a><span class="lineno"> 2085</span>&#160;                with torch.enable_grad():</div><div class="line"><a name="l02086"></a><span class="lineno"> 2086</span>&#160;                    ctx.output_var.sum().backward()</div><div class="line"><a name="l02087"></a><span class="lineno"> 2087</span>&#160;                <span class="keywordflow">return</span> ctx.x.grad * grad_output</div><div class="line"><a name="l02088"></a><span class="lineno"> 2088</span>&#160;</div><div class="line"><a name="l02089"></a><span class="lineno"> 2089</span>&#160;        x = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02090"></a><span class="lineno"> 2090</span>&#160;        out = Reenter.apply(x)</div><div class="line"><a name="l02091"></a><span class="lineno"> 2091</span>&#160;        out.sum().backward()</div><div class="line"><a name="l02092"></a><span class="lineno"> 2092</span>&#160;        self.assertEqual(x.grad.data, y_data)</div><div class="line"><a name="l02093"></a><span class="lineno"> 2093</span>&#160;</div><div class="line"><a name="l02094"></a><span class="lineno"> 2094</span>&#160;    <span class="keyword">def </span>test_broadcast_tensors(self):</div><div class="line"><a name="l02095"></a><span class="lineno"> 2095</span>&#160;        f_args_variable = (torch.randn(3, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02096"></a><span class="lineno"> 2096</span>&#160;                           torch.randn(1, 2, 1, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02097"></a><span class="lineno"> 2097</span>&#160;                           torch.randn(1, 1, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02098"></a><span class="lineno"> 2098</span>&#160;                           torch.randn(5, 1, 1, requires_grad=<span class="keyword">True</span>))</div><div class="line"><a name="l02099"></a><span class="lineno"> 2099</span>&#160;        f_args_tensor = deepcopy(unpack_variables(f_args_variable))</div><div class="line"><a name="l02100"></a><span class="lineno"> 2100</span>&#160;        run_functional_checks(self, <span class="stringliteral">&quot;test_broadcast_tensors&quot;</span>, <span class="stringliteral">&quot;broadcast&quot;</span>,</div><div class="line"><a name="l02101"></a><span class="lineno"> 2101</span>&#160;                              <span class="keyword">lambda</span> a, b, c, d: torch.broadcast_tensors(a, b, c, d),</div><div class="line"><a name="l02102"></a><span class="lineno"> 2102</span>&#160;                              <span class="keyword">True</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l02103"></a><span class="lineno"> 2103</span>&#160;</div><div class="line"><a name="l02104"></a><span class="lineno"> 2104</span>&#160;    <span class="keyword">def </span>test_cat(self):</div><div class="line"><a name="l02105"></a><span class="lineno"> 2105</span>&#160;        f_args_variable = (torch.randn(1, S, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02106"></a><span class="lineno"> 2106</span>&#160;                           torch.randn(2, S, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02107"></a><span class="lineno"> 2107</span>&#160;                           torch.randn(3, S, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02108"></a><span class="lineno"> 2108</span>&#160;                           0)</div><div class="line"><a name="l02109"></a><span class="lineno"> 2109</span>&#160;        f_args_tensor = deepcopy(unpack_variables(f_args_variable))</div><div class="line"><a name="l02110"></a><span class="lineno"> 2110</span>&#160;        run_functional_checks(self, <span class="stringliteral">&quot;test_cat&quot;</span>, <span class="stringliteral">&quot;cat&quot;</span>,</div><div class="line"><a name="l02111"></a><span class="lineno"> 2111</span>&#160;                              <span class="keyword">lambda</span> a, b, c, dim: torch.cat((a, b, c), dim),</div><div class="line"><a name="l02112"></a><span class="lineno"> 2112</span>&#160;                              <span class="keyword">True</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l02113"></a><span class="lineno"> 2113</span>&#160;</div><div class="line"><a name="l02114"></a><span class="lineno"> 2114</span>&#160;    <span class="keyword">def </span>test_cat_negdim_1(self):</div><div class="line"><a name="l02115"></a><span class="lineno"> 2115</span>&#160;        f_args_variable = (torch.randn(S, S, 1, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02116"></a><span class="lineno"> 2116</span>&#160;                           torch.randn(S, S, 2, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02117"></a><span class="lineno"> 2117</span>&#160;                           torch.randn(S, S, 3, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02118"></a><span class="lineno"> 2118</span>&#160;                           -1)</div><div class="line"><a name="l02119"></a><span class="lineno"> 2119</span>&#160;        f_args_tensor = deepcopy(unpack_variables(f_args_variable))</div><div class="line"><a name="l02120"></a><span class="lineno"> 2120</span>&#160;        run_functional_checks(self, <span class="stringliteral">&quot;test_cat_negdim_1&quot;</span>, <span class="stringliteral">&quot;cat&quot;</span>,</div><div class="line"><a name="l02121"></a><span class="lineno"> 2121</span>&#160;                              <span class="keyword">lambda</span> a, b, c, dim: torch.cat((a, b, c), dim),</div><div class="line"><a name="l02122"></a><span class="lineno"> 2122</span>&#160;                              <span class="keyword">True</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l02123"></a><span class="lineno"> 2123</span>&#160;</div><div class="line"><a name="l02124"></a><span class="lineno"> 2124</span>&#160;    <span class="keyword">def </span>test_cat_negdim_2(self):</div><div class="line"><a name="l02125"></a><span class="lineno"> 2125</span>&#160;        f_args_variable = (torch.randn(S, 1, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02126"></a><span class="lineno"> 2126</span>&#160;                           torch.randn(S, 2, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02127"></a><span class="lineno"> 2127</span>&#160;                           torch.randn(S, 3, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02128"></a><span class="lineno"> 2128</span>&#160;                           -2)</div><div class="line"><a name="l02129"></a><span class="lineno"> 2129</span>&#160;        f_args_tensor = deepcopy(unpack_variables(f_args_variable))</div><div class="line"><a name="l02130"></a><span class="lineno"> 2130</span>&#160;        run_functional_checks(self, <span class="stringliteral">&quot;test_cat_negdim_2&quot;</span>, <span class="stringliteral">&quot;cat&quot;</span>,</div><div class="line"><a name="l02131"></a><span class="lineno"> 2131</span>&#160;                              <span class="keyword">lambda</span> a, b, c, dim: torch.cat((a, b, c), dim),</div><div class="line"><a name="l02132"></a><span class="lineno"> 2132</span>&#160;                              <span class="keyword">True</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l02133"></a><span class="lineno"> 2133</span>&#160;</div><div class="line"><a name="l02134"></a><span class="lineno"> 2134</span>&#160;    <span class="keyword">def </span>test_cat_empty_legacy(self):</div><div class="line"><a name="l02135"></a><span class="lineno"> 2135</span>&#160;        f_args_variable = (torch.randn(0, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02136"></a><span class="lineno"> 2136</span>&#160;                           torch.randn(S, S, requires_grad=<span class="keyword">True</span>))</div><div class="line"><a name="l02137"></a><span class="lineno"> 2137</span>&#160;        <span class="comment"># gradgradcheck doesn&#39;t work, probably because legacy size tracking is wrong somewhere,</span></div><div class="line"><a name="l02138"></a><span class="lineno"> 2138</span>&#160;        <span class="comment"># hence False passed below, but gradcheck checked explicitly.</span></div><div class="line"><a name="l02139"></a><span class="lineno"> 2139</span>&#160;        f_args_tensor = deepcopy(unpack_variables(f_args_variable))</div><div class="line"><a name="l02140"></a><span class="lineno"> 2140</span>&#160;        run_functional_checks(self, <span class="stringliteral">&quot;test_cat_empty_legacy&quot;</span>, <span class="stringliteral">&quot;cat&quot;</span>,</div><div class="line"><a name="l02141"></a><span class="lineno"> 2141</span>&#160;                              <span class="keyword">lambda</span> a, b: torch.cat((a, b)),</div><div class="line"><a name="l02142"></a><span class="lineno"> 2142</span>&#160;                              <span class="keyword">False</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l02143"></a><span class="lineno"> 2143</span>&#160;        self.assertTrue(gradcheck(<span class="keyword">lambda</span> a, b: torch.cat((a, b)), f_args_variable, eps=1e-6, atol=PRECISION))</div><div class="line"><a name="l02144"></a><span class="lineno"> 2144</span>&#160;</div><div class="line"><a name="l02145"></a><span class="lineno"> 2145</span>&#160;    <span class="keyword">def </span>test_cat_empty(self):</div><div class="line"><a name="l02146"></a><span class="lineno"> 2146</span>&#160;        f_args_variable = (torch.randn(0, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02147"></a><span class="lineno"> 2147</span>&#160;                           torch.randn(S, S, requires_grad=<span class="keyword">True</span>))</div><div class="line"><a name="l02148"></a><span class="lineno"> 2148</span>&#160;        f_args_tensor = deepcopy(unpack_variables(f_args_variable))</div><div class="line"><a name="l02149"></a><span class="lineno"> 2149</span>&#160;        run_functional_checks(self, <span class="stringliteral">&quot;test_cat_empty&quot;</span>, <span class="stringliteral">&quot;cat&quot;</span>,</div><div class="line"><a name="l02150"></a><span class="lineno"> 2150</span>&#160;                              <span class="keyword">lambda</span> a, b: torch.cat((a, b)),</div><div class="line"><a name="l02151"></a><span class="lineno"> 2151</span>&#160;                              <span class="keyword">True</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l02152"></a><span class="lineno"> 2152</span>&#160;</div><div class="line"><a name="l02153"></a><span class="lineno"> 2153</span>&#160;    <span class="keyword">def </span>test_cdist(self):</div><div class="line"><a name="l02154"></a><span class="lineno"> 2154</span>&#160;        <span class="keywordflow">for</span> p <span class="keywordflow">in</span> [0, 1, 2, 3, 1.5, 2.5, float(<span class="stringliteral">&#39;inf&#39;</span>)]:</div><div class="line"><a name="l02155"></a><span class="lineno"> 2155</span>&#160;            f_args_variable = (torch.randn(S, S, requires_grad=<span class="keyword">True</span>),</div><div class="line"><a name="l02156"></a><span class="lineno"> 2156</span>&#160;                               torch.randn(S, S, requires_grad=<span class="keyword">True</span>))</div><div class="line"><a name="l02157"></a><span class="lineno"> 2157</span>&#160;            f = <span class="keyword">lambda</span> a, b: torch.cdist(a, b, p)</div><div class="line"><a name="l02158"></a><span class="lineno"> 2158</span>&#160;            f_args_tensor = deepcopy(unpack_variables(f_args_variable))</div><div class="line"><a name="l02159"></a><span class="lineno"> 2159</span>&#160;            run_functional_checks(self, <span class="stringliteral">&quot;test_cdist&quot;</span>, <span class="stringliteral">&quot;cdist&quot;</span>, f,</div><div class="line"><a name="l02160"></a><span class="lineno"> 2160</span>&#160;                                  <span class="keyword">True</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l02161"></a><span class="lineno"> 2161</span>&#160;</div><div class="line"><a name="l02162"></a><span class="lineno"> 2162</span>&#160;    @skipIfNoLapack</div><div class="line"><a name="l02163"></a><span class="lineno"> 2163</span>&#160;    <span class="keyword">def </span>test_cholesky(self):</div><div class="line"><a name="l02164"></a><span class="lineno"> 2164</span>&#160;        <span class="keyword">def </span>func(root):</div><div class="line"><a name="l02165"></a><span class="lineno"> 2165</span>&#160;            x = torch.matmul(root, root.transpose(-1, -2)) + 1e-05</div><div class="line"><a name="l02166"></a><span class="lineno"> 2166</span>&#160;            <span class="keywordflow">return</span> torch.cholesky(x, upper)</div><div class="line"><a name="l02167"></a><span class="lineno"> 2167</span>&#160;</div><div class="line"><a name="l02168"></a><span class="lineno"> 2168</span>&#160;        <span class="keyword">def </span>run_test(upper, dims):</div><div class="line"><a name="l02169"></a><span class="lineno"> 2169</span>&#160;            root = torch.rand(*dims)</div><div class="line"><a name="l02170"></a><span class="lineno"> 2170</span>&#160;            indices = torch.ones(dims[-1], dims[-1], dtype=torch.uint8).tril()</div><div class="line"><a name="l02171"></a><span class="lineno"> 2171</span>&#160;            indices = indices.expand_as(root)</div><div class="line"><a name="l02172"></a><span class="lineno"> 2172</span>&#160;            root[indices] = 0</div><div class="line"><a name="l02173"></a><span class="lineno"> 2173</span>&#160;            root.requires_grad_()</div><div class="line"><a name="l02174"></a><span class="lineno"> 2174</span>&#160;</div><div class="line"><a name="l02175"></a><span class="lineno"> 2175</span>&#160;            gradcheck(func, [root])</div><div class="line"><a name="l02176"></a><span class="lineno"> 2176</span>&#160;            gradgradcheck(func, [root])</div><div class="line"><a name="l02177"></a><span class="lineno"> 2177</span>&#160;</div><div class="line"><a name="l02178"></a><span class="lineno"> 2178</span>&#160;        <span class="keywordflow">for</span> upper, dims <span class="keywordflow">in</span> product([<span class="keyword">True</span>, <span class="keyword">False</span>], [(3, 3), (4, 3, 2, 2)]):</div><div class="line"><a name="l02179"></a><span class="lineno"> 2179</span>&#160;            run_test(upper, dims)</div><div class="line"><a name="l02180"></a><span class="lineno"> 2180</span>&#160;            run_test(upper, dims)</div><div class="line"><a name="l02181"></a><span class="lineno"> 2181</span>&#160;</div><div class="line"><a name="l02182"></a><span class="lineno"> 2182</span>&#160;    @skipIfNoLapack</div><div class="line"><a name="l02183"></a><span class="lineno"> 2183</span>&#160;    <span class="keyword">def </span>test_trtrs(self):</div><div class="line"><a name="l02184"></a><span class="lineno"> 2184</span>&#160;        <span class="keyword">def </span>_test_with_size(A_dims, B_dims):</div><div class="line"><a name="l02185"></a><span class="lineno"> 2185</span>&#160;            A = torch.rand(*A_dims).requires_grad_()</div><div class="line"><a name="l02186"></a><span class="lineno"> 2186</span>&#160;            b = torch.rand(*B_dims).requires_grad_()</div><div class="line"><a name="l02187"></a><span class="lineno"> 2187</span>&#160;</div><div class="line"><a name="l02188"></a><span class="lineno"> 2188</span>&#160;            <span class="keywordflow">for</span> upper, transpose, unitriangular <span class="keywordflow">in</span> product((<span class="keyword">True</span>, <span class="keyword">False</span>), repeat=3):</div><div class="line"><a name="l02189"></a><span class="lineno"> 2189</span>&#160;                <span class="keyword">def </span>func(A, b):</div><div class="line"><a name="l02190"></a><span class="lineno"> 2190</span>&#160;                    <span class="keywordflow">return</span> torch.trtrs(b, A, upper, transpose, unitriangular)</div><div class="line"><a name="l02191"></a><span class="lineno"> 2191</span>&#160;</div><div class="line"><a name="l02192"></a><span class="lineno"> 2192</span>&#160;                gradcheck(func, [A, b])</div><div class="line"><a name="l02193"></a><span class="lineno"> 2193</span>&#160;                gradgradcheck(func, [A, b])</div><div class="line"><a name="l02194"></a><span class="lineno"> 2194</span>&#160;</div><div class="line"><a name="l02195"></a><span class="lineno"> 2195</span>&#160;        _test_with_size((3, 3), (3, 4))</div><div class="line"><a name="l02196"></a><span class="lineno"> 2196</span>&#160;        _test_with_size((3, 3), (3, 2))</div><div class="line"><a name="l02197"></a><span class="lineno"> 2197</span>&#160;        _test_with_size((2, 3, 3), (2, 3, 4))</div><div class="line"><a name="l02198"></a><span class="lineno"> 2198</span>&#160;        _test_with_size((2, 3, 3), (2, 3, 2))</div><div class="line"><a name="l02199"></a><span class="lineno"> 2199</span>&#160;</div><div class="line"><a name="l02200"></a><span class="lineno"> 2200</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> TEST_MKL, <span class="stringliteral">&quot;PyTorch is built without MKL support&quot;</span>)</div><div class="line"><a name="l02201"></a><span class="lineno"> 2201</span>&#160;    <span class="keyword">def </span>test_fft_ifft_rfft_irfft(self):</div><div class="line"><a name="l02202"></a><span class="lineno"> 2202</span>&#160;        <span class="keyword">def </span>_test_complex(sizes, signal_ndim):</div><div class="line"><a name="l02203"></a><span class="lineno"> 2203</span>&#160;            x = torch.randn(sizes, requires_grad=<span class="keyword">True</span>, dtype=torch.double)</div><div class="line"><a name="l02204"></a><span class="lineno"> 2204</span>&#160;</div><div class="line"><a name="l02205"></a><span class="lineno"> 2205</span>&#160;            <span class="keywordflow">for</span> normalized <span class="keywordflow">in</span> (<span class="keyword">True</span>, <span class="keyword">False</span>):</div><div class="line"><a name="l02206"></a><span class="lineno"> 2206</span>&#160;                <span class="keyword">def </span>fft(x):</div><div class="line"><a name="l02207"></a><span class="lineno"> 2207</span>&#160;                    <span class="keywordflow">return</span> x.fft(signal_ndim, normalized=normalized)</div><div class="line"><a name="l02208"></a><span class="lineno"> 2208</span>&#160;</div><div class="line"><a name="l02209"></a><span class="lineno"> 2209</span>&#160;                gradcheck(fft, [x])</div><div class="line"><a name="l02210"></a><span class="lineno"> 2210</span>&#160;                gradgradcheck(fft, [x], gen_non_contig_grad_outputs=<span class="keyword">True</span>)</div><div class="line"><a name="l02211"></a><span class="lineno"> 2211</span>&#160;</div><div class="line"><a name="l02212"></a><span class="lineno"> 2212</span>&#160;                <span class="keyword">def </span>ifft(fx):</div><div class="line"><a name="l02213"></a><span class="lineno"> 2213</span>&#160;                    <span class="keywordflow">return</span> fx.ifft(signal_ndim, normalized=normalized)</div><div class="line"><a name="l02214"></a><span class="lineno"> 2214</span>&#160;</div><div class="line"><a name="l02215"></a><span class="lineno"> 2215</span>&#160;                <span class="comment"># Use output of fft(x) for inverse fft, due to symmetry requirements</span></div><div class="line"><a name="l02216"></a><span class="lineno"> 2216</span>&#160;                fx = fft(x).detach()</div><div class="line"><a name="l02217"></a><span class="lineno"> 2217</span>&#160;                fx.requires_grad = <span class="keyword">True</span></div><div class="line"><a name="l02218"></a><span class="lineno"> 2218</span>&#160;                gradcheck(ifft, [fx])</div><div class="line"><a name="l02219"></a><span class="lineno"> 2219</span>&#160;                gradgradcheck(ifft, [fx], gen_non_contig_grad_outputs=<span class="keyword">True</span>)</div><div class="line"><a name="l02220"></a><span class="lineno"> 2220</span>&#160;</div><div class="line"><a name="l02221"></a><span class="lineno"> 2221</span>&#160;        <span class="keyword">def </span>_test_real(sizes, signal_ndim):</div><div class="line"><a name="l02222"></a><span class="lineno"> 2222</span>&#160;            x = torch.randn(sizes, requires_grad=<span class="keyword">True</span>, dtype=torch.double)</div><div class="line"><a name="l02223"></a><span class="lineno"> 2223</span>&#160;            <span class="keywordflow">if</span> x.dim() == signal_ndim:</div><div class="line"><a name="l02224"></a><span class="lineno"> 2224</span>&#160;                start_dim = 0</div><div class="line"><a name="l02225"></a><span class="lineno"> 2225</span>&#160;            <span class="keywordflow">else</span>:</div><div class="line"><a name="l02226"></a><span class="lineno"> 2226</span>&#160;                start_dim = 1</div><div class="line"><a name="l02227"></a><span class="lineno"> 2227</span>&#160;            signal_sizes = x.size()[start_dim:start_dim + signal_ndim]</div><div class="line"><a name="l02228"></a><span class="lineno"> 2228</span>&#160;</div><div class="line"><a name="l02229"></a><span class="lineno"> 2229</span>&#160;            <span class="keywordflow">for</span> normalized, onesided <span class="keywordflow">in</span> product((<span class="keyword">True</span>, <span class="keyword">False</span>), repeat=2):</div><div class="line"><a name="l02230"></a><span class="lineno"> 2230</span>&#160;                <span class="keyword">def </span>rfft(x):</div><div class="line"><a name="l02231"></a><span class="lineno"> 2231</span>&#160;                    <span class="keywordflow">return</span> x.rfft(signal_ndim, normalized=normalized, onesided=onesided)</div><div class="line"><a name="l02232"></a><span class="lineno"> 2232</span>&#160;</div><div class="line"><a name="l02233"></a><span class="lineno"> 2233</span>&#160;                gradcheck(rfft, [x])</div><div class="line"><a name="l02234"></a><span class="lineno"> 2234</span>&#160;                gradgradcheck(rfft, [x], gen_non_contig_grad_outputs=<span class="keyword">True</span>)</div><div class="line"><a name="l02235"></a><span class="lineno"> 2235</span>&#160;</div><div class="line"><a name="l02236"></a><span class="lineno"> 2236</span>&#160;                <span class="comment"># Generally speaking, irfft itself won&#39;t and can&#39;t pass the</span></div><div class="line"><a name="l02237"></a><span class="lineno"> 2237</span>&#160;                <span class="comment"># current gradcheck as it assumes the input follows conjugate</span></div><div class="line"><a name="l02238"></a><span class="lineno"> 2238</span>&#160;                <span class="comment"># symmetry, an requirement that is never true with our point</span></div><div class="line"><a name="l02239"></a><span class="lineno"> 2239</span>&#160;                <span class="comment"># numerical Jacobian estimate. Without input symmtry, irfft&#39;s</span></div><div class="line"><a name="l02240"></a><span class="lineno"> 2240</span>&#160;                <span class="comment"># behavior is undefined.</span></div><div class="line"><a name="l02241"></a><span class="lineno"> 2241</span>&#160;                <span class="comment">#</span></div><div class="line"><a name="l02242"></a><span class="lineno"> 2242</span>&#160;                <span class="comment"># Even onesided results can&#39;t remove all redundancy. For</span></div><div class="line"><a name="l02243"></a><span class="lineno"> 2243</span>&#160;                <span class="comment"># example, consider the .select(last_signal_dim, 0) slice.</span></div><div class="line"><a name="l02244"></a><span class="lineno"> 2244</span>&#160;                <span class="comment"># It is entirely represented in the onesided results (except</span></div><div class="line"><a name="l02245"></a><span class="lineno"> 2245</span>&#160;                <span class="comment"># for 1D), and will be reflected onto itself!</span></div><div class="line"><a name="l02246"></a><span class="lineno"> 2246</span>&#160;                <span class="comment">#</span></div><div class="line"><a name="l02247"></a><span class="lineno"> 2247</span>&#160;                <span class="comment"># So only 1D onesided irfft should pass grad check as it is</span></div><div class="line"><a name="l02248"></a><span class="lineno"> 2248</span>&#160;                <span class="comment"># guaranteed that the input has no symmetrical values.</span></div><div class="line"><a name="l02249"></a><span class="lineno"> 2249</span>&#160;                <span class="comment">#</span></div><div class="line"><a name="l02250"></a><span class="lineno"> 2250</span>&#160;                <span class="comment"># In other cases, we test a function that first uses rfft to</span></div><div class="line"><a name="l02251"></a><span class="lineno"> 2251</span>&#160;                <span class="comment"># generate a tensor that follows the conjugate symmetry irfft</span></div><div class="line"><a name="l02252"></a><span class="lineno"> 2252</span>&#160;                <span class="comment"># expects, and then feeds it into irfft. Since rfft is already</span></div><div class="line"><a name="l02253"></a><span class="lineno"> 2253</span>&#160;                <span class="comment"># tested above, we thereby verify the correctness of irfft.</span></div><div class="line"><a name="l02254"></a><span class="lineno"> 2254</span>&#160;                <span class="keywordflow">if</span> signal_ndim == 1 <span class="keywordflow">and</span> onesided:</div><div class="line"><a name="l02255"></a><span class="lineno"> 2255</span>&#160;                    <span class="keyword">def </span>irfft(fx):</div><div class="line"><a name="l02256"></a><span class="lineno"> 2256</span>&#160;                        <span class="keywordflow">return</span> fx.irfft(signal_ndim, normalized=normalized,</div><div class="line"><a name="l02257"></a><span class="lineno"> 2257</span>&#160;                                        onesided=onesided, signal_sizes=signal_sizes)</div><div class="line"><a name="l02258"></a><span class="lineno"> 2258</span>&#160;</div><div class="line"><a name="l02259"></a><span class="lineno"> 2259</span>&#160;                    <span class="comment"># Use output of rfft(x) for inverse rfft, due to symmetry requirements</span></div><div class="line"><a name="l02260"></a><span class="lineno"> 2260</span>&#160;                    fx = rfft(x).detach()</div><div class="line"><a name="l02261"></a><span class="lineno"> 2261</span>&#160;                    fx.requires_grad = <span class="keyword">True</span></div><div class="line"><a name="l02262"></a><span class="lineno"> 2262</span>&#160;                    gradcheck(irfft, [fx])</div><div class="line"><a name="l02263"></a><span class="lineno"> 2263</span>&#160;                    gradgradcheck(irfft, [fx], gen_non_contig_grad_outputs=<span class="keyword">True</span>)</div><div class="line"><a name="l02264"></a><span class="lineno"> 2264</span>&#160;                <span class="keywordflow">else</span>:</div><div class="line"><a name="l02265"></a><span class="lineno"> 2265</span>&#160;                    <span class="comment"># Test this function: f(x) = ifft(rfft(x) + rfft(z)), where</span></div><div class="line"><a name="l02266"></a><span class="lineno"> 2266</span>&#160;                    <span class="comment"># z is some fixed tensor of same size as x. rfft(z) term is</span></div><div class="line"><a name="l02267"></a><span class="lineno"> 2267</span>&#160;                    <span class="comment"># needed because otherwise f becomes identity.</span></div><div class="line"><a name="l02268"></a><span class="lineno"> 2268</span>&#160;                    z = torch.randn(sizes, dtype=torch.double)</div><div class="line"><a name="l02269"></a><span class="lineno"> 2269</span>&#160;                    fz = z.rfft(signal_ndim, normalized=normalized, onesided=onesided)</div><div class="line"><a name="l02270"></a><span class="lineno"> 2270</span>&#160;</div><div class="line"><a name="l02271"></a><span class="lineno"> 2271</span>&#160;                    <span class="keyword">def </span>rfft_irfft(x):</div><div class="line"><a name="l02272"></a><span class="lineno"> 2272</span>&#160;                        fx = x.rfft(signal_ndim, normalized=normalized, onesided=onesided)</div><div class="line"><a name="l02273"></a><span class="lineno"> 2273</span>&#160;                        y = fx + fz</div><div class="line"><a name="l02274"></a><span class="lineno"> 2274</span>&#160;                        <span class="keywordflow">return</span> y.irfft(signal_ndim, normalized=normalized,</div><div class="line"><a name="l02275"></a><span class="lineno"> 2275</span>&#160;                                       onesided=onesided, signal_sizes=signal_sizes)</div><div class="line"><a name="l02276"></a><span class="lineno"> 2276</span>&#160;</div><div class="line"><a name="l02277"></a><span class="lineno"> 2277</span>&#160;                    gradcheck(rfft_irfft, [x])</div><div class="line"><a name="l02278"></a><span class="lineno"> 2278</span>&#160;                    gradgradcheck(rfft_irfft, [x], gen_non_contig_grad_outputs=<span class="keyword">True</span>)</div><div class="line"><a name="l02279"></a><span class="lineno"> 2279</span>&#160;</div><div class="line"><a name="l02280"></a><span class="lineno"> 2280</span>&#160;        _test_real((2, 10), 1)</div><div class="line"><a name="l02281"></a><span class="lineno"> 2281</span>&#160;        _test_real((2, 3, 4), 2)</div><div class="line"><a name="l02282"></a><span class="lineno"> 2282</span>&#160;        _test_real((2, 3, 4, 3), 3)</div><div class="line"><a name="l02283"></a><span class="lineno"> 2283</span>&#160;</div><div class="line"><a name="l02284"></a><span class="lineno"> 2284</span>&#160;        _test_complex((2, 2, 10, 2), 1)</div><div class="line"><a name="l02285"></a><span class="lineno"> 2285</span>&#160;        _test_complex((1, 2, 3, 4, 2), 2)</div><div class="line"><a name="l02286"></a><span class="lineno"> 2286</span>&#160;        _test_complex((2, 1, 3, 4, 3, 2), 3)</div><div class="line"><a name="l02287"></a><span class="lineno"> 2287</span>&#160;</div><div class="line"><a name="l02288"></a><span class="lineno"> 2288</span>&#160;    <span class="keyword">def </span>test_variable_traverse(self):</div><div class="line"><a name="l02289"></a><span class="lineno"> 2289</span>&#160;        <span class="keyword">def </span>get_out_and_unrefed_cycle():</div><div class="line"><a name="l02290"></a><span class="lineno"> 2290</span>&#160;            inp = torch.randn(10, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02291"></a><span class="lineno"> 2291</span>&#160;            tmp = inp.view(10, 1)</div><div class="line"><a name="l02292"></a><span class="lineno"> 2292</span>&#160;            out = tmp.view(10)</div><div class="line"><a name="l02293"></a><span class="lineno"> 2293</span>&#160;</div><div class="line"><a name="l02294"></a><span class="lineno"> 2294</span>&#160;            <span class="comment"># Create a reference cycle that contains an</span></div><div class="line"><a name="l02295"></a><span class="lineno"> 2295</span>&#160;            <span class="comment"># intermediary Variable in the graph</span></div><div class="line"><a name="l02296"></a><span class="lineno"> 2296</span>&#160;            my_list = []</div><div class="line"><a name="l02297"></a><span class="lineno"> 2297</span>&#160;            my_list.append(tmp)</div><div class="line"><a name="l02298"></a><span class="lineno"> 2298</span>&#160;            my_list.append(my_list)</div><div class="line"><a name="l02299"></a><span class="lineno"> 2299</span>&#160;</div><div class="line"><a name="l02300"></a><span class="lineno"> 2300</span>&#160;            <span class="keywordflow">return</span> out</div><div class="line"><a name="l02301"></a><span class="lineno"> 2301</span>&#160;</div><div class="line"><a name="l02302"></a><span class="lineno"> 2302</span>&#160;        out = get_out_and_unrefed_cycle()</div><div class="line"><a name="l02303"></a><span class="lineno"> 2303</span>&#160;        gc.collect()</div><div class="line"><a name="l02304"></a><span class="lineno"> 2304</span>&#160;        <span class="comment"># This will segfault if things have been erroneously released</span></div><div class="line"><a name="l02305"></a><span class="lineno"> 2305</span>&#160;        out.backward(torch.randn(out.size()))</div><div class="line"><a name="l02306"></a><span class="lineno"> 2306</span>&#160;</div><div class="line"><a name="l02307"></a><span class="lineno"> 2307</span>&#160;    <span class="keyword">def </span>test_norm_subgradient(self):</div><div class="line"><a name="l02308"></a><span class="lineno"> 2308</span>&#160;        <span class="keyword">def </span>run_test(input_size, norm_deg):</div><div class="line"><a name="l02309"></a><span class="lineno"> 2309</span>&#160;            input = torch.zeros(*input_size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02310"></a><span class="lineno"> 2310</span>&#160;            input.norm(norm_deg).backward()</div><div class="line"><a name="l02311"></a><span class="lineno"> 2311</span>&#160;            self.assertEqual(input.grad.data.abs().sum(), 0)</div><div class="line"><a name="l02312"></a><span class="lineno"> 2312</span>&#160;</div><div class="line"><a name="l02313"></a><span class="lineno"> 2313</span>&#160;        run_test((10,), 2)</div><div class="line"><a name="l02314"></a><span class="lineno"> 2314</span>&#160;        run_test((10, 10), 2)</div><div class="line"><a name="l02315"></a><span class="lineno"> 2315</span>&#160;        run_test((10,), 3)</div><div class="line"><a name="l02316"></a><span class="lineno"> 2316</span>&#160;        run_test((10,), 1)</div><div class="line"><a name="l02317"></a><span class="lineno"> 2317</span>&#160;        run_test((10,), 1.5)</div><div class="line"><a name="l02318"></a><span class="lineno"> 2318</span>&#160;</div><div class="line"><a name="l02319"></a><span class="lineno"> 2319</span>&#160;    <span class="keyword">def </span>test_pow_zero_tensor_gradient(self):</div><div class="line"><a name="l02320"></a><span class="lineno"> 2320</span>&#160;        <span class="keyword">def </span>run_test(input_size, exponent):</div><div class="line"><a name="l02321"></a><span class="lineno"> 2321</span>&#160;            input = torch.zeros(*input_size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02322"></a><span class="lineno"> 2322</span>&#160;            input.pow(exponent).sum().backward()</div><div class="line"><a name="l02323"></a><span class="lineno"> 2323</span>&#160;            self.assertEqual(input.grad.data.abs().sum(), 0)</div><div class="line"><a name="l02324"></a><span class="lineno"> 2324</span>&#160;</div><div class="line"><a name="l02325"></a><span class="lineno"> 2325</span>&#160;        run_test((10,), torch.zeros(10))</div><div class="line"><a name="l02326"></a><span class="lineno"> 2326</span>&#160;        run_test((10, 10), torch.zeros(10, 10))</div><div class="line"><a name="l02327"></a><span class="lineno"> 2327</span>&#160;        run_test((10,), 0)</div><div class="line"><a name="l02328"></a><span class="lineno"> 2328</span>&#160;</div><div class="line"><a name="l02329"></a><span class="lineno"> 2329</span>&#160;    <span class="keyword">def </span>test_pow_scalar_base(self):</div><div class="line"><a name="l02330"></a><span class="lineno"> 2330</span>&#160;        a = torch.arange(1, 13, dtype=torch.double).view(3, 4).requires_grad_()</div><div class="line"><a name="l02331"></a><span class="lineno"> 2331</span>&#160;        gradcheck(<span class="keyword">lambda</span> a: torch.pow(2, a), (a,))</div><div class="line"><a name="l02332"></a><span class="lineno"> 2332</span>&#160;</div><div class="line"><a name="l02333"></a><span class="lineno"> 2333</span>&#160;    <span class="comment"># test for backward in https://github.com/pytorch/pytorch/issues/15511</span></div><div class="line"><a name="l02334"></a><span class="lineno"> 2334</span>&#160;    <span class="keyword">def </span>test_pdist_large(self):</div><div class="line"><a name="l02335"></a><span class="lineno"> 2335</span>&#160;        <span class="keyword">def </span>func(x):</div><div class="line"><a name="l02336"></a><span class="lineno"> 2336</span>&#160;            <span class="keywordflow">return</span> torch.pdist(x, p=2)</div><div class="line"><a name="l02337"></a><span class="lineno"> 2337</span>&#160;</div><div class="line"><a name="l02338"></a><span class="lineno"> 2338</span>&#160;        devices = [<span class="stringliteral">&#39;cpu&#39;</span>] <span class="keywordflow">if</span> <span class="keywordflow">not</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>() <span class="keywordflow">else</span> [<span class="stringliteral">&#39;cpu&#39;</span>, <span class="stringliteral">&#39;cuda&#39;</span>]</div><div class="line"><a name="l02339"></a><span class="lineno"> 2339</span>&#160;        <span class="keywordflow">for</span> device <span class="keywordflow">in</span> devices:</div><div class="line"><a name="l02340"></a><span class="lineno"> 2340</span>&#160;            <span class="comment"># shape[0] should be able to be (roughly) arbitrarily large, but the kernel</span></div><div class="line"><a name="l02341"></a><span class="lineno"> 2341</span>&#160;            <span class="comment"># is currently limited to smaller sizes (see issue above); this is just testing</span></div><div class="line"><a name="l02342"></a><span class="lineno"> 2342</span>&#160;            <span class="comment"># a floor.</span></div><div class="line"><a name="l02343"></a><span class="lineno"> 2343</span>&#160;            shape = (1000, 1)</div><div class="line"><a name="l02344"></a><span class="lineno"> 2344</span>&#160;            x = torch.randn(shape, device=device).requires_grad_()</div><div class="line"><a name="l02345"></a><span class="lineno"> 2345</span>&#160;            output = torch.pdist(x, p=2)</div><div class="line"><a name="l02346"></a><span class="lineno"> 2346</span>&#160;            <span class="comment"># just run a single backward, as gradcheck/gradgradcheck is expensive here</span></div><div class="line"><a name="l02347"></a><span class="lineno"> 2347</span>&#160;            output.sum().backward()</div><div class="line"><a name="l02348"></a><span class="lineno"> 2348</span>&#160;</div><div class="line"><a name="l02349"></a><span class="lineno"> 2349</span>&#160;    @skipIfNoLapack</div><div class="line"><a name="l02350"></a><span class="lineno"> 2350</span>&#160;    <span class="keyword">def </span>test_pinverse(self):</div><div class="line"><a name="l02351"></a><span class="lineno"> 2351</span>&#160;        <span class="comment"># Why is pinverse tested this way, and not ordinarily as other linear algebra methods?</span></div><div class="line"><a name="l02352"></a><span class="lineno"> 2352</span>&#160;        <span class="comment"># 1. Pseudo-inverses are not generally continuous, which means that they are not differentiable</span></div><div class="line"><a name="l02353"></a><span class="lineno"> 2353</span>&#160;        <span class="comment"># 2. Derivatives for pseudo-inverses exist typically for constant rank (Golub et al, 1973)</span></div><div class="line"><a name="l02354"></a><span class="lineno"> 2354</span>&#160;        <span class="comment"># 3. This method creates two orthogonal matrices, and a constructs a test case with large</span></div><div class="line"><a name="l02355"></a><span class="lineno"> 2355</span>&#160;        <span class="comment">#    singular values (given by x to the function).</span></div><div class="line"><a name="l02356"></a><span class="lineno"> 2356</span>&#160;        <span class="comment"># 4. This will ensure that small perturbations don&#39;t affect the rank of matrix, in which case</span></div><div class="line"><a name="l02357"></a><span class="lineno"> 2357</span>&#160;        <span class="comment">#    a derivative exists.</span></div><div class="line"><a name="l02358"></a><span class="lineno"> 2358</span>&#160;        <span class="comment"># 5. This test exists since pinverse is implemented using SVD, and is hence a backpropable method</span></div><div class="line"><a name="l02359"></a><span class="lineno"> 2359</span>&#160;        m, n = 5, 10</div><div class="line"><a name="l02360"></a><span class="lineno"> 2360</span>&#160;        U = torch.randn(n, m).qr()[0].t()  <span class="comment"># Orthogonal with dimensions m x n</span></div><div class="line"><a name="l02361"></a><span class="lineno"> 2361</span>&#160;        V = torch.randn(n, m).qr()[0].t()  <span class="comment"># Orthogonal with dimensions m x n</span></div><div class="line"><a name="l02362"></a><span class="lineno"> 2362</span>&#160;</div><div class="line"><a name="l02363"></a><span class="lineno"> 2363</span>&#160;        <span class="keyword">def </span>func(x):</div><div class="line"><a name="l02364"></a><span class="lineno"> 2364</span>&#160;            S = torch.cat([x, torch.zeros(n - m)], 0)</div><div class="line"><a name="l02365"></a><span class="lineno"> 2365</span>&#160;            M = U.mm(torch.diag(S)).mm(V.t())</div><div class="line"><a name="l02366"></a><span class="lineno"> 2366</span>&#160;            <span class="keywordflow">return</span> M.pinverse()</div><div class="line"><a name="l02367"></a><span class="lineno"> 2367</span>&#160;</div><div class="line"><a name="l02368"></a><span class="lineno"> 2368</span>&#160;        gradcheck(func, [torch.rand(m).add_(1).requires_grad_()])</div><div class="line"><a name="l02369"></a><span class="lineno"> 2369</span>&#160;        gradcheck(func, [torch.rand(m).add_(10).requires_grad_()])</div><div class="line"><a name="l02370"></a><span class="lineno"> 2370</span>&#160;        gradgradcheck(func, [torch.rand(m).add_(1).requires_grad_()])</div><div class="line"><a name="l02371"></a><span class="lineno"> 2371</span>&#160;        gradgradcheck(func, [torch.rand(m).add_(10).requires_grad_()])</div><div class="line"><a name="l02372"></a><span class="lineno"> 2372</span>&#160;</div><div class="line"><a name="l02373"></a><span class="lineno"> 2373</span>&#160;    <span class="keyword">def </span>test_chain_matmul(self):</div><div class="line"><a name="l02374"></a><span class="lineno"> 2374</span>&#160;        <span class="keyword">def </span>gen_matrices(p):</div><div class="line"><a name="l02375"></a><span class="lineno"> 2375</span>&#160;            matrices = []</div><div class="line"><a name="l02376"></a><span class="lineno"> 2376</span>&#160;            <span class="keywordflow">for</span> (pi, pi_1) <span class="keywordflow">in</span> zip(p[:-1], p[1:]):</div><div class="line"><a name="l02377"></a><span class="lineno"> 2377</span>&#160;                matrices.append(torch.randn(pi, pi_1).requires_grad_())</div><div class="line"><a name="l02378"></a><span class="lineno"> 2378</span>&#160;            <span class="keywordflow">return</span> matrices</div><div class="line"><a name="l02379"></a><span class="lineno"> 2379</span>&#160;</div><div class="line"><a name="l02380"></a><span class="lineno"> 2380</span>&#160;        gradcheck(torch.chain_matmul, gen_matrices([5, 10, 15, 5]))</div><div class="line"><a name="l02381"></a><span class="lineno"> 2381</span>&#160;        gradcheck(torch.chain_matmul, gen_matrices([3, 5, 2, 6]))</div><div class="line"><a name="l02382"></a><span class="lineno"> 2382</span>&#160;        gradcheck(torch.chain_matmul, gen_matrices([6, 2, 4, 8, 10]))</div><div class="line"><a name="l02383"></a><span class="lineno"> 2383</span>&#160;        gradgradcheck(torch.chain_matmul, gen_matrices([5, 10, 15, 5]))</div><div class="line"><a name="l02384"></a><span class="lineno"> 2384</span>&#160;        gradgradcheck(torch.chain_matmul, gen_matrices([3, 5, 2, 6]))</div><div class="line"><a name="l02385"></a><span class="lineno"> 2385</span>&#160;        gradgradcheck(torch.chain_matmul, gen_matrices([6, 2, 4, 8, 10]))</div><div class="line"><a name="l02386"></a><span class="lineno"> 2386</span>&#160;</div><div class="line"><a name="l02387"></a><span class="lineno"> 2387</span>&#160;    <span class="keyword">def </span>test_profiler(self):</div><div class="line"><a name="l02388"></a><span class="lineno"> 2388</span>&#160;        x = torch.randn(10, 10)</div><div class="line"><a name="l02389"></a><span class="lineno"> 2389</span>&#160;</div><div class="line"><a name="l02390"></a><span class="lineno"> 2390</span>&#160;        with <a class="code" href="classtorch_1_1autograd_1_1profiler_1_1profile.html">profile</a>() <span class="keyword">as</span> p:</div><div class="line"><a name="l02391"></a><span class="lineno"> 2391</span>&#160;            y = x * 2 + 4</div><div class="line"><a name="l02392"></a><span class="lineno"> 2392</span>&#160;</div><div class="line"><a name="l02393"></a><span class="lineno"> 2393</span>&#160;        last_end = 0</div><div class="line"><a name="l02394"></a><span class="lineno"> 2394</span>&#160;        names = [<span class="stringliteral">&#39;mul&#39;</span>, <span class="stringliteral">&#39;add&#39;</span>]</div><div class="line"><a name="l02395"></a><span class="lineno"> 2395</span>&#160;        self.assertEqual(len(p.function_events), len(names))</div><div class="line"><a name="l02396"></a><span class="lineno"> 2396</span>&#160;        <span class="keywordflow">for</span> info, expected_name <span class="keywordflow">in</span> zip(p.function_events, names):</div><div class="line"><a name="l02397"></a><span class="lineno"> 2397</span>&#160;            self.assertGreater(info.cpu_interval.start, last_end)</div><div class="line"><a name="l02398"></a><span class="lineno"> 2398</span>&#160;            self.assertEqual(info.name, expected_name)</div><div class="line"><a name="l02399"></a><span class="lineno"> 2399</span>&#160;            last_end = info.cpu_interval.end</div><div class="line"><a name="l02400"></a><span class="lineno"> 2400</span>&#160;</div><div class="line"><a name="l02401"></a><span class="lineno"> 2401</span>&#160;    <span class="keyword">def </span>test_dir(self):</div><div class="line"><a name="l02402"></a><span class="lineno"> 2402</span>&#160;        x = torch.randn(10, 10)</div><div class="line"><a name="l02403"></a><span class="lineno"> 2403</span>&#160;        keys = dir(x)</div><div class="line"><a name="l02404"></a><span class="lineno"> 2404</span>&#160;        self.assertIn(<span class="stringliteral">&#39;shape&#39;</span>, keys)</div><div class="line"><a name="l02405"></a><span class="lineno"> 2405</span>&#160;</div><div class="line"><a name="l02406"></a><span class="lineno"> 2406</span>&#160;        <span class="keywordflow">for</span> key <span class="keywordflow">in</span> keys:</div><div class="line"><a name="l02407"></a><span class="lineno"> 2407</span>&#160;            self.assertTrue(hasattr(x, key))</div><div class="line"><a name="l02408"></a><span class="lineno"> 2408</span>&#160;</div><div class="line"><a name="l02409"></a><span class="lineno"> 2409</span>&#160;    <span class="keyword">def </span>test_as_strided(self):</div><div class="line"><a name="l02410"></a><span class="lineno"> 2410</span>&#160;</div><div class="line"><a name="l02411"></a><span class="lineno"> 2411</span>&#160;        <span class="keyword">def </span><a class="code" href="namespacetest.html">test</a>(x, prepro_fn, size, strides, offset=None):</div><div class="line"><a name="l02412"></a><span class="lineno"> 2412</span>&#160;            x = x.to(torch.double).detach().requires_grad_()</div><div class="line"><a name="l02413"></a><span class="lineno"> 2413</span>&#160;</div><div class="line"><a name="l02414"></a><span class="lineno"> 2414</span>&#160;            <span class="comment"># Check that forward will **not** resize storage because it may</span></div><div class="line"><a name="l02415"></a><span class="lineno"> 2415</span>&#160;            <span class="comment"># cause NaN in output and fail numerical Jacobian check consequently</span></div><div class="line"><a name="l02416"></a><span class="lineno"> 2416</span>&#160;            with torch.no_grad():</div><div class="line"><a name="l02417"></a><span class="lineno"> 2417</span>&#160;                y = prepro_fn(x) <span class="keywordflow">if</span> prepro_fn <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">else</span> x</div><div class="line"><a name="l02418"></a><span class="lineno"> 2418</span>&#160;                max_offset = sum((si - 1) * st <span class="keywordflow">for</span> si, st <span class="keywordflow">in</span> zip(size, strides))</div><div class="line"><a name="l02419"></a><span class="lineno"> 2419</span>&#160;                max_offset += offset <span class="keywordflow">if</span> offset <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">else</span> y.storage_offset()</div><div class="line"><a name="l02420"></a><span class="lineno"> 2420</span>&#160;                <span class="keyword">assert</span> max_offset &lt; len(y.storage()), <span class="stringliteral">&quot;test case resizes storage&quot;</span></div><div class="line"><a name="l02421"></a><span class="lineno"> 2421</span>&#160;</div><div class="line"><a name="l02422"></a><span class="lineno"> 2422</span>&#160;            <span class="keyword">def </span>closure(x):</div><div class="line"><a name="l02423"></a><span class="lineno"> 2423</span>&#160;                <span class="keywordflow">if</span> prepro_fn <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l02424"></a><span class="lineno"> 2424</span>&#160;                    x = prepro_fn(x)</div><div class="line"><a name="l02425"></a><span class="lineno"> 2425</span>&#160;                <span class="keywordflow">return</span> x.as_strided(size, strides, offset)</div><div class="line"><a name="l02426"></a><span class="lineno"> 2426</span>&#160;</div><div class="line"><a name="l02427"></a><span class="lineno"> 2427</span>&#160;            gradcheck(closure, [x])</div><div class="line"><a name="l02428"></a><span class="lineno"> 2428</span>&#160;            gradgradcheck(closure, [x])</div><div class="line"><a name="l02429"></a><span class="lineno"> 2429</span>&#160;</div><div class="line"><a name="l02430"></a><span class="lineno"> 2430</span>&#160;        <span class="comment"># test</span></div><div class="line"><a name="l02431"></a><span class="lineno"> 2431</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.arange(0, 25), <span class="keyword">lambda</span> x: x.view(5, 5), [3, 3], [6, 2], 2)</div><div class="line"><a name="l02432"></a><span class="lineno"> 2432</span>&#160;</div><div class="line"><a name="l02433"></a><span class="lineno"> 2433</span>&#160;        <span class="comment"># test crazy stride at dim with size 1 case</span></div><div class="line"><a name="l02434"></a><span class="lineno"> 2434</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(12), <span class="keywordtype">None</span>, [1, 2, 1, 5], [0, 5, 100, 1], 2)</div><div class="line"><a name="l02435"></a><span class="lineno"> 2435</span>&#160;</div><div class="line"><a name="l02436"></a><span class="lineno"> 2436</span>&#160;        <span class="comment"># test expand case</span></div><div class="line"><a name="l02437"></a><span class="lineno"> 2437</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(5), <span class="keywordtype">None</span>, [3, 3, 3], [0, 1, 0], 2)</div><div class="line"><a name="l02438"></a><span class="lineno"> 2438</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(5), <span class="keywordtype">None</span>, [3, 3, 3], [0, 0, 0], 4)</div><div class="line"><a name="l02439"></a><span class="lineno"> 2439</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(5), <span class="keyword">lambda</span> x: x.expand(5, 5), [5, 5], [0, 1], 0)</div><div class="line"><a name="l02440"></a><span class="lineno"> 2440</span>&#160;</div><div class="line"><a name="l02441"></a><span class="lineno"> 2441</span>&#160;        <span class="comment"># test non-expand overlapping case</span></div><div class="line"><a name="l02442"></a><span class="lineno"> 2442</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(35), <span class="keywordtype">None</span>, [6, 6], [5, 1], 2)</div><div class="line"><a name="l02443"></a><span class="lineno"> 2443</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(15), <span class="keywordtype">None</span>, [3, 2], [3, 6], 2)</div><div class="line"><a name="l02444"></a><span class="lineno"> 2444</span>&#160;</div><div class="line"><a name="l02445"></a><span class="lineno"> 2445</span>&#160;        <span class="comment"># test transpose case</span></div><div class="line"><a name="l02446"></a><span class="lineno"> 2446</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(3, 4), <span class="keywordtype">None</span>, [4, 3], [1, 4])</div><div class="line"><a name="l02447"></a><span class="lineno"> 2447</span>&#160;</div><div class="line"><a name="l02448"></a><span class="lineno"> 2448</span>&#160;        <span class="comment"># test &quot;getting things outside the input&quot; case</span></div><div class="line"><a name="l02449"></a><span class="lineno"> 2449</span>&#160;        x = torch.randn(6, 2)</div><div class="line"><a name="l02450"></a><span class="lineno"> 2450</span>&#160;        <a class="code" href="namespacetest.html">test</a>(x[3:], <span class="keywordtype">None</span>, [3, 2], [2, 1], 0)  <span class="comment"># should be all zeros</span></div><div class="line"><a name="l02451"></a><span class="lineno"> 2451</span>&#160;        self.assertEqual(x[3:].as_strided([3, 2], [2, 1], 0), x[:3])</div><div class="line"><a name="l02452"></a><span class="lineno"> 2452</span>&#160;</div><div class="line"><a name="l02453"></a><span class="lineno"> 2453</span>&#160;        <span class="comment"># test select on expanded input case</span></div><div class="line"><a name="l02454"></a><span class="lineno"> 2454</span>&#160;        <a class="code" href="namespacetest.html">test</a>(torch.randn(2, 3), <span class="keyword">lambda</span> x: x.expand(10, 2, 3), [2, 3], [3, 1], 0)</div><div class="line"><a name="l02455"></a><span class="lineno"> 2455</span>&#160;</div><div class="line"><a name="l02456"></a><span class="lineno"> 2456</span>&#160;    <span class="keyword">def </span>_test_where_functional(self, t):</div><div class="line"><a name="l02457"></a><span class="lineno"> 2457</span>&#160;        x = Variable(t(torch.randn(5, 5)), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02458"></a><span class="lineno"> 2458</span>&#160;        y = Variable(t(torch.randn(5, 5)), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02459"></a><span class="lineno"> 2459</span>&#160;        cond = Variable(t(mask_not_all_zeros((5, 5))), requires_grad=<span class="keyword">False</span>)</div><div class="line"><a name="l02460"></a><span class="lineno"> 2460</span>&#160;</div><div class="line"><a name="l02461"></a><span class="lineno"> 2461</span>&#160;        <span class="keyword">def </span>where(cond, x, y):</div><div class="line"><a name="l02462"></a><span class="lineno"> 2462</span>&#160;            <span class="keywordflow">return</span> torch.where(cond, x, y)</div><div class="line"><a name="l02463"></a><span class="lineno"> 2463</span>&#160;</div><div class="line"><a name="l02464"></a><span class="lineno"> 2464</span>&#160;        gradcheck(where, [cond, x, y], raise_exception=<span class="keyword">True</span>)</div><div class="line"><a name="l02465"></a><span class="lineno"> 2465</span>&#160;        gradgradcheck(where, [cond, x, y], [Variable(t(torch.randn(5, 5)))])</div><div class="line"><a name="l02466"></a><span class="lineno"> 2466</span>&#160;</div><div class="line"><a name="l02467"></a><span class="lineno"> 2467</span>&#160;        x = Variable(t(torch.randn(5, 1, 5)), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02468"></a><span class="lineno"> 2468</span>&#160;        y = Variable(t(torch.randn(5, 5, 1)), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02469"></a><span class="lineno"> 2469</span>&#160;        gradcheck(where, [cond, x, y], raise_exception=<span class="keyword">True</span>)</div><div class="line"><a name="l02470"></a><span class="lineno"> 2470</span>&#160;        gradgradcheck(where, [cond, x, y], [Variable(t(torch.randn(5, 5, 5)))])</div><div class="line"><a name="l02471"></a><span class="lineno"> 2471</span>&#160;</div><div class="line"><a name="l02472"></a><span class="lineno"> 2472</span>&#160;    <span class="keyword">def </span>test_where_functional(self):</div><div class="line"><a name="l02473"></a><span class="lineno"> 2473</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a374f1759fc0be246b2bfa3375feddfdf">_test_where_functional</a>(<span class="keyword">lambda</span> t: t)</div><div class="line"><a name="l02474"></a><span class="lineno"> 2474</span>&#160;</div><div class="line"><a name="l02475"></a><span class="lineno"> 2475</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>(), <span class="stringliteral">&quot;CUDA unavailable&quot;</span>)</div><div class="line"><a name="l02476"></a><span class="lineno"> 2476</span>&#160;    <span class="keyword">def </span>test_where_functional_cuda(self):</div><div class="line"><a name="l02477"></a><span class="lineno"> 2477</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a374f1759fc0be246b2bfa3375feddfdf">_test_where_functional</a>(<span class="keyword">lambda</span> t: t.cuda())</div><div class="line"><a name="l02478"></a><span class="lineno"> 2478</span>&#160;</div><div class="line"><a name="l02479"></a><span class="lineno"> 2479</span>&#160;    <span class="keyword">def </span>_test_lerp_tensor_weights(self, cast):</div><div class="line"><a name="l02480"></a><span class="lineno"> 2480</span>&#160;        <span class="keyword">def </span>construct_inputs(*shapes):</div><div class="line"><a name="l02481"></a><span class="lineno"> 2481</span>&#160;            start = cast(torch.randn(shapes[0])).requires_grad_()</div><div class="line"><a name="l02482"></a><span class="lineno"> 2482</span>&#160;            end = cast(torch.randn(shapes[1])).requires_grad_()</div><div class="line"><a name="l02483"></a><span class="lineno"> 2483</span>&#160;            weight = cast(torch.randn(shapes[2]))</div><div class="line"><a name="l02484"></a><span class="lineno"> 2484</span>&#160;            <span class="keywordflow">return</span> [start, end, weight]</div><div class="line"><a name="l02485"></a><span class="lineno"> 2485</span>&#160;</div><div class="line"><a name="l02486"></a><span class="lineno"> 2486</span>&#160;        all_test_shapes = [((3, 3, 3), (3, 3, 3), (3, 3, 3)),  <span class="comment"># no broadcasting</span></div><div class="line"><a name="l02487"></a><span class="lineno"> 2487</span>&#160;                           ((3,), (3, 3, 3), (3, 3, 3)),  <span class="comment"># start broadcasting - 1</span></div><div class="line"><a name="l02488"></a><span class="lineno"> 2488</span>&#160;                           ((3, 3, 3), (3,), (3, 3, 3)),  <span class="comment"># end broadcasting - 1</span></div><div class="line"><a name="l02489"></a><span class="lineno"> 2489</span>&#160;                           ((3, 3, 3), (3, 3, 3), (3,)),  <span class="comment"># weight broadcasting - 1</span></div><div class="line"><a name="l02490"></a><span class="lineno"> 2490</span>&#160;                           ((), (3, 3, 3), (3, 3, 3)),  <span class="comment"># start broadcasting - 2</span></div><div class="line"><a name="l02491"></a><span class="lineno"> 2491</span>&#160;                           ((3, 3, 3), (), (3, 3, 3)),  <span class="comment"># end broadcasting - 2</span></div><div class="line"><a name="l02492"></a><span class="lineno"> 2492</span>&#160;                           ((3, 3, 3), (3, 3, 3), ()),  <span class="comment"># weight broadcasting - 2</span></div><div class="line"><a name="l02493"></a><span class="lineno"> 2493</span>&#160;                           ((3, 3), (3, 3, 3), (3,))]  <span class="comment"># all broadcasting</span></div><div class="line"><a name="l02494"></a><span class="lineno"> 2494</span>&#160;</div><div class="line"><a name="l02495"></a><span class="lineno"> 2495</span>&#160;        <span class="keywordflow">for</span> shapes <span class="keywordflow">in</span> all_test_shapes:</div><div class="line"><a name="l02496"></a><span class="lineno"> 2496</span>&#160;            cur_inputs = construct_inputs(*shapes)</div><div class="line"><a name="l02497"></a><span class="lineno"> 2497</span>&#160;            gradcheck(torch.lerp, cur_inputs)</div><div class="line"><a name="l02498"></a><span class="lineno"> 2498</span>&#160;            gradgradcheck(torch.lerp, cur_inputs)</div><div class="line"><a name="l02499"></a><span class="lineno"> 2499</span>&#160;</div><div class="line"><a name="l02500"></a><span class="lineno"> 2500</span>&#160;    <span class="keyword">def </span>test_lerp_tensor_weights(self):</div><div class="line"><a name="l02501"></a><span class="lineno"> 2501</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#aa35d961c325d8da1556f0cfd2c4773f9">_test_lerp_tensor_weights</a>(<span class="keyword">lambda</span> t: t)</div><div class="line"><a name="l02502"></a><span class="lineno"> 2502</span>&#160;</div><div class="line"><a name="l02503"></a><span class="lineno"> 2503</span>&#160;    <span class="keyword">def </span>test_reduce_dtype(self):</div><div class="line"><a name="l02504"></a><span class="lineno"> 2504</span>&#160;        <span class="keyword">def </span>test_reduction(op, has_no_dim):</div><div class="line"><a name="l02505"></a><span class="lineno"> 2505</span>&#160;            x = torch.randn(3, 3, dtype=torch.float, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02506"></a><span class="lineno"> 2506</span>&#160;</div><div class="line"><a name="l02507"></a><span class="lineno"> 2507</span>&#160;            <span class="keywordflow">if</span> has_no_dim:</div><div class="line"><a name="l02508"></a><span class="lineno"> 2508</span>&#160;                grad1, = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>([op(x)], [x])</div><div class="line"><a name="l02509"></a><span class="lineno"> 2509</span>&#160;                grad2, = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>([op(x, dtype=torch.double)], [x])</div><div class="line"><a name="l02510"></a><span class="lineno"> 2510</span>&#160;                self.assertEqual(grad1, grad2)</div><div class="line"><a name="l02511"></a><span class="lineno"> 2511</span>&#160;                self.assertEqual(grad2.dtype, torch.float)</div><div class="line"><a name="l02512"></a><span class="lineno"> 2512</span>&#160;</div><div class="line"><a name="l02513"></a><span class="lineno"> 2513</span>&#160;            gi = torch.randn(op(x, dim=0).shape, dtype=torch.float)</div><div class="line"><a name="l02514"></a><span class="lineno"> 2514</span>&#160;            grad1, = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>([op(x, dim=0)], [x], gi)</div><div class="line"><a name="l02515"></a><span class="lineno"> 2515</span>&#160;            grad2, = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>([op(x, dim=0, dtype=torch.double)], [x], gi.double())</div><div class="line"><a name="l02516"></a><span class="lineno"> 2516</span>&#160;            self.assertEqual(grad1, grad2)</div><div class="line"><a name="l02517"></a><span class="lineno"> 2517</span>&#160;            self.assertEqual(grad2.dtype, torch.float)</div><div class="line"><a name="l02518"></a><span class="lineno"> 2518</span>&#160;</div><div class="line"><a name="l02519"></a><span class="lineno"> 2519</span>&#160;        test_reduction(torch.sum, <span class="keyword">True</span>)</div><div class="line"><a name="l02520"></a><span class="lineno"> 2520</span>&#160;        test_reduction(torch.prod, <span class="keyword">True</span>)</div><div class="line"><a name="l02521"></a><span class="lineno"> 2521</span>&#160;        test_reduction(torch.cumsum, <span class="keyword">False</span>)</div><div class="line"><a name="l02522"></a><span class="lineno"> 2522</span>&#160;        test_reduction(torch.cumprod, <span class="keyword">False</span>)</div><div class="line"><a name="l02523"></a><span class="lineno"> 2523</span>&#160;</div><div class="line"><a name="l02524"></a><span class="lineno"> 2524</span>&#160;    <span class="keyword">def </span>test_inplace_view_backprop_base(self):</div><div class="line"><a name="l02525"></a><span class="lineno"> 2525</span>&#160;        <span class="comment"># modify view and back-prop through base</span></div><div class="line"><a name="l02526"></a><span class="lineno"> 2526</span>&#160;        root = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02527"></a><span class="lineno"> 2527</span>&#160;        x = root.clone()</div><div class="line"><a name="l02528"></a><span class="lineno"> 2528</span>&#160;        v1 = x.narrow(0, 0, 1)</div><div class="line"><a name="l02529"></a><span class="lineno"> 2529</span>&#160;        v1.mul_(2)</div><div class="line"><a name="l02530"></a><span class="lineno"> 2530</span>&#160;        x.sum().backward()</div><div class="line"><a name="l02531"></a><span class="lineno"> 2531</span>&#160;        self.assertEqual(root.grad.data.tolist(), [[2, 2], [1, 1]])</div><div class="line"><a name="l02532"></a><span class="lineno"> 2532</span>&#160;</div><div class="line"><a name="l02533"></a><span class="lineno"> 2533</span>&#160;    <span class="keyword">def </span>test_inplace_view_backprop_view_of_view(self):</div><div class="line"><a name="l02534"></a><span class="lineno"> 2534</span>&#160;        <span class="comment"># modify view and backprop through view-of-view</span></div><div class="line"><a name="l02535"></a><span class="lineno"> 2535</span>&#160;        root = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02536"></a><span class="lineno"> 2536</span>&#160;        x = root.clone()</div><div class="line"><a name="l02537"></a><span class="lineno"> 2537</span>&#160;        v1 = x.narrow(0, 0, 1)</div><div class="line"><a name="l02538"></a><span class="lineno"> 2538</span>&#160;        v2 = x.narrow(0, 0, 1)</div><div class="line"><a name="l02539"></a><span class="lineno"> 2539</span>&#160;        v1.mul_(2)</div><div class="line"><a name="l02540"></a><span class="lineno"> 2540</span>&#160;        v2.sum().backward()</div><div class="line"><a name="l02541"></a><span class="lineno"> 2541</span>&#160;        self.assertEqual(root.grad.data.tolist(), [[2, 2], [0, 0]])</div><div class="line"><a name="l02542"></a><span class="lineno"> 2542</span>&#160;</div><div class="line"><a name="l02543"></a><span class="lineno"> 2543</span>&#160;    <span class="keyword">def </span>test_inplace_view_of_view(self):</div><div class="line"><a name="l02544"></a><span class="lineno"> 2544</span>&#160;        <span class="comment"># modify view-of-view and backprop through base</span></div><div class="line"><a name="l02545"></a><span class="lineno"> 2545</span>&#160;        root = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02546"></a><span class="lineno"> 2546</span>&#160;        x = root.clone()</div><div class="line"><a name="l02547"></a><span class="lineno"> 2547</span>&#160;        v1 = x.narrow(0, 0, 1)</div><div class="line"><a name="l02548"></a><span class="lineno"> 2548</span>&#160;        v2 = v1.narrow(1, 1, 1)</div><div class="line"><a name="l02549"></a><span class="lineno"> 2549</span>&#160;        v2.mul_(2)</div><div class="line"><a name="l02550"></a><span class="lineno"> 2550</span>&#160;        x.sum().backward()</div><div class="line"><a name="l02551"></a><span class="lineno"> 2551</span>&#160;        self.assertEqual(root.grad.data.tolist(), [[1, 2], [1, 1]])</div><div class="line"><a name="l02552"></a><span class="lineno"> 2552</span>&#160;</div><div class="line"><a name="l02553"></a><span class="lineno"> 2553</span>&#160;    <span class="keyword">def </span>test_inplace_view_gradcheck(self):</div><div class="line"><a name="l02554"></a><span class="lineno"> 2554</span>&#160;        <span class="comment"># gradcheck modifications to views</span></div><div class="line"><a name="l02555"></a><span class="lineno"> 2555</span>&#160;        a = torch.randn(4, 4, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02556"></a><span class="lineno"> 2556</span>&#160;        b = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02557"></a><span class="lineno"> 2557</span>&#160;</div><div class="line"><a name="l02558"></a><span class="lineno"> 2558</span>&#160;        <span class="keyword">def </span>func(root, b):</div><div class="line"><a name="l02559"></a><span class="lineno"> 2559</span>&#160;            x = root.clone()</div><div class="line"><a name="l02560"></a><span class="lineno"> 2560</span>&#160;            x.narrow(1, 2, 2).narrow(0, 1, 2).mul_(b)</div><div class="line"><a name="l02561"></a><span class="lineno"> 2561</span>&#160;            x.narrow(1, 0, 2).narrow(0, 1, 2).mul_(b)</div><div class="line"><a name="l02562"></a><span class="lineno"> 2562</span>&#160;            <span class="keywordflow">return</span> x</div><div class="line"><a name="l02563"></a><span class="lineno"> 2563</span>&#160;</div><div class="line"><a name="l02564"></a><span class="lineno"> 2564</span>&#160;        gradcheck(func, [a, b], raise_exception=<span class="keyword">True</span>)</div><div class="line"><a name="l02565"></a><span class="lineno"> 2565</span>&#160;        go = torch.randn(a.size(), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02566"></a><span class="lineno"> 2566</span>&#160;        gradgradcheck(func, (a, b), (go,))</div><div class="line"><a name="l02567"></a><span class="lineno"> 2567</span>&#160;</div><div class="line"><a name="l02568"></a><span class="lineno"> 2568</span>&#160;    <span class="keyword">def </span>test_inplace_view_makes_base_require_grad(self):</div><div class="line"><a name="l02569"></a><span class="lineno"> 2569</span>&#160;        <span class="comment"># in-place modification to view makes base require grad</span></div><div class="line"><a name="l02570"></a><span class="lineno"> 2570</span>&#160;        a = torch.randn(4, 4, requires_grad=<span class="keyword">False</span>)</div><div class="line"><a name="l02571"></a><span class="lineno"> 2571</span>&#160;        b = torch.randn(4, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02572"></a><span class="lineno"> 2572</span>&#160;</div><div class="line"><a name="l02573"></a><span class="lineno"> 2573</span>&#160;        <span class="keyword">def </span>func(root, b):</div><div class="line"><a name="l02574"></a><span class="lineno"> 2574</span>&#160;            x = root.clone()</div><div class="line"><a name="l02575"></a><span class="lineno"> 2575</span>&#160;            self.assertFalse(x.requires_grad)</div><div class="line"><a name="l02576"></a><span class="lineno"> 2576</span>&#160;            x.narrow(1, 2, 2).mul_(b)</div><div class="line"><a name="l02577"></a><span class="lineno"> 2577</span>&#160;            self.assertTrue(x.requires_grad)</div><div class="line"><a name="l02578"></a><span class="lineno"> 2578</span>&#160;            <span class="keywordflow">return</span> x</div><div class="line"><a name="l02579"></a><span class="lineno"> 2579</span>&#160;</div><div class="line"><a name="l02580"></a><span class="lineno"> 2580</span>&#160;        gradcheck(func, [a, b], raise_exception=<span class="keyword">True</span>)</div><div class="line"><a name="l02581"></a><span class="lineno"> 2581</span>&#160;        go = torch.randn(a.size(), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02582"></a><span class="lineno"> 2582</span>&#160;        gradgradcheck(func, (a, b), (go,))</div><div class="line"><a name="l02583"></a><span class="lineno"> 2583</span>&#160;</div><div class="line"><a name="l02584"></a><span class="lineno"> 2584</span>&#160;    <span class="keyword">def </span>test_inplace_view_backprop_view(self):</div><div class="line"><a name="l02585"></a><span class="lineno"> 2585</span>&#160;        <span class="comment"># modify view and backprop through view</span></div><div class="line"><a name="l02586"></a><span class="lineno"> 2586</span>&#160;        a = Variable(torch.Tensor([2, 5]), requires_grad=<span class="keyword">False</span>)</div><div class="line"><a name="l02587"></a><span class="lineno"> 2587</span>&#160;        b = Variable(torch.Tensor([3]), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02588"></a><span class="lineno"> 2588</span>&#160;        res = a.narrow(0, 1, 1).mul_(b)</div><div class="line"><a name="l02589"></a><span class="lineno"> 2589</span>&#160;        res.sum().backward()</div><div class="line"><a name="l02590"></a><span class="lineno"> 2590</span>&#160;        self.assertEqual(b.grad.data.tolist(), [5])</div><div class="line"><a name="l02591"></a><span class="lineno"> 2591</span>&#160;        self.assertIsNone(a.grad)</div><div class="line"><a name="l02592"></a><span class="lineno"> 2592</span>&#160;</div><div class="line"><a name="l02593"></a><span class="lineno"> 2593</span>&#160;    <span class="keyword">def </span>test_inplace_view_modify_base(self):</div><div class="line"><a name="l02594"></a><span class="lineno"> 2594</span>&#160;        <span class="comment"># Test that an in-place operation on a base that forced it to require</span></div><div class="line"><a name="l02595"></a><span class="lineno"> 2595</span>&#160;        <span class="comment"># grad also forces any previous views to require grad and backprop</span></div><div class="line"><a name="l02596"></a><span class="lineno"> 2596</span>&#160;        <span class="comment"># correctly</span></div><div class="line"><a name="l02597"></a><span class="lineno"> 2597</span>&#160;        r = torch.ones(1, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02598"></a><span class="lineno"> 2598</span>&#160;</div><div class="line"><a name="l02599"></a><span class="lineno"> 2599</span>&#160;        <span class="keyword">def </span>fn(r):</div><div class="line"><a name="l02600"></a><span class="lineno"> 2600</span>&#160;            x = torch.ones(5)</div><div class="line"><a name="l02601"></a><span class="lineno"> 2601</span>&#160;            v = x.select(0, 1)</div><div class="line"><a name="l02602"></a><span class="lineno"> 2602</span>&#160;            self.assertFalse(v.requires_grad)</div><div class="line"><a name="l02603"></a><span class="lineno"> 2603</span>&#160;            self.assertIsNone(v.grad_fn)</div><div class="line"><a name="l02604"></a><span class="lineno"> 2604</span>&#160;            x.add_(r)  <span class="comment"># v is now dependent on r due to the in-place op on x</span></div><div class="line"><a name="l02605"></a><span class="lineno"> 2605</span>&#160;            self.assertTrue(v.requires_grad)</div><div class="line"><a name="l02606"></a><span class="lineno"> 2606</span>&#160;            <span class="keywordflow">return</span> v</div><div class="line"><a name="l02607"></a><span class="lineno"> 2607</span>&#160;</div><div class="line"><a name="l02608"></a><span class="lineno"> 2608</span>&#160;        gradcheck(fn, [r])</div><div class="line"><a name="l02609"></a><span class="lineno"> 2609</span>&#160;        gradgradcheck(fn, [r])</div><div class="line"><a name="l02610"></a><span class="lineno"> 2610</span>&#160;</div><div class="line"><a name="l02611"></a><span class="lineno"> 2611</span>&#160;    <span class="keyword">def </span>test_inplace_view_python(self):</div><div class="line"><a name="l02612"></a><span class="lineno"> 2612</span>&#160;        <span class="comment"># in-place modifications of Python-autograd created view</span></div><div class="line"><a name="l02613"></a><span class="lineno"> 2613</span>&#160;        a = torch.randn(4, 4, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02614"></a><span class="lineno"> 2614</span>&#160;        b = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02615"></a><span class="lineno"> 2615</span>&#160;</div><div class="line"><a name="l02616"></a><span class="lineno"> 2616</span>&#160;        <span class="keyword">class </span>PyAdd(torch.autograd.Function):</div><div class="line"><a name="l02617"></a><span class="lineno"> 2617</span>&#160;            @staticmethod</div><div class="line"><a name="l02618"></a><span class="lineno"> 2618</span>&#160;            <span class="keyword">def </span>forward(ctx, x, y):</div><div class="line"><a name="l02619"></a><span class="lineno"> 2619</span>&#160;                ctx.mark_dirty(x)</div><div class="line"><a name="l02620"></a><span class="lineno"> 2620</span>&#160;                x.add_(y)</div><div class="line"><a name="l02621"></a><span class="lineno"> 2621</span>&#160;                <span class="keywordflow">return</span> x</div><div class="line"><a name="l02622"></a><span class="lineno"> 2622</span>&#160;</div><div class="line"><a name="l02623"></a><span class="lineno"> 2623</span>&#160;            @staticmethod</div><div class="line"><a name="l02624"></a><span class="lineno"> 2624</span>&#160;            <span class="keyword">def </span>backward(ctx, grad):</div><div class="line"><a name="l02625"></a><span class="lineno"> 2625</span>&#160;                <span class="keywordflow">return</span> grad, grad</div><div class="line"><a name="l02626"></a><span class="lineno"> 2626</span>&#160;</div><div class="line"><a name="l02627"></a><span class="lineno"> 2627</span>&#160;        <span class="keyword">def </span>func(root, b):</div><div class="line"><a name="l02628"></a><span class="lineno"> 2628</span>&#160;            x = root.clone()</div><div class="line"><a name="l02629"></a><span class="lineno"> 2629</span>&#160;            PyAdd.apply(x.narrow(1, 2, 2).narrow(0, 1, 2), b)</div><div class="line"><a name="l02630"></a><span class="lineno"> 2630</span>&#160;            PyAdd.apply(x.narrow(1, 0, 2).narrow(0, 1, 2), b)</div><div class="line"><a name="l02631"></a><span class="lineno"> 2631</span>&#160;            <span class="keywordflow">return</span> x</div><div class="line"><a name="l02632"></a><span class="lineno"> 2632</span>&#160;</div><div class="line"><a name="l02633"></a><span class="lineno"> 2633</span>&#160;        gradcheck(func, [a, b], raise_exception=<span class="keyword">True</span>)</div><div class="line"><a name="l02634"></a><span class="lineno"> 2634</span>&#160;        go = torch.randn(a.size(), requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02635"></a><span class="lineno"> 2635</span>&#160;        gradgradcheck(func, (a, b), (go,))</div><div class="line"><a name="l02636"></a><span class="lineno"> 2636</span>&#160;</div><div class="line"><a name="l02637"></a><span class="lineno"> 2637</span>&#160;    <span class="keyword">def </span>test_inplace_view_non_contig(self):</div><div class="line"><a name="l02638"></a><span class="lineno"> 2638</span>&#160;        data = torch.ones(2, 3, 2).select(2, 1).t()</div><div class="line"><a name="l02639"></a><span class="lineno"> 2639</span>&#160;        root = Variable(data, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02640"></a><span class="lineno"> 2640</span>&#160;        x = root.clone()</div><div class="line"><a name="l02641"></a><span class="lineno"> 2641</span>&#160;        v1 = x.narrow(0, 0, 1)</div><div class="line"><a name="l02642"></a><span class="lineno"> 2642</span>&#160;        v2 = v1.narrow(1, 1, 1)</div><div class="line"><a name="l02643"></a><span class="lineno"> 2643</span>&#160;        v2.mul_(2)</div><div class="line"><a name="l02644"></a><span class="lineno"> 2644</span>&#160;        x.sum().backward()</div><div class="line"><a name="l02645"></a><span class="lineno"> 2645</span>&#160;        self.assertEqual(root.grad.data.tolist(), [[1, 2], [1, 1], [1, 1]])</div><div class="line"><a name="l02646"></a><span class="lineno"> 2646</span>&#160;</div><div class="line"><a name="l02647"></a><span class="lineno"> 2647</span>&#160;    <span class="keyword">def </span>test_inplace_view_saved_output(self):</div><div class="line"><a name="l02648"></a><span class="lineno"> 2648</span>&#160;        <span class="comment"># Test an in-place operation on a view in which the in-place op saves</span></div><div class="line"><a name="l02649"></a><span class="lineno"> 2649</span>&#160;        <span class="comment"># its output. Previously, this created a reference cycle.</span></div><div class="line"><a name="l02650"></a><span class="lineno"> 2650</span>&#160;        dealloc = [0]</div><div class="line"><a name="l02651"></a><span class="lineno"> 2651</span>&#160;</div><div class="line"><a name="l02652"></a><span class="lineno"> 2652</span>&#160;        <span class="keyword">class </span>IncrementOnDelete(object):</div><div class="line"><a name="l02653"></a><span class="lineno"> 2653</span>&#160;            <span class="keyword">def </span>__del__(self):</div><div class="line"><a name="l02654"></a><span class="lineno"> 2654</span>&#160;                dealloc[0] += 1</div><div class="line"><a name="l02655"></a><span class="lineno"> 2655</span>&#160;</div><div class="line"><a name="l02656"></a><span class="lineno"> 2656</span>&#160;        <span class="keyword">def </span><a class="code" href="namespacetest.html">test</a>():</div><div class="line"><a name="l02657"></a><span class="lineno"> 2657</span>&#160;            root = torch.randn(3, 3, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02658"></a><span class="lineno"> 2658</span>&#160;            copy = root.clone()</div><div class="line"><a name="l02659"></a><span class="lineno"> 2659</span>&#160;            copy.grad_fn.register_hook(IncrementOnDelete())</div><div class="line"><a name="l02660"></a><span class="lineno"> 2660</span>&#160;            view = copy.view(9)</div><div class="line"><a name="l02661"></a><span class="lineno"> 2661</span>&#160;            <a class="code" href="torch_2nn_2functional_8py.html#a1425cdc9a08e40d8269b3f606660d611">torch.nn.functional.relu</a>(view, inplace=<span class="keyword">True</span>)</div><div class="line"><a name="l02662"></a><span class="lineno"> 2662</span>&#160;</div><div class="line"><a name="l02663"></a><span class="lineno"> 2663</span>&#160;        <a class="code" href="namespacetest.html">test</a>()</div><div class="line"><a name="l02664"></a><span class="lineno"> 2664</span>&#160;        self.assertEqual(dealloc[0], 1)</div><div class="line"><a name="l02665"></a><span class="lineno"> 2665</span>&#160;</div><div class="line"><a name="l02666"></a><span class="lineno"> 2666</span>&#160;    <span class="keyword">def </span>test_mul_out(self):</div><div class="line"><a name="l02667"></a><span class="lineno"> 2667</span>&#160;        a = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02668"></a><span class="lineno"> 2668</span>&#160;        b = torch.randn(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02669"></a><span class="lineno"> 2669</span>&#160;        x = torch.zeros_like(a)</div><div class="line"><a name="l02670"></a><span class="lineno"> 2670</span>&#160;</div><div class="line"><a name="l02671"></a><span class="lineno"> 2671</span>&#160;        <span class="comment"># out=... functions don&#39;t support automatic differentiation currently</span></div><div class="line"><a name="l02672"></a><span class="lineno"> 2672</span>&#160;        self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;out=&#39;</span>, <span class="keyword">lambda</span>: torch.mul(a, b, out=x))</div><div class="line"><a name="l02673"></a><span class="lineno"> 2673</span>&#160;</div><div class="line"><a name="l02674"></a><span class="lineno"> 2674</span>&#160;        <span class="comment"># the inputs can require grad if we&#39;re in no_grad() mode</span></div><div class="line"><a name="l02675"></a><span class="lineno"> 2675</span>&#160;        with torch.no_grad():</div><div class="line"><a name="l02676"></a><span class="lineno"> 2676</span>&#160;            torch.mul(a, b, out=x)</div><div class="line"><a name="l02677"></a><span class="lineno"> 2677</span>&#160;            self.assertEqual(x, a * b)</div><div class="line"><a name="l02678"></a><span class="lineno"> 2678</span>&#160;</div><div class="line"><a name="l02679"></a><span class="lineno"> 2679</span>&#160;    <span class="keyword">def </span>test_mul_out_result_requires_grad(self):</div><div class="line"><a name="l02680"></a><span class="lineno"> 2680</span>&#160;        a = torch.randn(2, 2)</div><div class="line"><a name="l02681"></a><span class="lineno"> 2681</span>&#160;        b = torch.randn(2, 2)</div><div class="line"><a name="l02682"></a><span class="lineno"> 2682</span>&#160;        x = torch.zeros(2, 2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02683"></a><span class="lineno"> 2683</span>&#160;        <span class="comment"># we should throw an exception if the output requires grad</span></div><div class="line"><a name="l02684"></a><span class="lineno"> 2684</span>&#160;        self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;out=&#39;</span>, <span class="keyword">lambda</span>: torch.mul(a, b, out=x))</div><div class="line"><a name="l02685"></a><span class="lineno"> 2685</span>&#160;</div><div class="line"><a name="l02686"></a><span class="lineno"> 2686</span>&#160;    <span class="keyword">def </span>test_diagonal_derivative_requires_grad(self):</div><div class="line"><a name="l02687"></a><span class="lineno"> 2687</span>&#160;        <span class="comment"># test that the backward requires grad</span></div><div class="line"><a name="l02688"></a><span class="lineno"> 2688</span>&#160;        <span class="comment"># we do this is because diagonal_backward uses inplace</span></div><div class="line"><a name="l02689"></a><span class="lineno"> 2689</span>&#160;        <span class="comment"># operations and gradgradcheck does not catch whether</span></div><div class="line"><a name="l02690"></a><span class="lineno"> 2690</span>&#160;        <span class="comment"># they works as expected (it will succeed even if</span></div><div class="line"><a name="l02691"></a><span class="lineno"> 2691</span>&#160;        <span class="comment"># the gradient has requires_grad == False</span></div><div class="line"><a name="l02692"></a><span class="lineno"> 2692</span>&#160;        a = torch.randn(5, 6, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02693"></a><span class="lineno"> 2693</span>&#160;        b = torch.diagonal(a)**2</div><div class="line"><a name="l02694"></a><span class="lineno"> 2694</span>&#160;        c = b.sum()</div><div class="line"><a name="l02695"></a><span class="lineno"> 2695</span>&#160;        d, = <a class="code" href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a>(c, a, retain_graph=<span class="keyword">True</span>, create_graph=<span class="keyword">True</span>)</div><div class="line"><a name="l02696"></a><span class="lineno"> 2696</span>&#160;        self.assertTrue(d.requires_grad)</div><div class="line"><a name="l02697"></a><span class="lineno"> 2697</span>&#160;</div><div class="line"><a name="l02698"></a><span class="lineno"> 2698</span>&#160;    @staticmethod</div><div class="line"><a name="l02699"></a><span class="lineno"> 2699</span>&#160;    <span class="keyword">def </span>_test_set_requires_grad_only_for_floats(self, cuda):</div><div class="line"><a name="l02700"></a><span class="lineno"> 2700</span>&#160;        dtypes = [torch.int64, torch.int32, torch.int16, torch.int8,</div><div class="line"><a name="l02701"></a><span class="lineno"> 2701</span>&#160;                  torch.float, torch.double]</div><div class="line"><a name="l02702"></a><span class="lineno"> 2702</span>&#160;        <span class="keywordflow">if</span> cuda:</div><div class="line"><a name="l02703"></a><span class="lineno"> 2703</span>&#160;            dtypes.append(torch.half)</div><div class="line"><a name="l02704"></a><span class="lineno"> 2704</span>&#160;</div><div class="line"><a name="l02705"></a><span class="lineno"> 2705</span>&#160;        <span class="keyword">def </span>f1(dt):</div><div class="line"><a name="l02706"></a><span class="lineno"> 2706</span>&#160;            a = torch.ones(1, dtype=dt, device=<span class="stringliteral">&#39;cuda&#39;</span> <span class="keywordflow">if</span> cuda <span class="keywordflow">else</span> <span class="stringliteral">&#39;cpu&#39;</span>)</div><div class="line"><a name="l02707"></a><span class="lineno"> 2707</span>&#160;            a.requires_grad_()</div><div class="line"><a name="l02708"></a><span class="lineno"> 2708</span>&#160;</div><div class="line"><a name="l02709"></a><span class="lineno"> 2709</span>&#160;        <span class="keyword">def </span>f2(dt):</div><div class="line"><a name="l02710"></a><span class="lineno"> 2710</span>&#160;            a = torch.ones(1, dtype=dt, device=<span class="stringliteral">&#39;cuda&#39;</span> <span class="keywordflow">if</span> cuda <span class="keywordflow">else</span> <span class="stringliteral">&#39;cpu&#39;</span>)</div><div class="line"><a name="l02711"></a><span class="lineno"> 2711</span>&#160;            a.requires_grad = <span class="keyword">True</span></div><div class="line"><a name="l02712"></a><span class="lineno"> 2712</span>&#160;</div><div class="line"><a name="l02713"></a><span class="lineno"> 2713</span>&#160;        <span class="keyword">def </span>f3(dt):</div><div class="line"><a name="l02714"></a><span class="lineno"> 2714</span>&#160;            torch.ones(1, dtype=dt, device=<span class="stringliteral">&#39;cuda&#39;</span> <span class="keywordflow">if</span> cuda <span class="keywordflow">else</span> <span class="stringliteral">&#39;cpu&#39;</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02715"></a><span class="lineno"> 2715</span>&#160;</div><div class="line"><a name="l02716"></a><span class="lineno"> 2716</span>&#160;        <span class="keywordflow">for</span> dt <span class="keywordflow">in</span> dtypes:</div><div class="line"><a name="l02717"></a><span class="lineno"> 2717</span>&#160;            a = torch.ones(1, dtype=dt, device=<span class="stringliteral">&#39;cuda&#39;</span> <span class="keywordflow">if</span> cuda <span class="keywordflow">else</span> <span class="stringliteral">&#39;cpu&#39;</span>)</div><div class="line"><a name="l02718"></a><span class="lineno"> 2718</span>&#160;            a.requires_grad = <span class="keyword">False</span>  <span class="comment"># should always work</span></div><div class="line"><a name="l02719"></a><span class="lineno"> 2719</span>&#160;            a.requires_grad_(<span class="keyword">False</span>)</div><div class="line"><a name="l02720"></a><span class="lineno"> 2720</span>&#160;</div><div class="line"><a name="l02721"></a><span class="lineno"> 2721</span>&#160;            <span class="keywordflow">for</span> f <span class="keywordflow">in</span> [f1, f2, f3]:</div><div class="line"><a name="l02722"></a><span class="lineno"> 2722</span>&#160;                <span class="keywordflow">if</span> dt.is_floating_point:</div><div class="line"><a name="l02723"></a><span class="lineno"> 2723</span>&#160;                    f(dt)</div><div class="line"><a name="l02724"></a><span class="lineno"> 2724</span>&#160;                <span class="keywordflow">else</span>:</div><div class="line"><a name="l02725"></a><span class="lineno"> 2725</span>&#160;                    with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;floating point&#39;</span>,</div><div class="line"><a name="l02726"></a><span class="lineno"> 2726</span>&#160;                                                msg=<span class="stringliteral">&quot;dt: {} device: {}&quot;</span>.format(a.dtype, a.device)):</div><div class="line"><a name="l02727"></a><span class="lineno"> 2727</span>&#160;                        f(dt)</div><div class="line"><a name="l02728"></a><span class="lineno"> 2728</span>&#160;</div><div class="line"><a name="l02729"></a><span class="lineno"> 2729</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>(), <span class="stringliteral">&quot;CUDA unavailable&quot;</span>)</div><div class="line"><a name="l02730"></a><span class="lineno"> 2730</span>&#160;    <span class="keyword">def </span>test_set_requires_grad_only_for_floats_cuda(self):</div><div class="line"><a name="l02731"></a><span class="lineno"> 2731</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a245f883a0b3daf962dc5f7bfa7657a06">_test_set_requires_grad_only_for_floats</a>(self, <span class="keyword">True</span>)</div><div class="line"><a name="l02732"></a><span class="lineno"> 2732</span>&#160;</div><div class="line"><a name="l02733"></a><span class="lineno"> 2733</span>&#160;    <span class="keyword">def </span>test_set_requires_grad_only_for_floats(self):</div><div class="line"><a name="l02734"></a><span class="lineno"> 2734</span>&#160;        self.<a class="code" href="classtest__autograd_1_1_test_autograd.html#a245f883a0b3daf962dc5f7bfa7657a06">_test_set_requires_grad_only_for_floats</a>(self, <span class="keyword">False</span>)</div><div class="line"><a name="l02735"></a><span class="lineno"> 2735</span>&#160;</div><div class="line"><a name="l02736"></a><span class="lineno"> 2736</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>(), <span class="stringliteral">&quot;CUDA unavailable&quot;</span>)</div><div class="line"><a name="l02737"></a><span class="lineno"> 2737</span>&#160;    <span class="keyword">def </span>test_rnn_backward_to_input_but_not_parameters_cuda(self):</div><div class="line"><a name="l02738"></a><span class="lineno"> 2738</span>&#160;        <span class="comment"># this checks whether it is possible to not require</span></div><div class="line"><a name="l02739"></a><span class="lineno"> 2739</span>&#160;        <span class="comment"># weight parameters, but require inputs, see #7722</span></div><div class="line"><a name="l02740"></a><span class="lineno"> 2740</span>&#160;        dev = torch.device(<span class="stringliteral">&#39;cuda&#39;</span>)</div><div class="line"><a name="l02741"></a><span class="lineno"> 2741</span>&#160;        l = torch.nn.LSTM(2, 3).to(dev)</div><div class="line"><a name="l02742"></a><span class="lineno"> 2742</span>&#160;        <span class="keywordflow">for</span> p <span class="keywordflow">in</span> l.parameters():</div><div class="line"><a name="l02743"></a><span class="lineno"> 2743</span>&#160;            p.requires_grad = <span class="keyword">False</span></div><div class="line"><a name="l02744"></a><span class="lineno"> 2744</span>&#160;        s = torch.randn(1, 1, 2, requires_grad=<span class="keyword">True</span>, device=dev)</div><div class="line"><a name="l02745"></a><span class="lineno"> 2745</span>&#160;        out, _ = l(s)</div><div class="line"><a name="l02746"></a><span class="lineno"> 2746</span>&#160;        out.sum().backward()</div><div class="line"><a name="l02747"></a><span class="lineno"> 2747</span>&#160;        self.assertFalse(s.grad <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">or</span> s.grad.abs().sum().item() == 0)</div><div class="line"><a name="l02748"></a><span class="lineno"> 2748</span>&#160;</div><div class="line"><a name="l02749"></a><span class="lineno"> 2749</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> <a class="code" href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a>(), <span class="stringliteral">&quot;CUDA unavailable&quot;</span>)</div><div class="line"><a name="l02750"></a><span class="lineno"> 2750</span>&#160;    <span class="keyword">def </span>test_lstmcell_backward_only_one_output_grad(self):</div><div class="line"><a name="l02751"></a><span class="lineno"> 2751</span>&#160;        <span class="comment"># checks that undefined gradients doen&#39;t hamper the backward</span></div><div class="line"><a name="l02752"></a><span class="lineno"> 2752</span>&#160;        <span class="comment"># see #11872</span></div><div class="line"><a name="l02753"></a><span class="lineno"> 2753</span>&#160;        dev = torch.device(<span class="stringliteral">&#39;cuda&#39;</span>)</div><div class="line"><a name="l02754"></a><span class="lineno"> 2754</span>&#160;        l = torch.nn.LSTMCell(2, 3).to(dev).double()</div><div class="line"><a name="l02755"></a><span class="lineno"> 2755</span>&#160;        s = torch.randn(1, 2, device=dev, dtype=torch.double, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02756"></a><span class="lineno"> 2756</span>&#160;        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(2):</div><div class="line"><a name="l02757"></a><span class="lineno"> 2757</span>&#160;            out = l(s)[i]</div><div class="line"><a name="l02758"></a><span class="lineno"> 2758</span>&#160;            out.sum().backward()</div><div class="line"><a name="l02759"></a><span class="lineno"> 2759</span>&#160;            self.assertFalse(s.grad <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">or</span> s.grad.abs().sum().item() == 0)</div><div class="line"><a name="l02760"></a><span class="lineno"> 2760</span>&#160;</div><div class="line"><a name="l02761"></a><span class="lineno"> 2761</span>&#160;    <span class="keyword">def </span>test_anomaly_detect_nan(self):</div><div class="line"><a name="l02762"></a><span class="lineno"> 2762</span>&#160;        size = 10</div><div class="line"><a name="l02763"></a><span class="lineno"> 2763</span>&#160;</div><div class="line"><a name="l02764"></a><span class="lineno"> 2764</span>&#160;        <span class="keyword">class </span>MyFunc(Function):</div><div class="line"><a name="l02765"></a><span class="lineno"> 2765</span>&#160;            @staticmethod</div><div class="line"><a name="l02766"></a><span class="lineno"> 2766</span>&#160;            <span class="keyword">def </span>forward(ctx, inp1, inp2, fail_0th):</div><div class="line"><a name="l02767"></a><span class="lineno"> 2767</span>&#160;                ctx.fail_0th = fail_0th</div><div class="line"><a name="l02768"></a><span class="lineno"> 2768</span>&#160;                <span class="keywordflow">return</span> inp1.sum(0, keepdim=<span class="keyword">True</span>)</div><div class="line"><a name="l02769"></a><span class="lineno"> 2769</span>&#160;</div><div class="line"><a name="l02770"></a><span class="lineno"> 2770</span>&#160;            @staticmethod</div><div class="line"><a name="l02771"></a><span class="lineno"> 2771</span>&#160;            <span class="keyword">def </span>backward(ctx, gO):</div><div class="line"><a name="l02772"></a><span class="lineno"> 2772</span>&#160;                gI = gO.clone().expand(size)</div><div class="line"><a name="l02773"></a><span class="lineno"> 2773</span>&#160;                gI[0] = 0</div><div class="line"><a name="l02774"></a><span class="lineno"> 2774</span>&#160;                gI[0] /= 0  <span class="comment"># Generate a nan</span></div><div class="line"><a name="l02775"></a><span class="lineno"> 2775</span>&#160;                <span class="keywordflow">if</span> ctx.fail_0th:</div><div class="line"><a name="l02776"></a><span class="lineno"> 2776</span>&#160;                    <span class="keywordflow">return</span> gI, <span class="keywordtype">None</span>, <span class="keywordtype">None</span></div><div class="line"><a name="l02777"></a><span class="lineno"> 2777</span>&#160;                <span class="keywordflow">else</span>:</div><div class="line"><a name="l02778"></a><span class="lineno"> 2778</span>&#160;                    <span class="keywordflow">return</span> <span class="keywordtype">None</span>, gI, <span class="keywordtype">None</span></div><div class="line"><a name="l02779"></a><span class="lineno"> 2779</span>&#160;</div><div class="line"><a name="l02780"></a><span class="lineno"> 2780</span>&#160;        inp = torch.rand(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02781"></a><span class="lineno"> 2781</span>&#160;        out = MyFunc.apply(inp, inp, <span class="keyword">True</span>)</div><div class="line"><a name="l02782"></a><span class="lineno"> 2782</span>&#160;        out.backward()  <span class="comment"># Should not fail</span></div><div class="line"><a name="l02783"></a><span class="lineno"> 2783</span>&#160;</div><div class="line"><a name="l02784"></a><span class="lineno"> 2784</span>&#160;        inp = torch.rand(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02785"></a><span class="lineno"> 2785</span>&#160;        out = MyFunc.apply(inp, inp, <span class="keyword">True</span>)</div><div class="line"><a name="l02786"></a><span class="lineno"> 2786</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&quot;Function &#39;MyFuncBackward&#39; returned nan values in its 0th output.&quot;</span>):</div><div class="line"><a name="l02787"></a><span class="lineno"> 2787</span>&#160;            with warnings.catch_warnings(record=<span class="keyword">True</span>) <span class="keyword">as</span> w:</div><div class="line"><a name="l02788"></a><span class="lineno"> 2788</span>&#160;                with detect_anomaly():</div><div class="line"><a name="l02789"></a><span class="lineno"> 2789</span>&#160;                    out.backward()</div><div class="line"><a name="l02790"></a><span class="lineno"> 2790</span>&#160;            self.assertIn(<span class="stringliteral">&#39;No forward pass information&#39;</span>, str(w[0].message))</div><div class="line"><a name="l02791"></a><span class="lineno"> 2791</span>&#160;</div><div class="line"><a name="l02792"></a><span class="lineno"> 2792</span>&#160;        inp = torch.rand(size, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02793"></a><span class="lineno"> 2793</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&quot;Function &#39;MyFuncBackward&#39; returned nan values in its 1th output.&quot;</span>):</div><div class="line"><a name="l02794"></a><span class="lineno"> 2794</span>&#160;            with warnings.catch_warnings(record=<span class="keyword">True</span>) <span class="keyword">as</span> w:</div><div class="line"><a name="l02795"></a><span class="lineno"> 2795</span>&#160;                with detect_anomaly():</div><div class="line"><a name="l02796"></a><span class="lineno"> 2796</span>&#160;                    out = MyFunc.apply(inp, inp, <span class="keyword">False</span>)</div><div class="line"><a name="l02797"></a><span class="lineno"> 2797</span>&#160;                    out.backward()</div><div class="line"><a name="l02798"></a><span class="lineno"> 2798</span>&#160;            self.assertIn(<span class="stringliteral">&#39;MyFunc.apply&#39;</span>, str(w[0].message))</div><div class="line"><a name="l02799"></a><span class="lineno"> 2799</span>&#160;</div><div class="line"><a name="l02800"></a><span class="lineno"> 2800</span>&#160;    @skipIfNoLapack</div><div class="line"><a name="l02801"></a><span class="lineno"> 2801</span>&#160;    <span class="keyword">def </span>test_symeig_no_eigenvectors(self):</div><div class="line"><a name="l02802"></a><span class="lineno"> 2802</span>&#160;        A = <a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>([[1., 2.], [2., 4.]], dtype=torch.float32, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02803"></a><span class="lineno"> 2803</span>&#160;        w, v = torch.symeig(A, eigenvectors=<span class="keyword">False</span>)</div><div class="line"><a name="l02804"></a><span class="lineno"> 2804</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;cannot compute backward&#39;</span>):</div><div class="line"><a name="l02805"></a><span class="lineno"> 2805</span>&#160;            <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([w, v], [torch.ones_like(w), torch.ones_like(v)])</div><div class="line"><a name="l02806"></a><span class="lineno"> 2806</span>&#160;</div><div class="line"><a name="l02807"></a><span class="lineno"> 2807</span>&#160;    @skipIfNoLapack</div><div class="line"><a name="l02808"></a><span class="lineno"> 2808</span>&#160;    <span class="keyword">def </span>test_svd_no_singularvectors(self):</div><div class="line"><a name="l02809"></a><span class="lineno"> 2809</span>&#160;        A = torch.randn(2, 2, dtype=torch.float32, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02810"></a><span class="lineno"> 2810</span>&#160;        u, s, v = torch.svd(A, compute_uv=<span class="keyword">False</span>)</div><div class="line"><a name="l02811"></a><span class="lineno"> 2811</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;cannot compute backward&#39;</span>):</div><div class="line"><a name="l02812"></a><span class="lineno"> 2812</span>&#160;            <a class="code" href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a>([u, s, v], [torch.ones_like(u), torch.ones_like(s), torch.ones_like(v)])</div><div class="line"><a name="l02813"></a><span class="lineno"> 2813</span>&#160;</div><div class="line"><a name="l02814"></a><span class="lineno"> 2814</span>&#160;    <span class="keyword">def </span>test_no_grad_copy(self):</div><div class="line"><a name="l02815"></a><span class="lineno"> 2815</span>&#160;        <span class="comment"># create autograd function that saves grad pointer as class static</span></div><div class="line"><a name="l02816"></a><span class="lineno"> 2816</span>&#160;        <span class="keyword">class </span>MyFunc(Function):</div><div class="line"><a name="l02817"></a><span class="lineno"> 2817</span>&#160;            static_grad_ptr = <span class="keywordtype">None</span></div><div class="line"><a name="l02818"></a><span class="lineno"> 2818</span>&#160;</div><div class="line"><a name="l02819"></a><span class="lineno"> 2819</span>&#160;            @staticmethod</div><div class="line"><a name="l02820"></a><span class="lineno"> 2820</span>&#160;            <span class="keyword">def </span>forward(ctx, inp1, inp2):</div><div class="line"><a name="l02821"></a><span class="lineno"> 2821</span>&#160;                <span class="keywordflow">return</span> inp1 + inp2</div><div class="line"><a name="l02822"></a><span class="lineno"> 2822</span>&#160;</div><div class="line"><a name="l02823"></a><span class="lineno"> 2823</span>&#160;            @staticmethod</div><div class="line"><a name="l02824"></a><span class="lineno"> 2824</span>&#160;            <span class="keyword">def </span>backward(ctx, grad):</div><div class="line"><a name="l02825"></a><span class="lineno"> 2825</span>&#160;                MyFunc.static_grad_ptr = grad.data_ptr()</div><div class="line"><a name="l02826"></a><span class="lineno"> 2826</span>&#160;                <span class="keywordflow">return</span> grad, grad</div><div class="line"><a name="l02827"></a><span class="lineno"> 2827</span>&#160;</div><div class="line"><a name="l02828"></a><span class="lineno"> 2828</span>&#160;        <span class="keyword">class </span>NonContGradFunc(Function):</div><div class="line"><a name="l02829"></a><span class="lineno"> 2829</span>&#160;            @staticmethod</div><div class="line"><a name="l02830"></a><span class="lineno"> 2830</span>&#160;            <span class="keyword">def </span>forward(ctx, inp1):</div><div class="line"><a name="l02831"></a><span class="lineno"> 2831</span>&#160;                ctx.size = inp1.size()</div><div class="line"><a name="l02832"></a><span class="lineno"> 2832</span>&#160;                <span class="keywordflow">return</span> <a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>([1.])</div><div class="line"><a name="l02833"></a><span class="lineno"> 2833</span>&#160;</div><div class="line"><a name="l02834"></a><span class="lineno"> 2834</span>&#160;            @staticmethod</div><div class="line"><a name="l02835"></a><span class="lineno"> 2835</span>&#160;            <span class="keyword">def </span>backward(ctx, grad):</div><div class="line"><a name="l02836"></a><span class="lineno"> 2836</span>&#160;                <span class="keywordflow">return</span> torch.ones(1).expand(ctx.size)</div><div class="line"><a name="l02837"></a><span class="lineno"> 2837</span>&#160;</div><div class="line"><a name="l02838"></a><span class="lineno"> 2838</span>&#160;        a = torch.randn(5, 6, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02839"></a><span class="lineno"> 2839</span>&#160;        b = torch.randn(5, 6, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02840"></a><span class="lineno"> 2840</span>&#160;        <span class="comment"># non-contiguous grad should be copied</span></div><div class="line"><a name="l02841"></a><span class="lineno"> 2841</span>&#160;        NonContGradFunc.apply(MyFunc.apply(a, b)).backward()</div><div class="line"><a name="l02842"></a><span class="lineno"> 2842</span>&#160;        self.assertFalse(a.grad.data_ptr() == MyFunc.static_grad_ptr)</div><div class="line"><a name="l02843"></a><span class="lineno"> 2843</span>&#160;        self.assertFalse(b.grad.data_ptr() == MyFunc.static_grad_ptr)</div><div class="line"><a name="l02844"></a><span class="lineno"> 2844</span>&#160;        <span class="comment"># test case that should trigger no copy for one of a,b</span></div><div class="line"><a name="l02845"></a><span class="lineno"> 2845</span>&#160;        a.grad = b.grad = <span class="keywordtype">None</span></div><div class="line"><a name="l02846"></a><span class="lineno"> 2846</span>&#160;        MyFunc.apply(a, b)[1][0].backward()</div><div class="line"><a name="l02847"></a><span class="lineno"> 2847</span>&#160;        p_g = MyFunc.static_grad_ptr</div><div class="line"><a name="l02848"></a><span class="lineno"> 2848</span>&#160;        p_a = a.grad.data_ptr()</div><div class="line"><a name="l02849"></a><span class="lineno"> 2849</span>&#160;        p_b = b.grad.data_ptr()</div><div class="line"><a name="l02850"></a><span class="lineno"> 2850</span>&#160;        <span class="comment"># check a,b uses different grad buffer</span></div><div class="line"><a name="l02851"></a><span class="lineno"> 2851</span>&#160;        self.assertFalse(p_a == p_b)</div><div class="line"><a name="l02852"></a><span class="lineno"> 2852</span>&#160;        <span class="comment"># check one of them is using the computed buffer</span></div><div class="line"><a name="l02853"></a><span class="lineno"> 2853</span>&#160;        self.assertTrue(p_a == p_g <span class="keywordflow">or</span> p_b == p_g)</div><div class="line"><a name="l02854"></a><span class="lineno"> 2854</span>&#160;</div><div class="line"><a name="l02855"></a><span class="lineno"> 2855</span>&#160;    <span class="keyword">def </span>test_gradcheck_single_input(self):</div><div class="line"><a name="l02856"></a><span class="lineno"> 2856</span>&#160;        <span class="keyword">def </span>f(inp):</div><div class="line"><a name="l02857"></a><span class="lineno"> 2857</span>&#160;            <span class="keywordflow">return</span> inp.mul(5)</div><div class="line"><a name="l02858"></a><span class="lineno"> 2858</span>&#160;</div><div class="line"><a name="l02859"></a><span class="lineno"> 2859</span>&#160;        gradcheck(f, torch.rand(10, dtype=torch.float64, requires_grad=<span class="keyword">True</span>))</div><div class="line"><a name="l02860"></a><span class="lineno"> 2860</span>&#160;        gradgradcheck(f, torch.rand(10, dtype=torch.float64, requires_grad=<span class="keyword">True</span>))</div><div class="line"><a name="l02861"></a><span class="lineno"> 2861</span>&#160;</div><div class="line"><a name="l02862"></a><span class="lineno"> 2862</span>&#160;    <span class="keyword">def </span>test_gradcheck_sparse_input(self):</div><div class="line"><a name="l02863"></a><span class="lineno"> 2863</span>&#160;        <span class="keyword">def </span>fn(sparse):</div><div class="line"><a name="l02864"></a><span class="lineno"> 2864</span>&#160;            <span class="keywordflow">return</span> <a class="code" href="torch_2sparse_2____init_____8py.html#ade8a78138321665999db9cd6849c29a8">torch.sparse.sum</a>(sparse)</div><div class="line"><a name="l02865"></a><span class="lineno"> 2865</span>&#160;</div><div class="line"><a name="l02866"></a><span class="lineno"> 2866</span>&#160;        gradcheck(fn, torch.rand(10).to_sparse().requires_grad_(<span class="keyword">True</span>), check_sparse_nnz=<span class="keyword">True</span>)</div><div class="line"><a name="l02867"></a><span class="lineno"> 2867</span>&#160;        with self.assertRaisesRegex(RuntimeError, <span class="stringliteral">&#39;gradcheck expects all tensor inputs are dense&#39;</span>):</div><div class="line"><a name="l02868"></a><span class="lineno"> 2868</span>&#160;            gradcheck(fn, torch.rand(10).to_sparse().requires_grad_(<span class="keyword">True</span>), check_sparse_nnz=<span class="keyword">False</span>)</div><div class="line"><a name="l02869"></a><span class="lineno"> 2869</span>&#160;</div><div class="line"><a name="l02870"></a><span class="lineno"> 2870</span>&#160;    @unittest.skipIf(<span class="keywordflow">not</span> TEST_CUDA, <span class="stringliteral">&quot;Requires cuda for multi device&quot;</span>)</div><div class="line"><a name="l02871"></a><span class="lineno"> 2871</span>&#160;    <span class="keyword">def </span>test_multi_device_reentrant_autograd(self):</div><div class="line"><a name="l02872"></a><span class="lineno"> 2872</span>&#160;        <span class="comment"># Output on gpu so that this task will be associated with the gpu thread</span></div><div class="line"><a name="l02873"></a><span class="lineno"> 2873</span>&#160;        <span class="keyword">def </span>fn_on_gpu(inp):</div><div class="line"><a name="l02874"></a><span class="lineno"> 2874</span>&#160;            <span class="comment"># Artificially increase the priority of the next op to make sure it runs</span></div><div class="line"><a name="l02875"></a><span class="lineno"> 2875</span>&#160;            <span class="comment"># as soon as we reach it before the ops of branch1.</span></div><div class="line"><a name="l02876"></a><span class="lineno"> 2876</span>&#160;            dummy = inp * 2 * 2 * 2 * 2</div><div class="line"><a name="l02877"></a><span class="lineno"> 2877</span>&#160;            <span class="keywordflow">return</span> inp.cuda()</div><div class="line"><a name="l02878"></a><span class="lineno"> 2878</span>&#160;</div><div class="line"><a name="l02879"></a><span class="lineno"> 2879</span>&#160;        <span class="keyword">def </span>parent_on_cpu(inp):</div><div class="line"><a name="l02880"></a><span class="lineno"> 2880</span>&#160;            <span class="comment"># Slow branch of ops on gpu so that the work queue for the gpu thread</span></div><div class="line"><a name="l02881"></a><span class="lineno"> 2881</span>&#160;            <span class="comment"># won&#39;t empty too quickly. They also have smaller priorities than the</span></div><div class="line"><a name="l02882"></a><span class="lineno"> 2882</span>&#160;            <span class="comment"># ones created by fn_on_gpu</span></div><div class="line"><a name="l02883"></a><span class="lineno"> 2883</span>&#160;            branch1 = inp.cuda()</div><div class="line"><a name="l02884"></a><span class="lineno"> 2884</span>&#160;            branch1 = branch1 / branch1</div><div class="line"><a name="l02885"></a><span class="lineno"> 2885</span>&#160;            branch1 = branch1 / branch1</div><div class="line"><a name="l02886"></a><span class="lineno"> 2886</span>&#160;            branch1 = branch1 / branch1</div><div class="line"><a name="l02887"></a><span class="lineno"> 2887</span>&#160;            <span class="comment"># Perform checkpoint on cpu tensors. So the last op performed in the reentrant</span></div><div class="line"><a name="l02888"></a><span class="lineno"> 2888</span>&#160;            <span class="comment"># autograd is an AccumulateGrad that runs on the cpu thread for the gpu thread.</span></div><div class="line"><a name="l02889"></a><span class="lineno"> 2889</span>&#160;            <span class="comment"># So the cpu thread will notify the gpu thread with an empty FunctionTask.</span></div><div class="line"><a name="l02890"></a><span class="lineno"> 2890</span>&#160;            branch2 = <a class="code" href="namespacecheckpoint.html">checkpoint</a>(fn_on_gpu, inp)</div><div class="line"><a name="l02891"></a><span class="lineno"> 2891</span>&#160;            out = branch2 + branch1</div><div class="line"><a name="l02892"></a><span class="lineno"> 2892</span>&#160;            <span class="keywordflow">return</span> out</div><div class="line"><a name="l02893"></a><span class="lineno"> 2893</span>&#160;</div><div class="line"><a name="l02894"></a><span class="lineno"> 2894</span>&#160;        inp = torch.rand(2, requires_grad=<span class="keyword">True</span>)</div><div class="line"><a name="l02895"></a><span class="lineno"> 2895</span>&#160;        out = parent_on_cpu(inp)</div><div class="line"><a name="l02896"></a><span class="lineno"> 2896</span>&#160;        <span class="comment"># This will segfault if the empty FunctionTask is not handled properly in the</span></div><div class="line"><a name="l02897"></a><span class="lineno"> 2897</span>&#160;        <span class="comment"># gpu thread ReadyQueue</span></div><div class="line"><a name="l02898"></a><span class="lineno"> 2898</span>&#160;        out.sum().backward()</div><div class="line"><a name="l02899"></a><span class="lineno"> 2899</span>&#160;</div><div class="line"><a name="l02900"></a><span class="lineno"> 2900</span>&#160;</div><div class="line"><a name="l02901"></a><span class="lineno"> 2901</span>&#160;<span class="keyword">def </span>index_variable(shape, max_indices):</div><div class="line"><a name="l02902"></a><span class="lineno"> 2902</span>&#160;    <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(shape, tuple):</div><div class="line"><a name="l02903"></a><span class="lineno"> 2903</span>&#160;        shape = (shape,)</div><div class="line"><a name="l02904"></a><span class="lineno"> 2904</span>&#160;    index = torch.rand(*shape).mul_(max_indices).floor_().long()</div><div class="line"><a name="l02905"></a><span class="lineno"> 2905</span>&#160;    <span class="keywordflow">return</span> index</div><div class="line"><a name="l02906"></a><span class="lineno"> 2906</span>&#160;</div><div class="line"><a name="l02907"></a><span class="lineno"> 2907</span>&#160;</div><div class="line"><a name="l02908"></a><span class="lineno"> 2908</span>&#160;<span class="keyword">def </span>index_perm_variable(shape, max_indices):</div><div class="line"><a name="l02909"></a><span class="lineno"> 2909</span>&#160;    <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(shape, tuple):</div><div class="line"><a name="l02910"></a><span class="lineno"> 2910</span>&#160;        shape = (shape,)</div><div class="line"><a name="l02911"></a><span class="lineno"> 2911</span>&#160;</div><div class="line"><a name="l02912"></a><span class="lineno"> 2912</span>&#160;    index = torch.randperm(max_indices).narrow(0, 0, reduce(mul, shape)).view(shape)</div><div class="line"><a name="l02913"></a><span class="lineno"> 2913</span>&#160;    <span class="keywordflow">return</span> index</div><div class="line"><a name="l02914"></a><span class="lineno"> 2914</span>&#160;</div><div class="line"><a name="l02915"></a><span class="lineno"> 2915</span>&#160;</div><div class="line"><a name="l02916"></a><span class="lineno"> 2916</span>&#160;<span class="keyword">def </span>gather_variable(shape, index_dim, max_indices, duplicate=False):</div><div class="line"><a name="l02917"></a><span class="lineno"> 2917</span>&#160;    <span class="keyword">assert</span> len(shape) == 2</div><div class="line"><a name="l02918"></a><span class="lineno"> 2918</span>&#160;    <span class="keyword">assert</span> index_dim &lt; 2</div><div class="line"><a name="l02919"></a><span class="lineno"> 2919</span>&#160;    batch_dim = 1 - index_dim</div><div class="line"><a name="l02920"></a><span class="lineno"> 2920</span>&#160;    index = torch.LongTensor(*shape)</div><div class="line"><a name="l02921"></a><span class="lineno"> 2921</span>&#160;    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(shape[index_dim]):</div><div class="line"><a name="l02922"></a><span class="lineno"> 2922</span>&#160;        index.select(index_dim, i).copy_(</div><div class="line"><a name="l02923"></a><span class="lineno"> 2923</span>&#160;            torch.randperm(max_indices)[:shape[batch_dim]])</div><div class="line"><a name="l02924"></a><span class="lineno"> 2924</span>&#160;    <span class="keywordflow">if</span> duplicate:</div><div class="line"><a name="l02925"></a><span class="lineno"> 2925</span>&#160;        index.select(batch_dim, 0).copy_(index.select(batch_dim, 1))</div><div class="line"><a name="l02926"></a><span class="lineno"> 2926</span>&#160;    <span class="keywordflow">return</span> index</div><div class="line"><a name="l02927"></a><span class="lineno"> 2927</span>&#160;</div><div class="line"><a name="l02928"></a><span class="lineno"> 2928</span>&#160;</div><div class="line"><a name="l02929"></a><span class="lineno"> 2929</span>&#160;<span class="keyword">def </span>bernoulli_scalar():</div><div class="line"><a name="l02930"></a><span class="lineno"> 2930</span>&#160;    <span class="keywordflow">return</span> <a class="code" href="namespacetorch_1_1tensor.html">torch.tensor</a>(0, dtype=torch.uint8).bernoulli_()</div><div class="line"><a name="l02931"></a><span class="lineno"> 2931</span>&#160;</div><div class="line"><a name="l02932"></a><span class="lineno"> 2932</span>&#160;</div><div class="line"><a name="l02933"></a><span class="lineno"> 2933</span>&#160;<span class="keyword">def </span>gradgradcheck_method_precision_override(test_name):</div><div class="line"><a name="l02934"></a><span class="lineno"> 2934</span>&#160;    <span class="comment"># these are just empirical observations, we should improve</span></div><div class="line"><a name="l02935"></a><span class="lineno"> 2935</span>&#160;    gradgradcheck_precision_override = {</div><div class="line"><a name="l02936"></a><span class="lineno"> 2936</span>&#160;        <span class="stringliteral">&#39;test_norm&#39;</span>: {<span class="stringliteral">&#39;atol&#39;</span>: 2e-2, <span class="stringliteral">&#39;rtol&#39;</span>: 1e-2},</div><div class="line"><a name="l02937"></a><span class="lineno"> 2937</span>&#160;        <span class="stringliteral">&#39;test_norm_1_5&#39;</span>: {<span class="stringliteral">&#39;atol&#39;</span>: 1.5e-2, <span class="stringliteral">&#39;rtol&#39;</span>: 1e-2},</div><div class="line"><a name="l02938"></a><span class="lineno"> 2938</span>&#160;        <span class="stringliteral">&#39;test_norm_3&#39;</span>: {<span class="stringliteral">&#39;atol&#39;</span>: 5e-2, <span class="stringliteral">&#39;rtol&#39;</span>: 1e-2},</div><div class="line"><a name="l02939"></a><span class="lineno"> 2939</span>&#160;        <span class="stringliteral">&#39;test_dist&#39;</span>: {<span class="stringliteral">&#39;atol&#39;</span>: 5e-2, <span class="stringliteral">&#39;rtol&#39;</span>: 1e-2},</div><div class="line"><a name="l02940"></a><span class="lineno"> 2940</span>&#160;        <span class="stringliteral">&#39;test_dist_4&#39;</span>: {<span class="stringliteral">&#39;atol&#39;</span>: 8e-2, <span class="stringliteral">&#39;rtol&#39;</span>: 1e-2},</div><div class="line"><a name="l02941"></a><span class="lineno"> 2941</span>&#160;    }</div><div class="line"><a name="l02942"></a><span class="lineno"> 2942</span>&#160;    non_broadcasted_test_name = test_name.split(<span class="stringliteral">&quot;_broadcast&quot;</span>)[0]</div><div class="line"><a name="l02943"></a><span class="lineno"> 2943</span>&#160;    override = gradgradcheck_precision_override.get(non_broadcasted_test_name)</div><div class="line"><a name="l02944"></a><span class="lineno"> 2944</span>&#160;    <span class="keywordflow">if</span> override:</div><div class="line"><a name="l02945"></a><span class="lineno"> 2945</span>&#160;        <span class="keywordflow">if</span> <span class="stringliteral">&#39;broadcast_lhs&#39;</span> <span class="keywordflow">in</span> test_name <span class="keywordflow">or</span> <span class="stringliteral">&#39;broadcast_rhs&#39;</span> <span class="keywordflow">in</span> test_name:</div><div class="line"><a name="l02946"></a><span class="lineno"> 2946</span>&#160;            <span class="comment"># errors accumulated across 1 dimension</span></div><div class="line"><a name="l02947"></a><span class="lineno"> 2947</span>&#160;            override = {<span class="stringliteral">&#39;atol&#39;</span>: override[<span class="stringliteral">&#39;atol&#39;</span>] * S, <span class="stringliteral">&#39;rtol&#39;</span>: override[<span class="stringliteral">&#39;atol&#39;</span>] * S}</div><div class="line"><a name="l02948"></a><span class="lineno"> 2948</span>&#160;        <span class="keywordflow">elif</span> <span class="stringliteral">&#39;broadcast_all&#39;</span> <span class="keywordflow">in</span> test_name:</div><div class="line"><a name="l02949"></a><span class="lineno"> 2949</span>&#160;            <span class="comment"># errors accumulated across multiple dimensions</span></div><div class="line"><a name="l02950"></a><span class="lineno"> 2950</span>&#160;            override = {<span class="stringliteral">&#39;atol&#39;</span>: override[<span class="stringliteral">&#39;atol&#39;</span>] * S * S, <span class="stringliteral">&#39;rtol&#39;</span>: override[<span class="stringliteral">&#39;atol&#39;</span>] * S * S}</div><div class="line"><a name="l02951"></a><span class="lineno"> 2951</span>&#160;    <span class="keywordflow">return</span> override</div><div class="line"><a name="l02952"></a><span class="lineno"> 2952</span>&#160;</div><div class="line"><a name="l02953"></a><span class="lineno"> 2953</span>&#160;</div><div class="line"><a name="l02954"></a><span class="lineno"> 2954</span>&#160;<span class="keyword">def </span>run_grad_and_gradgrad_checks(test_case, name, test_name, apply_method, output_variable,</div><div class="line"><a name="l02955"></a><span class="lineno"> 2955</span>&#160;                                 input_variables, run_gradgradcheck=<span class="keyword">True</span>):</div><div class="line"><a name="l02956"></a><span class="lineno"> 2956</span>&#160;    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))</div><div class="line"><a name="l02957"></a><span class="lineno"> 2957</span>&#160;    <span class="keywordflow">if</span> name <span class="keywordflow">in</span> EXCLUDE_GRADGRADCHECK <span class="keywordflow">or</span> test_name <span class="keywordflow">in</span> EXCLUDE_GRADGRADCHECK_BY_TEST_NAME:</div><div class="line"><a name="l02958"></a><span class="lineno"> 2958</span>&#160;        <span class="keywordflow">return</span></div><div class="line"><a name="l02959"></a><span class="lineno"> 2959</span>&#160;    gradgradcheck_precision_override = gradgradcheck_method_precision_override(test_name)</div><div class="line"><a name="l02960"></a><span class="lineno"> 2960</span>&#160;    <span class="keywordflow">if</span> gradgradcheck_precision_override <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l02961"></a><span class="lineno"> 2961</span>&#160;        atol = gradgradcheck_precision_override[<span class="stringliteral">&#39;atol&#39;</span>]</div><div class="line"><a name="l02962"></a><span class="lineno"> 2962</span>&#160;        rtol = gradgradcheck_precision_override[<span class="stringliteral">&#39;rtol&#39;</span>]</div><div class="line"><a name="l02963"></a><span class="lineno"> 2963</span>&#160;        test_case.assertTrue(gradgradcheck(apply_method, input_variables, <span class="keywordtype">None</span>, atol=atol, rtol=rtol,</div><div class="line"><a name="l02964"></a><span class="lineno"> 2964</span>&#160;                                           gen_non_contig_grad_outputs=<span class="keyword">True</span>))</div><div class="line"><a name="l02965"></a><span class="lineno"> 2965</span>&#160;    <span class="keywordflow">else</span>:</div><div class="line"><a name="l02966"></a><span class="lineno"> 2966</span>&#160;        test_case.assertTrue(gradgradcheck(apply_method, input_variables, gen_non_contig_grad_outputs=<span class="keyword">True</span>))</div><div class="line"><a name="l02967"></a><span class="lineno"> 2967</span>&#160;</div><div class="line"><a name="l02968"></a><span class="lineno"> 2968</span>&#160;</div><div class="line"><a name="l02969"></a><span class="lineno"> 2969</span>&#160;<span class="keyword">def </span>run_functional_checks(test_case, test_name, name, apply_fn, run_grad_checks,</div><div class="line"><a name="l02970"></a><span class="lineno"> 2970</span>&#160;                          f_args_variable, f_args_tensor):</div><div class="line"><a name="l02971"></a><span class="lineno"> 2971</span>&#160;    output_variable = apply_fn(*f_args_variable)</div><div class="line"><a name="l02972"></a><span class="lineno"> 2972</span>&#160;</div><div class="line"><a name="l02973"></a><span class="lineno"> 2973</span>&#160;    <span class="keywordflow">if</span> run_grad_checks:</div><div class="line"><a name="l02974"></a><span class="lineno"> 2974</span>&#160;        run_grad_and_gradgrad_checks(test_case, name, test_name, apply_fn,</div><div class="line"><a name="l02975"></a><span class="lineno"> 2975</span>&#160;                                     output_variable, f_args_variable)</div><div class="line"><a name="l02976"></a><span class="lineno"> 2976</span>&#160;</div><div class="line"><a name="l02977"></a><span class="lineno"> 2977</span>&#160;    self_variable = f_args_variable[0]</div><div class="line"><a name="l02978"></a><span class="lineno"> 2978</span>&#160;    <span class="keywordflow">if</span> isinstance(output_variable, torch.Tensor) <span class="keywordflow">and</span> output_variable.requires_grad <span class="keywordflow">and</span> self_variable <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l02979"></a><span class="lineno"> 2979</span>&#160;        output_variable.backward(randn_like(output_variable))</div><div class="line"><a name="l02980"></a><span class="lineno"> 2980</span>&#160;        test_case.assertEqual(self_variable.type(), self_variable.grad.type())</div><div class="line"><a name="l02981"></a><span class="lineno"> 2981</span>&#160;        test_case.assertEqual(self_variable.size(), self_variable.grad.size())</div><div class="line"><a name="l02982"></a><span class="lineno"> 2982</span>&#160;</div><div class="line"><a name="l02983"></a><span class="lineno"> 2983</span>&#160;</div><div class="line"><a name="l02984"></a><span class="lineno"> 2984</span>&#160;<span class="keyword">def </span>add_test(</div><div class="line"><a name="l02985"></a><span class="lineno"> 2985</span>&#160;        name,</div><div class="line"><a name="l02986"></a><span class="lineno"> 2986</span>&#160;        self_size,</div><div class="line"><a name="l02987"></a><span class="lineno"> 2987</span>&#160;        args,</div><div class="line"><a name="l02988"></a><span class="lineno"> 2988</span>&#160;        variant_name=<span class="stringliteral">&#39;&#39;</span>,</div><div class="line"><a name="l02989"></a><span class="lineno"> 2989</span>&#160;        dim_args_idx=(),</div><div class="line"><a name="l02990"></a><span class="lineno"> 2990</span>&#160;        skipTestIf=(),</div><div class="line"><a name="l02991"></a><span class="lineno"> 2991</span>&#160;        output_process_fn=<span class="keyword">lambda</span> x: x,</div><div class="line"><a name="l02992"></a><span class="lineno"> 2992</span>&#160;        kwargs=<span class="keywordtype">None</span>):</div><div class="line"><a name="l02993"></a><span class="lineno"> 2993</span>&#160;    kwargs = kwargs <span class="keywordflow">if</span> kwargs <span class="keywordflow">else</span> {}</div><div class="line"><a name="l02994"></a><span class="lineno"> 2994</span>&#160;    basic_test_name = <span class="stringliteral">&#39;test_&#39;</span> + name</div><div class="line"><a name="l02995"></a><span class="lineno"> 2995</span>&#160;    <span class="keywordflow">if</span> variant_name != <span class="stringliteral">&#39;&#39;</span>:</div><div class="line"><a name="l02996"></a><span class="lineno"> 2996</span>&#160;        basic_test_name += <span class="stringliteral">&#39;_&#39;</span> + variant_name</div><div class="line"><a name="l02997"></a><span class="lineno"> 2997</span>&#160;</div><div class="line"><a name="l02998"></a><span class="lineno"> 2998</span>&#160;    <span class="keywordflow">for</span> dim_perm <span class="keywordflow">in</span> product([-1, 1], repeat=len(dim_args_idx)):</div><div class="line"><a name="l02999"></a><span class="lineno"> 2999</span>&#160;        test_name = basic_test_name</div><div class="line"><a name="l03000"></a><span class="lineno"> 3000</span>&#160;        new_args = [arg * dim_perm[dim_args_idx.index(i)] <span class="keywordflow">if</span> i <span class="keywordflow">in</span> dim_args_idx <span class="keywordflow">else</span> arg <span class="keywordflow">for</span> i, arg <span class="keywordflow">in</span> enumerate(args)]</div><div class="line"><a name="l03001"></a><span class="lineno"> 3001</span>&#160;        test_name = basic_test_name + <span class="stringliteral">&#39;&#39;</span>.join(<span class="stringliteral">&#39;_neg&#39;</span> + str(i) <span class="keywordflow">for</span> i, idx <span class="keywordflow">in</span> enumerate(dim_perm) <span class="keywordflow">if</span> idx &lt; 0)</div><div class="line"><a name="l03002"></a><span class="lineno"> 3002</span>&#160;        new_args = tuple(new_args)</div><div class="line"><a name="l03003"></a><span class="lineno"> 3003</span>&#160;</div><div class="line"><a name="l03004"></a><span class="lineno"> 3004</span>&#160;        <span class="comment"># for-loop bodies don&#39;t define scopes, so we have to save the variables</span></div><div class="line"><a name="l03005"></a><span class="lineno"> 3005</span>&#160;        <span class="comment"># we want to close over in some way</span></div><div class="line"><a name="l03006"></a><span class="lineno"> 3006</span>&#160;        <span class="keyword">def </span>do_test(self, name=name, self_size=self_size, args=new_args, test_name=test_name,</div><div class="line"><a name="l03007"></a><span class="lineno"> 3007</span>&#160;                    output_process_fn=output_process_fn):</div><div class="line"><a name="l03008"></a><span class="lineno"> 3008</span>&#160;            <span class="keyword">def </span>check(name):</div><div class="line"><a name="l03009"></a><span class="lineno"> 3009</span>&#160;                is_magic_method = name[:2] == <span class="stringliteral">&#39;__&#39;</span> <span class="keywordflow">and</span> name[-2:] == <span class="stringliteral">&#39;__&#39;</span></div><div class="line"><a name="l03010"></a><span class="lineno"> 3010</span>&#160;                is_inplace = name[-1] == <span class="stringliteral">&quot;_&quot;</span> <span class="keywordflow">and</span> <span class="keywordflow">not</span> is_magic_method</div><div class="line"><a name="l03011"></a><span class="lineno"> 3011</span>&#160;                self_variable = create_input((self_size,))[0][0]</div><div class="line"><a name="l03012"></a><span class="lineno"> 3012</span>&#160;                <span class="comment"># FixMe: run grad checks on inplace self</span></div><div class="line"><a name="l03013"></a><span class="lineno"> 3013</span>&#160;                <span class="keywordflow">if</span> is_inplace:</div><div class="line"><a name="l03014"></a><span class="lineno"> 3014</span>&#160;                    self_variable.requires_grad = <span class="keyword">False</span></div><div class="line"><a name="l03015"></a><span class="lineno"> 3015</span>&#160;                <span class="comment"># need to record this because methods can change the size (e.g. unsqueeze)</span></div><div class="line"><a name="l03016"></a><span class="lineno"> 3016</span>&#160;                args_variable, kwargs_variable = create_input(args, requires_grad=<span class="keywordflow">not</span> is_inplace, call_kwargs=kwargs)</div><div class="line"><a name="l03017"></a><span class="lineno"> 3017</span>&#160;                self_tensor = deepcopy(self_variable.data)</div><div class="line"><a name="l03018"></a><span class="lineno"> 3018</span>&#160;                args_tensor = deepcopy(unpack_variables(args_variable))</div><div class="line"><a name="l03019"></a><span class="lineno"> 3019</span>&#160;                output_variable = getattr(self_variable, name)(*args_variable, **kwargs_variable)</div><div class="line"><a name="l03020"></a><span class="lineno"> 3020</span>&#160;                <span class="keywordflow">if</span> <span class="keywordflow">not</span> exclude_tensor_method(name, test_name):</div><div class="line"><a name="l03021"></a><span class="lineno"> 3021</span>&#160;                    output_tensor = getattr(self_tensor, name)(*args_tensor, **kwargs_variable)</div><div class="line"><a name="l03022"></a><span class="lineno"> 3022</span>&#160;                    <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(output_tensor, torch.Tensor) <span class="keywordflow">and</span> <span class="keywordflow">not</span> istuple(output_tensor):</div><div class="line"><a name="l03023"></a><span class="lineno"> 3023</span>&#160;                        output_tensor = torch.DoubleTensor((output_tensor,))</div><div class="line"><a name="l03024"></a><span class="lineno"> 3024</span>&#160;                    self.assertEqual(unpack_variables(output_variable), output_tensor)</div><div class="line"><a name="l03025"></a><span class="lineno"> 3025</span>&#160;                    <span class="comment"># TODO: check that both have changed after adding all inplace ops</span></div><div class="line"><a name="l03026"></a><span class="lineno"> 3026</span>&#160;</div><div class="line"><a name="l03027"></a><span class="lineno"> 3027</span>&#160;                <span class="keyword">def </span>fn(*inputs):</div><div class="line"><a name="l03028"></a><span class="lineno"> 3028</span>&#160;                    output = getattr(inputs[0], name)(*inputs[1:], **kwargs)</div><div class="line"><a name="l03029"></a><span class="lineno"> 3029</span>&#160;                    <span class="keywordflow">return</span> output_process_fn(output)</div><div class="line"><a name="l03030"></a><span class="lineno"> 3030</span>&#160;</div><div class="line"><a name="l03031"></a><span class="lineno"> 3031</span>&#160;                <span class="keywordflow">if</span> <span class="keywordflow">not</span> is_inplace <span class="keywordflow">and</span> name <span class="keywordflow">not</span> <span class="keywordflow">in</span> EXCLUDE_GRADCHECK:</div><div class="line"><a name="l03032"></a><span class="lineno"> 3032</span>&#160;                    run_grad_and_gradgrad_checks(self, name, test_name, fn,</div><div class="line"><a name="l03033"></a><span class="lineno"> 3033</span>&#160;                                                 output_variable, (self_variable,) + args_variable)</div><div class="line"><a name="l03034"></a><span class="lineno"> 3034</span>&#160;</div><div class="line"><a name="l03035"></a><span class="lineno"> 3035</span>&#160;                <span class="comment"># functional interface tests</span></div><div class="line"><a name="l03036"></a><span class="lineno"> 3036</span>&#160;                <span class="keywordflow">if</span> hasattr(torch, name) <span class="keywordflow">and</span> name <span class="keywordflow">not</span> <span class="keywordflow">in</span> EXCLUDE_FUNCTIONAL:</div><div class="line"><a name="l03037"></a><span class="lineno"> 3037</span>&#160;                    <span class="keyword">def </span>fn(*inputs):</div><div class="line"><a name="l03038"></a><span class="lineno"> 3038</span>&#160;                        output = getattr(torch, name)(*inputs)</div><div class="line"><a name="l03039"></a><span class="lineno"> 3039</span>&#160;                        <span class="keywordflow">return</span> output_process_fn(output)</div><div class="line"><a name="l03040"></a><span class="lineno"> 3040</span>&#160;</div><div class="line"><a name="l03041"></a><span class="lineno"> 3041</span>&#160;                    f_args_variable = (self_variable,) + args_variable</div><div class="line"><a name="l03042"></a><span class="lineno"> 3042</span>&#160;                    f_args_tensor = (self_tensor,) + args_tensor</div><div class="line"><a name="l03043"></a><span class="lineno"> 3043</span>&#160;                    <span class="comment"># could run the gradchecks again, but skip since we did it for the methods above.</span></div><div class="line"><a name="l03044"></a><span class="lineno"> 3044</span>&#160;                    run_functional_checks(self, test_name, name, fn,</div><div class="line"><a name="l03045"></a><span class="lineno"> 3045</span>&#160;                                          <span class="keyword">False</span>, f_args_variable, f_args_tensor)</div><div class="line"><a name="l03046"></a><span class="lineno"> 3046</span>&#160;</div><div class="line"><a name="l03047"></a><span class="lineno"> 3047</span>&#160;                <span class="comment"># check for correct type of input.data and input.grad.data</span></div><div class="line"><a name="l03048"></a><span class="lineno"> 3048</span>&#160;                <span class="keywordflow">if</span> <span class="keywordflow">not</span> is_inplace:</div><div class="line"><a name="l03049"></a><span class="lineno"> 3049</span>&#160;                    self_variable = create_input((self_size,), requires_grad=<span class="keyword">True</span>)[0][0]</div><div class="line"><a name="l03050"></a><span class="lineno"> 3050</span>&#160;                    args_variable, kwargs_variable = create_input(args, requires_grad=<span class="keyword">False</span>, call_kwargs=kwargs)</div><div class="line"><a name="l03051"></a><span class="lineno"> 3051</span>&#160;                    output_variable = getattr(self_variable, name)(*args_variable, **kwargs_variable)</div><div class="line"><a name="l03052"></a><span class="lineno"> 3052</span>&#160;                    <span class="keywordflow">if</span> isinstance(output_variable, torch.autograd.Variable):</div><div class="line"><a name="l03053"></a><span class="lineno"> 3053</span>&#160;                        output_variable.backward(randn_like(output_variable))</div><div class="line"><a name="l03054"></a><span class="lineno"> 3054</span>&#160;                        self.assertTrue(type(self_variable.data) == type(self_variable.grad.data))</div><div class="line"><a name="l03055"></a><span class="lineno"> 3055</span>&#160;                        self.assertTrue(self_variable.size() == self_variable.grad.size())</div><div class="line"><a name="l03056"></a><span class="lineno"> 3056</span>&#160;</div><div class="line"><a name="l03057"></a><span class="lineno"> 3057</span>&#160;                    <span class="comment"># compare grads to inplace grads</span></div><div class="line"><a name="l03058"></a><span class="lineno"> 3058</span>&#160;                    inplace_name = name + <span class="stringliteral">&#39;_&#39;</span></div><div class="line"><a name="l03059"></a><span class="lineno"> 3059</span>&#160;                    <span class="comment"># can&#39;t broadcast inplace to left hand side</span></div><div class="line"><a name="l03060"></a><span class="lineno"> 3060</span>&#160;                    skip_inplace = (<span class="stringliteral">&#39;broadcast_lhs&#39;</span> <span class="keywordflow">in</span> test_name <span class="keywordflow">or</span></div><div class="line"><a name="l03061"></a><span class="lineno"> 3061</span>&#160;                                    <span class="stringliteral">&#39;broadcast_all&#39;</span> <span class="keywordflow">in</span> test_name)</div><div class="line"><a name="l03062"></a><span class="lineno"> 3062</span>&#160;                    <span class="keywordflow">if</span> hasattr(torch.ones(1), inplace_name) <span class="keywordflow">and</span> <span class="keywordflow">not</span> skip_inplace:</div><div class="line"><a name="l03063"></a><span class="lineno"> 3063</span>&#160;                        output_variable = getattr(self_variable, name)(*args_variable, **kwargs_variable)</div><div class="line"><a name="l03064"></a><span class="lineno"> 3064</span>&#160;                        <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(output_variable, tuple):</div><div class="line"><a name="l03065"></a><span class="lineno"> 3065</span>&#160;                            output_variable = (output_variable,)</div><div class="line"><a name="l03066"></a><span class="lineno"> 3066</span>&#160;                        inplace_self_variable = deepcopy(self_variable)</div><div class="line"><a name="l03067"></a><span class="lineno"> 3067</span>&#160;                        inplace_self_variable_copy = tuple(i.clone() <span class="keywordflow">if</span> isinstance(i, torch.Tensor) <span class="keywordflow">else</span> i</div><div class="line"><a name="l03068"></a><span class="lineno"> 3068</span>&#160;                                                           <span class="keywordflow">for</span> i <span class="keywordflow">in</span> (inplace_self_variable,))</div><div class="line"><a name="l03069"></a><span class="lineno"> 3069</span>&#160;                        inplace_args_variable = deepcopy(args_variable)</div><div class="line"><a name="l03070"></a><span class="lineno"> 3070</span>&#160;                        inplace_args_variable_copy = tuple(i.clone() <span class="keywordflow">if</span> isinstance(i, torch.Tensor) <span class="keywordflow">else</span> i</div><div class="line"><a name="l03071"></a><span class="lineno"> 3071</span>&#160;                                                           <span class="keywordflow">for</span> i <span class="keywordflow">in</span> inplace_args_variable)</div><div class="line"><a name="l03072"></a><span class="lineno"> 3072</span>&#160;</div><div class="line"><a name="l03073"></a><span class="lineno"> 3073</span>&#160;                        inplace_output_variable = (</div><div class="line"><a name="l03074"></a><span class="lineno"> 3074</span>&#160;                            getattr(inplace_self_variable_copy[0], inplace_name)(*inplace_args_variable_copy,</div><div class="line"><a name="l03075"></a><span class="lineno"> 3075</span>&#160;                                                                                 **kwargs_variable))</div><div class="line"><a name="l03076"></a><span class="lineno"> 3076</span>&#160;                        <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(inplace_output_variable, tuple):</div><div class="line"><a name="l03077"></a><span class="lineno"> 3077</span>&#160;                            inplace_output_variable = (inplace_output_variable,)</div><div class="line"><a name="l03078"></a><span class="lineno"> 3078</span>&#160;                        self.assertEqual(inplace_output_variable, output_variable)</div><div class="line"><a name="l03079"></a><span class="lineno"> 3079</span>&#160;                        <span class="comment"># Check that gradient is the same</span></div><div class="line"><a name="l03080"></a><span class="lineno"> 3080</span>&#160;                        <span class="keywordflow">for</span> inp_i, i <span class="keywordflow">in</span> zip((inplace_self_variable,) + inplace_args_variable,</div><div class="line"><a name="l03081"></a><span class="lineno"> 3081</span>&#160;                                            (self_variable,) + args_variable):</div><div class="line"><a name="l03082"></a><span class="lineno"> 3082</span>&#160;                            <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(inp_i, torch.Tensor):</div><div class="line"><a name="l03083"></a><span class="lineno"> 3083</span>&#160;                                <span class="keyword">assert</span> <span class="keywordflow">not</span> isinstance(i, torch.Tensor)</div><div class="line"><a name="l03084"></a><span class="lineno"> 3084</span>&#160;                                <span class="keywordflow">continue</span></div><div class="line"><a name="l03085"></a><span class="lineno"> 3085</span>&#160;                            <span class="keywordflow">if</span> inp_i.grad <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l03086"></a><span class="lineno"> 3086</span>&#160;                                inp_i.grad.data.zero_()</div><div class="line"><a name="l03087"></a><span class="lineno"> 3087</span>&#160;                            <span class="keywordflow">if</span> i.grad <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l03088"></a><span class="lineno"> 3088</span>&#160;                                i.grad.data.zero_()</div><div class="line"><a name="l03089"></a><span class="lineno"> 3089</span>&#160;                        <span class="keywordflow">for</span> io, o <span class="keywordflow">in</span> zip(inplace_output_variable, output_variable):</div><div class="line"><a name="l03090"></a><span class="lineno"> 3090</span>&#160;                            grad = randn_like(io).double()</div><div class="line"><a name="l03091"></a><span class="lineno"> 3091</span>&#160;                            io.backward(grad)</div><div class="line"><a name="l03092"></a><span class="lineno"> 3092</span>&#160;                            o.backward(grad)</div><div class="line"><a name="l03093"></a><span class="lineno"> 3093</span>&#160;                        <span class="keywordflow">for</span> inp_i, i <span class="keywordflow">in</span> zip((inplace_self_variable,) + inplace_args_variable,</div><div class="line"><a name="l03094"></a><span class="lineno"> 3094</span>&#160;                                            (self_variable,) + args_variable):</div><div class="line"><a name="l03095"></a><span class="lineno"> 3095</span>&#160;                            <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(inp_i, torch.Tensor):</div><div class="line"><a name="l03096"></a><span class="lineno"> 3096</span>&#160;                                <span class="keywordflow">continue</span></div><div class="line"><a name="l03097"></a><span class="lineno"> 3097</span>&#160;                            self.assertEqual(inp_i.grad, i.grad)</div><div class="line"><a name="l03098"></a><span class="lineno"> 3098</span>&#160;</div><div class="line"><a name="l03099"></a><span class="lineno"> 3099</span>&#160;            check(name)</div><div class="line"><a name="l03100"></a><span class="lineno"> 3100</span>&#160;            inplace_name = name + <span class="stringliteral">&#39;_&#39;</span></div><div class="line"><a name="l03101"></a><span class="lineno"> 3101</span>&#160;            <span class="comment"># can&#39;t broadcast inplace to left hand side</span></div><div class="line"><a name="l03102"></a><span class="lineno"> 3102</span>&#160;            broadcast_skip_inplace = <span class="stringliteral">&#39;broadcast_lhs&#39;</span> <span class="keywordflow">in</span> test_name <span class="keywordflow">or</span> <span class="stringliteral">&#39;broadcast_all&#39;</span> <span class="keywordflow">in</span> test_name</div><div class="line"><a name="l03103"></a><span class="lineno"> 3103</span>&#160;            <span class="keywordflow">if</span> hasattr(torch.ones(1), inplace_name) <span class="keywordflow">and</span> <span class="keywordflow">not</span> broadcast_skip_inplace:</div><div class="line"><a name="l03104"></a><span class="lineno"> 3104</span>&#160;                check(inplace_name)</div><div class="line"><a name="l03105"></a><span class="lineno"> 3105</span>&#160;</div><div class="line"><a name="l03106"></a><span class="lineno"> 3106</span>&#160;        <span class="keyword">assert</span> <span class="keywordflow">not</span> hasattr(TestAutograd, test_name), <span class="stringliteral">&#39;Two tests have the same name: &#39;</span> + test_name</div><div class="line"><a name="l03107"></a><span class="lineno"> 3107</span>&#160;</div><div class="line"><a name="l03108"></a><span class="lineno"> 3108</span>&#160;        <span class="keywordflow">for</span> skip <span class="keywordflow">in</span> skipTestIf:</div><div class="line"><a name="l03109"></a><span class="lineno"> 3109</span>&#160;            do_test = skip(do_test)</div><div class="line"><a name="l03110"></a><span class="lineno"> 3110</span>&#160;</div><div class="line"><a name="l03111"></a><span class="lineno"> 3111</span>&#160;        setattr(TestAutograd, test_name, do_test)</div><div class="line"><a name="l03112"></a><span class="lineno"> 3112</span>&#160;</div><div class="line"><a name="l03113"></a><span class="lineno"> 3113</span>&#160;<span class="keywordflow">for</span> test <span class="keywordflow">in</span> method_tests():</div><div class="line"><a name="l03114"></a><span class="lineno"> 3114</span>&#160;    add_test(*test)</div><div class="line"><a name="l03115"></a><span class="lineno"> 3115</span>&#160;</div><div class="line"><a name="l03116"></a><span class="lineno"> 3116</span>&#160;<span class="keywordflow">if</span> __name__ == <span class="stringliteral">&#39;__main__&#39;</span>:</div><div class="line"><a name="l03117"></a><span class="lineno"> 3117</span>&#160;    run_tests()</div><div class="ttc" id="torch_2nn_2functional_8py_html_a1425cdc9a08e40d8269b3f606660d611"><div class="ttname"><a href="torch_2nn_2functional_8py.html#a1425cdc9a08e40d8269b3f606660d611">torch.nn.functional.relu</a></div><div class="ttdeci">def relu(input, inplace=False)</div><div class="ttdef"><b>Definition:</b> <a href="torch_2nn_2functional_8py_source.html#l00929">functional.py:929</a></div></div>
<div class="ttc" id="classtorch_1_1autograd_1_1function_1_1_inplace_function_html"><div class="ttname"><a href="classtorch_1_1autograd_1_1function_1_1_inplace_function.html">torch.autograd.function.InplaceFunction</a></div><div class="ttdef"><b>Definition:</b> <a href="function_8py_source.html#l00243">function.py:243</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_a39787af23a8c99676aaea7ce13d2f233"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#a39787af23a8c99676aaea7ce13d2f233">test_autograd.TestAutograd._test_setitem_tensor</a></div><div class="ttdeci">def _test_setitem_tensor(self, size, index)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l01388">test_autograd.py:1388</a></div></div>
<div class="ttc" id="namespacetorch_1_1nn_1_1parallel_1_1__functions_html"><div class="ttname"><a href="namespacetorch_1_1nn_1_1parallel_1_1__functions.html">torch.nn.parallel._functions</a></div><div class="ttdef"><b>Definition:</b> <a href="parallel_2__functions_8py_source.html#l00001">_functions.py:1</a></div></div>
<div class="ttc" id="namespacescope_html"><div class="ttname"><a href="namespacescope.html">scope</a></div><div class="ttdoc">Module caffe2.python.scope. </div></div>
<div class="ttc" id="namespacetorch_1_1utils_1_1checkpoint_html"><div class="ttname"><a href="namespacetorch_1_1utils_1_1checkpoint.html">torch.utils.checkpoint</a></div><div class="ttdef"><b>Definition:</b> <a href="torch_2utils_2checkpoint_8py_source.html#l00001">checkpoint.py:1</a></div></div>
<div class="ttc" id="namespacetorch_1_1autograd_html_a82d2dec71e1d9a2dce3ad3b286237ffe"><div class="ttname"><a href="namespacetorch_1_1autograd.html#a82d2dec71e1d9a2dce3ad3b286237ffe">torch.autograd.backward</a></div><div class="ttdeci">def backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)</div><div class="ttdef"><b>Definition:</b> <a href="torch_2autograd_2____init_____8py_source.html#l00038">__init__.py:38</a></div></div>
<div class="ttc" id="namespacecheckpoint_html"><div class="ttname"><a href="namespacecheckpoint.html">checkpoint</a></div><div class="ttdoc">Module caffe2.python.checkpoint. </div></div>
<div class="ttc" id="namespacetest_html"><div class="ttname"><a href="namespacetest.html">test</a></div><div class="ttdef"><b>Definition:</b> <a href="bottleneck_2test_8py_source.html#l00001">test.py:1</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_adb8918803c42299b58ef47d07a9f33e5"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#adb8918803c42299b58ef47d07a9f33e5">test_autograd.TestAutograd.test_gc_in_destructor</a></div><div class="ttdeci">def test_gc_in_destructor(self)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l01601">test_autograd.py:1601</a></div></div>
<div class="ttc" id="torch_2cuda_2____init_____8py_html_a89aba7c2da9db4355a86512e3276ab8a"><div class="ttname"><a href="torch_2cuda_2____init_____8py.html#a89aba7c2da9db4355a86512e3276ab8a">torch.cuda.is_available</a></div><div class="ttdeci">def is_available()</div><div class="ttdef"><b>Definition:</b> <a href="torch_2cuda_2____init_____8py_source.html#l00045">__init__.py:45</a></div></div>
<div class="ttc" id="namespacetorch_1_1__six_html"><div class="ttname"><a href="namespacetorch_1_1__six.html">torch._six</a></div><div class="ttdef"><b>Definition:</b> <a href="__six_8py_source.html#l00001">_six.py:1</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html">test_autograd.TestAutograd</a></div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l00072">test_autograd.py:72</a></div></div>
<div class="ttc" id="torch_2cuda_2____init_____8py_html_afc565147dae449b070aaf1f6c086cb51"><div class="ttname"><a href="torch_2cuda_2____init_____8py.html#afc565147dae449b070aaf1f6c086cb51">torch.cuda.device_count</a></div><div class="ttdeci">def device_count()</div><div class="ttdef"><b>Definition:</b> <a href="torch_2cuda_2____init_____8py_source.html#l00341">__init__.py:341</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_a6b73c08752701bcfd98c59de3ce91550"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#a6b73c08752701bcfd98c59de3ce91550">test_autograd.TestAutograd._test_pyscalar_conversions</a></div><div class="ttdeci">def _test_pyscalar_conversions(self, t, integral_conv)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l01758">test_autograd.py:1758</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_aeec79a3d621dc4e50030dc79dbff7c3d"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#aeec79a3d621dc4e50030dc79dbff7c3d">test_autograd.TestAutograd.test_no_grad_python_function</a></div><div class="ttdeci">def test_no_grad_python_function(self)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l00874">test_autograd.py:874</a></div></div>
<div class="ttc" id="namespacetorch_1_1autograd_1_1profiler_html"><div class="ttname"><a href="namespacetorch_1_1autograd_1_1profiler.html">torch.autograd.profiler</a></div><div class="ttdef"><b>Definition:</b> <a href="autograd_2profiler_8py_source.html#l00001">profiler.py:1</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_acc21b5260d1198c9b945b956c60afb11"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#acc21b5260d1198c9b945b956c60afb11">test_autograd.TestAutograd.grad</a></div><div class="ttdeci">grad</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l00578">test_autograd.py:578</a></div></div>
<div class="ttc" id="namespacetorch_1_1autograd_1_1function_html"><div class="ttname"><a href="namespacetorch_1_1autograd_1_1function.html">torch.autograd.function</a></div><div class="ttdef"><b>Definition:</b> <a href="function_8py_source.html#l00001">function.py:1</a></div></div>
<div class="ttc" id="namespacetorch_1_1testing_html"><div class="ttname"><a href="namespacetorch_1_1testing.html">torch.testing</a></div><div class="ttdef"><b>Definition:</b> <a href="torch_2testing_2____init_____8py_source.html#l00001">__init__.py:1</a></div></div>
<div class="ttc" id="namespacetorch_1_1autograd_html"><div class="ttname"><a href="namespacetorch_1_1autograd.html">torch.autograd</a></div><div class="ttdef"><b>Definition:</b> <a href="torch_2autograd_2____init_____8py_source.html#l00001">__init__.py:1</a></div></div>
<div class="ttc" id="torch_2cuda_2____init_____8py_html_a55f030e9a32b289b723487de9af1f5d9"><div class="ttname"><a href="torch_2cuda_2____init_____8py.html#a55f030e9a32b289b723487de9af1f5d9">torch.cuda.memory_allocated</a></div><div class="ttdeci">def memory_allocated(device=None)</div><div class="ttdef"><b>Definition:</b> <a href="torch_2cuda_2____init_____8py_source.html#l00409">__init__.py:409</a></div></div>
<div class="ttc" id="classtorch_1_1autograd_1_1profiler_1_1profile_html"><div class="ttname"><a href="classtorch_1_1autograd_1_1profiler_1_1profile.html">torch.autograd.profiler.profile</a></div><div class="ttdef"><b>Definition:</b> <a href="autograd_2profiler_8py_source.html#l00129">profiler.py:129</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_aa35d961c325d8da1556f0cfd2c4773f9"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#aa35d961c325d8da1556f0cfd2c4773f9">test_autograd.TestAutograd._test_lerp_tensor_weights</a></div><div class="ttdeci">def _test_lerp_tensor_weights(self, cast)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l02479">test_autograd.py:2479</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_a3195838ca5a03a3b24576c7146e93f44"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#a3195838ca5a03a3b24576c7146e93f44">test_autograd.TestAutograd._function_test</a></div><div class="ttdeci">def _function_test(self, cls)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l00074">test_autograd.py:74</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_ad46368b6d7f3b992d8e20d9d196bc7b7"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#ad46368b6d7f3b992d8e20d9d196bc7b7">test_autograd.TestAutograd._test_type_conversion_backward</a></div><div class="ttdeci">def _test_type_conversion_backward(self, t)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l01695">test_autograd.py:1695</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_a245f883a0b3daf962dc5f7bfa7657a06"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#a245f883a0b3daf962dc5f7bfa7657a06">test_autograd.TestAutograd._test_set_requires_grad_only_for_floats</a></div><div class="ttdeci">def _test_set_requires_grad_only_for_floats(self, cuda)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l02699">test_autograd.py:2699</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_abc80f819c303132afbba9b6b160b79bd"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#abc80f819c303132afbba9b6b160b79bd">test_autograd.TestAutograd._test_setitem</a></div><div class="ttdeci">def _test_setitem(self, size, index)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l01377">test_autograd.py:1377</a></div></div>
<div class="ttc" id="torch_2nn_2functional_8py_html_af6226b2d76538bc3946825155e260ef9"><div class="ttname"><a href="torch_2nn_2functional_8py.html#af6226b2d76538bc3946825155e260ef9">torch.nn.functional.ctc_loss</a></div><div class="ttdeci">def ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)</div><div class="ttdef"><b>Definition:</b> <a href="torch_2nn_2functional_8py_source.html#l01744">functional.py:1744</a></div></div>
<div class="ttc" id="torch_2sparse_2____init_____8py_html_ade8a78138321665999db9cd6849c29a8"><div class="ttname"><a href="torch_2sparse_2____init_____8py.html#ade8a78138321665999db9cd6849c29a8">torch.sparse.sum</a></div><div class="ttdeci">def sum(input, dim=None, dtype=None)</div><div class="ttdef"><b>Definition:</b> <a href="torch_2sparse_2____init_____8py_source.html#l00070">__init__.py:70</a></div></div>
<div class="ttc" id="namespacetorch_1_1autograd_1_1gradcheck_html"><div class="ttname"><a href="namespacetorch_1_1autograd_1_1gradcheck.html">torch.autograd.gradcheck</a></div><div class="ttdef"><b>Definition:</b> <a href="gradcheck_8py_source.html#l00001">gradcheck.py:1</a></div></div>
<div class="ttc" id="torch_2cuda_2____init_____8py_html_a1d0466ead04bf87515421e836c858d74"><div class="ttname"><a href="torch_2cuda_2____init_____8py.html#a1d0466ead04bf87515421e836c858d74">torch.cuda.current_device</a></div><div class="ttdeci">def current_device()</div><div class="ttdef"><b>Definition:</b> <a href="torch_2cuda_2____init_____8py_source.html#l00349">__init__.py:349</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_add0f247ddbbb614bed6116f90e84c7c5"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#add0f247ddbbb614bed6116f90e84c7c5">test_autograd.TestAutograd._test_sparse_gather</a></div><div class="ttdeci">def _test_sparse_gather(self, size_x, size_ind, dim)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l01568">test_autograd.py:1568</a></div></div>
<div class="ttc" id="namespacetorch_1_1autograd_html_a475508b88050645cdc4432306ab78d3a"><div class="ttname"><a href="namespacetorch_1_1autograd.html#a475508b88050645cdc4432306ab78d3a">torch.autograd.grad</a></div><div class="ttdeci">def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)</div><div class="ttdef"><b>Definition:</b> <a href="torch_2autograd_2____init_____8py_source.html#l00097">__init__.py:97</a></div></div>
<div class="ttc" id="classtest__autograd_1_1_test_autograd_html_a374f1759fc0be246b2bfa3375feddfdf"><div class="ttname"><a href="classtest__autograd_1_1_test_autograd.html#a374f1759fc0be246b2bfa3375feddfdf">test_autograd.TestAutograd._test_where_functional</a></div><div class="ttdeci">def _test_where_functional(self, t)</div><div class="ttdef"><b>Definition:</b> <a href="test__autograd_8py_source.html#l02456">test_autograd.py:2456</a></div></div>
<div class="ttc" id="namespacetest__indexing_html"><div class="ttname"><a href="namespacetest__indexing.html">test_indexing</a></div><div class="ttdef"><b>Definition:</b> <a href="test__indexing_8py_source.html#l00001">test_indexing.py:1</a></div></div>
<div class="ttc" id="namespacetorch_1_1tensor_html"><div class="ttname"><a href="namespacetorch_1_1tensor.html">torch.tensor</a></div><div class="ttdef"><b>Definition:</b> <a href="tensor_8py_source.html#l00001">tensor.py:1</a></div></div>
</div><!-- fragment --></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.14-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Thu Mar 21 2019 13:06:36 for Caffe2 - Python API by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
<div class="footerContainer">
  <div id="footer_wrap" class="wrapper footerWrapper">
    <div class="footerBlocks">
      <div id="fb_oss" class="footerSection fbOpenSourceFooter">
          <svg class="facebookOSSLogoSvg" viewBox="0 0 1133.9 1133.9" x="0px" y="0px" height=50 width=50>
            <g>
              <path class="logoRing outerRing" d="M 498.3 3.7 c 153.6 88.9 307.3 177.7 461.1 266.2 c 7.6 4.4 10.3 9.1 10.3 17.8 c -0.3 179.1 -0.2 358.3 0 537.4 c 0 8.1 -2.4 12.8 -9.7 17.1 c -154.5 88.9 -308.8 178.1 -462.9 267.5 c -9 5.2 -15.5 5.3 -24.6 0.1 c -153.9 -89.2 -307.9 -178 -462.1 -266.8 C 3 838.8 0 833.9 0 825.1 c 0.3 -179.1 0.2 -358.3 0 -537.4 c 0 -8.6 2.6 -13.6 10.2 -18 C 164.4 180.9 318.4 92 472.4 3 C 477 -1.5 494.3 -0.7 498.3 3.7 Z M 48.8 555.3 c 0 79.9 0.2 159.9 -0.2 239.8 c -0.1 10 3 15.6 11.7 20.6 c 137.2 78.8 274.2 157.8 411 237.3 c 9.9 5.7 17 5.7 26.8 0.1 c 137.5 -79.8 275.2 -159.2 412.9 -238.5 c 7.4 -4.3 10.5 -8.9 10.5 -17.8 c -0.3 -160.2 -0.3 -320.5 0 -480.7 c 0 -8.8 -2.8 -13.6 -10.3 -18 C 772.1 218 633.1 137.8 494.2 57.4 c -6.5 -3.8 -11.5 -4.5 -18.5 -0.5 C 336.8 137.4 197.9 217.7 58.8 297.7 c -7.7 4.4 -10.2 9.2 -10.2 17.9 C 48.9 395.5 48.8 475.4 48.8 555.3 Z" />
              <path class="logoRing middleRing" d="M 184.4 555.9 c 0 -33.3 -1 -66.7 0.3 -100 c 1.9 -48 24.1 -86 64.7 -110.9 c 54.8 -33.6 110.7 -65.5 167 -96.6 c 45.7 -25.2 92.9 -24.7 138.6 1 c 54.4 30.6 108.7 61.5 162.2 93.7 c 44 26.5 67.3 66.8 68 118.4 c 0.9 63.2 0.9 126.5 0 189.7 c -0.7 50.6 -23.4 90.7 -66.6 116.9 c -55 33.4 -110.8 65.4 -167.1 96.5 c -43.4 24 -89 24.2 -132.3 0.5 c -57.5 -31.3 -114.2 -64 -170 -98.3 c -41 -25.1 -62.9 -63.7 -64.5 -112.2 C 183.5 621.9 184.3 588.9 184.4 555.9 Z M 232.9 556.3 c 0 29.5 0.5 59.1 -0.1 88.6 c -0.8 39.2 16.9 67.1 50.2 86.2 c 51.2 29.4 102.2 59.2 153.4 88.4 c 31.4 17.9 63.6 18.3 95 0.6 c 53.7 -30.3 107.1 -61.2 160.3 -92.5 c 29.7 -17.5 45 -44.5 45.3 -78.8 c 0.6 -61.7 0.5 -123.5 0 -185.2 c -0.3 -34.4 -15.3 -61.5 -44.9 -79 C 637.7 352.6 583 320.8 527.9 290 c -27.5 -15.4 -57.2 -16.1 -84.7 -0.7 c -56.9 31.6 -113.4 64 -169.1 97.6 c -26.4 15.9 -40.7 41.3 -41.1 72.9 C 232.6 491.9 232.9 524.1 232.9 556.3 Z" />
              <path class="logoRing innerRing" d="M 484.9 424.4 c 69.8 -2.8 133.2 57.8 132.6 132 C 617 630 558.5 688.7 484.9 689.1 c -75.1 0.4 -132.6 -63.6 -132.7 -132.7 C 352.1 485 413.4 421.5 484.9 424.4 Z M 401.3 556.7 c -3.4 37.2 30.5 83.6 83 84.1 c 46.6 0.4 84.8 -37.6 84.9 -84 c 0.1 -46.6 -37.2 -84.4 -84.2 -84.6 C 432.2 472.1 397.9 518.3 401.3 556.7 Z" />
            </g>
          </svg>
        <h2>Facebook Open Source</h2>
      </div>
      <div class="footerSection">
        <a class="footerLink" href="https://code.facebook.com/projects/" target="_blank">Open Source Projects</a>
        <a class="footerLink" href="https://github.com/facebook/" target="_blank">GitHub</a>
        <a class="footerLink" href="https://twitter.com/fbOpenSource" target="_blank">Twitter</a>
      </div>
      <div class="footerSection rightAlign">
        <a class="footerLink" href="https://github.com/caffe2/caffe2" target="_blank">Contribute to this project on GitHub</a>
      </div>
    </div>
  </div>
</div>
<script type="text/javascript" src="/js/jekyll-link-anchors.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '{{ site.gacode }}', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
